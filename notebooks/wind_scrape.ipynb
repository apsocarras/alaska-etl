{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import datetime as dt\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url:str, delay=0) -> BeautifulSoup:\n",
    "  \"\"\"Simple wrapper for getting beautiful soup object from url with sleep delay\n",
    "  \n",
    "  Args: \n",
    "\n",
    "  url (str): url you're scraping\n",
    "\n",
    "  delay (int): time you want to wait between next request (default 0)\n",
    "  \"\"\"\n",
    "  result = requests.get(url)\n",
    "  time.sleep(delay)\n",
    "  return BeautifulSoup(result.content, \"html.parser\") \n",
    "\n",
    "def get_year_urls(uscrn_directory:str) -> list: \n",
    "  \"\"\"\n",
    "  Retrieves the URLs for every year's page in the given USCRN directory.\n",
    "  \n",
    "  Arguments:\n",
    "  uscrn_directory (str): Either 'hourly02' or 'subhourly01' (i.e. source of wind data)\n",
    "\n",
    "  Returns:\n",
    "  year_urls (list): A list of URLs for every year's page.\n",
    "  \"\"\"\n",
    "\n",
    "  if uscrn_directory not in (\"hourly02\", 'subhourly01'):\n",
    "    raise Exception(f\"Invalid directory given: {uscrn_directory} -- give 'hourly02' or 'subhourly01'\")\n",
    "  \n",
    "  url = f\"https://www.ncei.noaa.gov/pub/data/uscrn/products/{uscrn_directory}/\"\n",
    "  soup = get_soup(url, 1)\n",
    "\n",
    "  # Wind data is first available in 2012\n",
    "  start_year = 2012 if uscrn_directory == \"subhourly01\" else 2000\n",
    "\n",
    "  links = soup.find_all(\"a\") \n",
    "  years = [str(y).zfill(1) for y in range(start_year, 2024)]\n",
    "  year_urls = [url + link['href'] for link in links if link['href'].rstrip('/') in years]\n",
    "  return year_urls\n",
    "\n",
    "def get_file_urls(uscrn_directory:str) -> list: \n",
    "  \"\"\"\n",
    "  Retrieves the URLs for every file contained on each year's page in the given USCRN directory\n",
    "\n",
    "  Arguments:\n",
    "  uscrn_directory (str): Either 'hourly02' or 'subhourly01'\n",
    "\n",
    "  Returns: \n",
    "  file_urls (list): A list of file URLs.\n",
    "  \"\"\"\n",
    "\n",
    "  if uscrn_directory not in (\"hourly02\", 'subhourly01'):\n",
    "    raise Exception(f\"Invalid directory given: {uscrn_directory} -- give 'hourly02' or 'subhourly01'\")\n",
    "\n",
    "  year_urls = get_year_urls(uscrn_directory)\n",
    "\n",
    "  file_urls = []\n",
    "  for url in year_urls: \n",
    "    soup = get_soup(url)\n",
    "    file_links = soup.find_all('a', href=re.compile(r'AK.*\\.txt'))\n",
    "    if file_links:\n",
    "      new_file_urls = [url + link.getText() for link in file_links]\n",
    "      file_urls.extend(new_file_urls)\n",
    "  return file_urls\n",
    "\n",
    "def get_station_location(url) -> str: \n",
    "  \"\"\"\n",
    "  Extracts the name of the station from a given URL.\n",
    "  \n",
    "  Args:\n",
    "  url (str): The URL to extract the station name from.\n",
    "  \n",
    "  Returns:\n",
    "  station_location (str): The name of the station.\n",
    "  \"\"\"\n",
    "  regex = r\"([St.]*[A-Z][a-z]+_*[A-Za-z]*).*.txt\" \n",
    "  file_name = re.search(regex, url).group(0)\n",
    "  station_location = re.sub(\"(_formerly_Barrow.*|_[0-9].*)\", \"\", file_name)\n",
    "  return  station_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_urls = get_file_urls(\"subhourly01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_rows(file_urls, output_file) -> None:\n",
    "  \"\"\"\n",
    "\n",
    "  Args:\n",
    "    file_urls (list): List of text file urls. \n",
    "    output_file (str): The path to the output CSV file.\n",
    "\n",
    "  Returns:\n",
    "    None\n",
    "  \"\"\"\n",
    "  for url in file_urls:\n",
    "    # Get location from url\n",
    "    station_location = get_station_location(url)\n",
    "    # Get new rows \n",
    "    soup = get_soup(url, delay=.5)\n",
    "    lines = [re.split('\\s+', line) for line in str(soup).strip().splitlines()]\n",
    "    # We're only scraping this data for the wind information, so we ignore rows that don't have any (i.e wind < 0)\n",
    "    wind_cols = [[station_location] + line[:5] + line[-2:] for line in lines if float(line[-2]) >= 0]\n",
    "    # Write rows to CSV\n",
    "    if wind_cols:\n",
    "      with open(output_file, \"a+\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(wind_cols)\n",
    "      del wind_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = ['station_location','wbanno','utc_date','utc_time',\n",
    "  'lst_date','lst_time',\"wind_1_5\", \"wind_flag\"]\n",
    "\n",
    "df = pd.read_csv(\"../../data/uscrn_wind_raw.csv\", names=colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['wind_1_5'] = df['wind_1_5'].astype(float)\n",
    "\n",
    "# convert to datetimes\n",
    "df['utc_datetime'] = pd.to_datetime(df['utc_date'].astype(int).astype(str) + df['utc_time'].astype(int).astype(str).str.zfill(4), format='%Y%m%d%H%M')\n",
    "df['lst_datetime'] = pd.to_datetime(df['lst_date'].astype(int).astype(str) + df['lst_time'].astype(int).astype(str).str.zfill(4), format='%Y%m%d%H%M')\n",
    "\n",
    "# round to nearest hour \n",
    "df['utc_datetime'] = df['utc_datetime'].dt.floor(\"H\")\n",
    "df['lst_datetime'] = df['lst_datetime'].dt.floor(\"H\")\n",
    "\n",
    "# drop poor quality data (wind_flag == 3: roughly 1.9% of rows)\n",
    "df = df[df['wind_flag'] == 0]\n",
    "df.drop(\"wind_flag\", axis=1, inplace=True)\n",
    "\n",
    "# # calculate hourly averages \n",
    "# df = df.groupby(['station_location','wbanno','utc_datetime','lst_datetime','wind_flag'])['wind_1_5'].mean().reset_index()\n",
    "\n",
    "# rename wind column \n",
    "# df.rename({\"wind_1_5\":\"wind_hr_avg\"}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[['station_location', 'wbanno', 'utc_datetime', 'lst_datetime', 'wind_hr_avg', 'wind_flag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_location</th>\n",
       "      <th>wbanno</th>\n",
       "      <th>utc_datetime</th>\n",
       "      <th>lst_datetime</th>\n",
       "      <th>wind_1_5</th>\n",
       "      <th>wind_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fairbanks</td>\n",
       "      <td>26494</td>\n",
       "      <td>2012-08-09 17:00:00</td>\n",
       "      <td>2012-08-09 08:00:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fairbanks</td>\n",
       "      <td>26494</td>\n",
       "      <td>2012-08-09 17:00:00</td>\n",
       "      <td>2012-08-09 08:00:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fairbanks</td>\n",
       "      <td>26494</td>\n",
       "      <td>2012-08-09 17:00:00</td>\n",
       "      <td>2012-08-09 08:00:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fairbanks</td>\n",
       "      <td>26494</td>\n",
       "      <td>2012-08-09 17:00:00</td>\n",
       "      <td>2012-08-09 08:00:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fairbanks</td>\n",
       "      <td>26494</td>\n",
       "      <td>2012-08-09 17:00:00</td>\n",
       "      <td>2012-08-09 08:00:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20740031</th>\n",
       "      <td>Yakutat</td>\n",
       "      <td>25382</td>\n",
       "      <td>2023-03-04 20:00:00</td>\n",
       "      <td>2023-03-04 11:00:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20740032</th>\n",
       "      <td>Yakutat</td>\n",
       "      <td>25382</td>\n",
       "      <td>2023-03-04 20:00:00</td>\n",
       "      <td>2023-03-04 11:00:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20740033</th>\n",
       "      <td>Yakutat</td>\n",
       "      <td>25382</td>\n",
       "      <td>2023-03-04 20:00:00</td>\n",
       "      <td>2023-03-04 11:00:00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20740034</th>\n",
       "      <td>Yakutat</td>\n",
       "      <td>25382</td>\n",
       "      <td>2023-03-04 20:00:00</td>\n",
       "      <td>2023-03-04 11:00:00</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20740035</th>\n",
       "      <td>Yakutat</td>\n",
       "      <td>25382</td>\n",
       "      <td>2023-03-04 21:00:00</td>\n",
       "      <td>2023-03-04 12:00:00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20740036 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         station_location  wbanno        utc_datetime        lst_datetime  \\\n",
       "0               Fairbanks   26494 2012-08-09 17:00:00 2012-08-09 08:00:00   \n",
       "1               Fairbanks   26494 2012-08-09 17:00:00 2012-08-09 08:00:00   \n",
       "2               Fairbanks   26494 2012-08-09 17:00:00 2012-08-09 08:00:00   \n",
       "3               Fairbanks   26494 2012-08-09 17:00:00 2012-08-09 08:00:00   \n",
       "4               Fairbanks   26494 2012-08-09 17:00:00 2012-08-09 08:00:00   \n",
       "...                   ...     ...                 ...                 ...   \n",
       "20740031          Yakutat   25382 2023-03-04 20:00:00 2023-03-04 11:00:00   \n",
       "20740032          Yakutat   25382 2023-03-04 20:00:00 2023-03-04 11:00:00   \n",
       "20740033          Yakutat   25382 2023-03-04 20:00:00 2023-03-04 11:00:00   \n",
       "20740034          Yakutat   25382 2023-03-04 20:00:00 2023-03-04 11:00:00   \n",
       "20740035          Yakutat   25382 2023-03-04 21:00:00 2023-03-04 12:00:00   \n",
       "\n",
       "          wind_1_5  wind_flag  \n",
       "0             0.00          3  \n",
       "1             0.00          3  \n",
       "2             0.00          3  \n",
       "3             0.00          3  \n",
       "4             0.00          3  \n",
       "...            ...        ...  \n",
       "20740031      0.00          0  \n",
       "20740032      0.00          0  \n",
       "20740033      0.02          0  \n",
       "20740034      0.24          0  \n",
       "20740035      0.07          0  \n",
       "\n",
       "[20740036 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.groupby(['station_location', 'wbanno', 'utc_datetime', 'lst_datetime','wind_flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    20345198\n",
       "3      394838\n",
       "Name: wind_flag, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['wind_flag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2[df2['wind_flag'] == 0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "70623b801652781c2389d9f74154af1ef3dd8a50bfe8b7cd6824c1648ddc5ad0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
