{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USCRN Data: High-Octane Scraping\n",
    "\n",
    "This notebook contains the initial scrape and upload to BigQuery for the USCRN historical weather data. The DAG contained in the file `airflow/dags/uscrn_dag.py` adds to the main data table created by this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import yaml \n",
    "import re\n",
    "import itertools\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open (\"../airflow/data/sources.yaml\", \"r\") as yaml_file:\n",
    "  sources = yaml.load(yaml_file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.) Column Headers and Descriptions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The station WBAN number. The UTC date of the observation. The UTC time of the observation. Time is the end of the observed hour, so the 0000 hour is actually the last hour of the previous day's observation (starting just after 11:00 PM through midnight). The Local Standard Time (LST) date of the observation. The Local Standard Time (LST) time of the observation. Time is the end of the observed hour (see UTC_TIME description). The version number of the station datalogger program that was in effect at the time of the observation. Note: This field should be treated as text (i.e. string). Station longitude, using WGS-84. Station latitude, using WGS-84. Average air temperature, in degrees C, during the last 5 minutes of the hour. See Note F. Average air temperature, in degrees C, for the entire hour. See Note F. Maximum air temperature, in degrees C, during the hour. See Note F. Minimum air temperature, in degrees C, during the hour. See Note F. Total amount of precipitation, in mm, recorded during the hour. See Note F. Average global solar radiation, in watts/meter^2. QC flag for average global solar radiation. See Note G. Maximum global solar radiation, in watts/meter^2. QC flag for maximum global solar radiation. See Note G. Minimum global solar radiation, in watts/meter^2. QC flag for minimum global solar radiation. See Note G. Type of infrared surface temperature measurement: 'R' denotes raw (uncorrected), 'C' denotes corrected, and 'U' when unknown/missing. See Note H. Average infrared surface temperature, in degrees C. See Note H. QC flag for infrared surface temperature. See Note G. Maximum infrared surface temperature, in degrees C. QC flag for infrared surface temperature maximum. See Note G. Minimum infrared surface temperature, in degrees C. QC flag for infrared surface temperature minimum. See Note G. RH average for hour, in percentage. See Note I. QC flag for RH average. See Note G. Average soil moisture at 5 cm below the surface, in m^3/m^3. See Note K. Average soil moisture at 10 cm below the surface, in m^3/m^3. See Note K. Average soil moisture at 20 cm below the surface, in m^3/m^3. See Note K. Average soil moisture at 50 cm below the surface, in m^3/m^3. See Note K. Average soil moisture at 100 cm below the surface, in m^3/m^3. See Note K. Average soil temperature at 5 cm below the surface, in degrees C. See Note K. Average soil temperature at 10 cm below the surface, in degrees C. See Note K. Average soil temperature at 20 cm below the surface, in degrees C. See Note K. Average soil temperature at 50 cm below the surface, in degrees C. See Note K. Average soil temperature at 100 cm below the surface, in degrees C. See Note K. \""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header_url = sources['USCRN']['headers']\n",
    "header_response = requests.get(header_url)\n",
    "header_soup = BeautifulSoup(header_response.content, \"html.parser\")\n",
    "\n",
    "columns = str(header_soup).split(\"\\n\")[1].strip(\" \").split(\" \")\n",
    "columns = list(map(lambda x: str.lower(x), columns)) # columns = [str.lower(c) for c in columns] -- faster?\n",
    "columns.insert(0,'station_location')\n",
    "\n",
    "descrip_text = str(header_soup).split(\"\\n\")[2] # raw text block containing column descriptions\n",
    "descrip_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The descriptions of the columns are quite the mess, as there is no standard separator used. We will have to work our way through it step by step: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_split = [re.sub(r'(\\([^)]*)$', r\"\\1)\", s) for s in descrip_text.split(\"). \")] # add ')' back after splitting text on ').' \n",
    "no_notes = [re.sub(r' See Note [A-Z]\\.',\"\",s) for s in first_split] # drop any references to notes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third entry in `no_notes` is ready. The last set of descriptions in `no_notes` can be split on `\". \"`, but the first two sets need special attention. We will pop the last set out and split it, then pop the third set out, and then address the first two sets. At that point we will recombine everything into one list while preserving the original order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_set = no_notes.pop().strip().split(\". \")\n",
    "third_set = no_notes.pop() # Note: just a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(ls:list): \n",
    "  return list(itertools.chain.from_iterable(ls)) \n",
    "\n",
    "no_notes = [re.sub(\". Time is\", \" at\", s) for s in no_notes]\n",
    "first_second = flatten([s.split(\". \") for s in no_notes])\n",
    "\n",
    "# Finally:\n",
    "descriptions = flatten([[\"Location name for USCRN station\"], first_second, [third_set], last_set]) # Description added for \"station_location\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_info = {\n",
    "  'col_name': columns,\n",
    "  'description': descriptions, \n",
    "  'units': [\"X...(Various Lengths)\", \"XXXXX\", \"YYYYMMDD\", \"HHmm\", \"YYYYMMDD\", \"HHmm\", \n",
    "  \"XXXXXX\",\"Decimal_degrees\", \"Decimal_degrees\", \"Celsius\", \"Celsius\", \"Celsius\", \"Celsius\", \n",
    "  \"mm\", \"W/m^2\", \"X\", \"W/m^2\", \"X\", \"W/m^2\", \"X\", \"X\", \"Celsius\", \"X\", \"Celsius\", \"X\", \n",
    "  \"Celsius\", \"X\", \"%\", \"X\", \"m^3/m^3\", \"m^3/m^3\", \"m^3/m^3\", \"m^3/m^3\", \"m^3/m^3\", \"Celsius\", \n",
    "  \"Celsius\", \"Celsius\", \"Celsius\", \"Celsius\"] \n",
    "  # See: https://www.ncei.noaa.gov/pub/data/uscrn/products/hourly02/readme.txt\n",
    "  # Contained in: /airflow/data/sources.yaml\n",
    "}\n",
    "\n",
    "header_df = pd.DataFrame(header_info)\n",
    "# header_df.to_csv(\"../airflow/data/column_descriptions.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.) Main Data (>2 million rows)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main data we're interested in is stored in a nested series of linked HTML pages accessible from [this](https://www.ncei.noaa.gov/pub/data/uscrn/products/hourly02/) index page. \n",
    "\n",
    "Pandas has a neat function for reading HTML tables to dataframes (`pd.read_html`). Unfortunately it doesn't work for our present task since these tables are stored as loose text within body elements -- `pd.read_html` relies on explicit HTML table syntax to work. It also isn't ideal for iterating through lots of HTML pages like our task calls for: iteratively creating and appending dataframes is very slow given the size of dataframe objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = sources[\"USCRN\"][\"index\"]\n",
    "base_soup = BeautifulSoup(requests.get(base_url).content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileUrls(): \n",
    "  links = base_soup.find_all(\"a\") # 'links' in this notebook will refer to <a> elements, not urls\n",
    "  years = [str(x).zfill(1) for x in range(2000,2024)]\n",
    "  year_links = [link for link in links if link['href'].rstrip('/') in years]\n",
    "\n",
    "  file_urls = []\n",
    "  for year_link in year_links: \n",
    "    year_url = base_url + year_link.get(\"href\")\n",
    "    response = requests.get(year_url) \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    file_links = soup.find_all('a', href=re.compile(r'AK.*\\.txt'))\n",
    "    if file_links:\n",
    "      new_file_urls = [year_url + link.getText() for link in file_links]\n",
    "      file_urls.extend(new_file_urls)\n",
    "  return file_urls\n",
    "\n",
    "file_urls=getFileUrls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "regex = r\"([St.]*[A-Z][a-z]+_*[A-Za-z]*).*.txt\" \n",
    "for url in file_urls:\n",
    "  # Get location from url\n",
    "  file_name = re.search(regex, url).group(0)\n",
    "  station_location = re.sub(\"(_formerly_Barrow.*|_[0-9].*)\", \"\", file_name)\n",
    "  # Get results, add station location\n",
    "  response = requests.get(url)\n",
    "  soup = BeautifulSoup(response.content,'html.parser')\n",
    "  soup_lines = [station_location + \" \" + line for line in str(soup).strip().split(\"\\n\")]\n",
    "  new_rows = [re.split('\\s+', row) for row in soup_lines]\n",
    "  # Add to list\n",
    "  rows.extend(new_rows)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your PC setup, working with large dataframes can crash your kernel (it did in my case). I recommended that you save this dataframe as a .csv after we make it in order to save your progress. That way you can re-read the dataframe rather than re-running previous cells of work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rows, columns=columns) \n",
    "# df.to_csv(\"../airflow/data/uscrn.csv\", index=False)\n",
    "# df = pd.read_csv(\"../airflow/data/uscrn.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_From the original data source [README](https://www.ncei.noaa.gov/pub/data/uscrn/products/hourly02/readme.txt):_  \n",
    "\n",
    "_\"Missing data are indicated by the lowest possible integer for a given column format, such as -9999.0 for 7-character fields with one decimal place or -99.000 for 7-character fields with three decimal places.\"_\n",
    "\n",
    "We can find these missing value indicators by getting the min of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'station_location': 'Aleknagik', 'wbanno': '23583', 'utc_date': '20230217', 'utc_time': '0000', 'lst_date': '20230217', 'lst_time': '1100', 'crx_vn': '2.424', 'longitude': '-131.59', 'latitude': '55.05', 't_calc': '-0.1', 't_hr_avg': '-0.3', 't_max': '-0.1', 't_min': '-0.1', 'p_calc': '-9999.0', 'solarad': '0', 'solarad_flag': '0', 'solarad_max': '0', 'solarad_max_flag': '0', 'solarad_min': '0', 'solarad_min_flag': '0', 'sur_temp_type': 'C', 'sur_temp': '-0.1', 'sur_temp_flag': '0', 'sur_temp_max': '-0.1', 'sur_temp_max_flag': '0', 'sur_temp_min': '-0.1', 'sur_temp_min_flag': '0', 'rh_hr_avg': '15', 'rh_hr_avg_flag': '0', 'soil_moisture_5': '-99.000', 'soil_moisture_10': '-99.000', 'soil_moisture_20': '-99.000', 'soil_moisture_50': '-99.000', 'soil_moisture_100': '-99.000', 'soil_temp_5': '-0.1', 'soil_temp_10': '-0.1', 'soil_temp_20': '-9999.0', 'soil_temp_50': '-9999.0', 'soil_temp_100': '-9999.0'}\n"
     ]
    }
   ],
   "source": [
    "def minMap(df):\n",
    "    min_values = {}\n",
    "    for col in df.columns:\n",
    "        mv = df[col].min()\n",
    "        min_values[col] = mv\n",
    "    return min_values\n",
    "\n",
    "print(minMap(df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will replace these values with `NaNs`, but we need to be careful: since the source does not normally have empty records, any `NaNs` entering our pipeline on read will likely come either from errors in the data source or errors in our attempts to read from it. When writing our update DAG, before we replace any values with `NaNs` we'll need to check for `NaNs` and log an alert if any are found. (**TO-DO**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dataframe(df):\n",
    "  # replace missing value designators\n",
    "  df.replace([-99999,-9999], np.nan, inplace=True) # Can safely assume these are always missing values in every column they appear in\n",
    "  df = df.filter(regex=\"^((?!soil).)*$\") # vast majority of soil columns have missing data\n",
    "  df.replace({'crx_vn':{-9:np.nan}}, inplace=True)\n",
    "\n",
    "  # convert to datetimes\n",
    "  df['utc_datetime'] = pd.to_datetime(df['utc_date'].astype(int).astype(str) + df['utc_time'].astype(int).astype(str).str.zfill(4), format='%Y%m%d%H%M')\n",
    "  df['lst_datetime'] = pd.to_datetime(df['lst_date'].astype(int).astype(str) + df['lst_time'].astype(int).astype(str).str.zfill(4), format='%Y%m%d%H%M')\n",
    "\n",
    "  # drop old date and time columns\n",
    "  df.drop(['utc_date', 'utc_time', 'lst_date', 'lst_time'], axis=1, inplace=True)\n",
    "\n",
    "  # reorder columns \n",
    "  cols = ['station_location','wbanno','crx_vn','utc_datetime','lst_datetime'] + list(df.columns)[3:-2]\n",
    "  df = df[cols]\n",
    "\n",
    "  # add date-added column\n",
    "  df['date_added_utc'] = datetime.utcnow() \n",
    "\n",
    "  return df \n",
    "  \n",
    "df = transform_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_location</th>\n",
       "      <th>wbanno</th>\n",
       "      <th>crx_vn</th>\n",
       "      <th>utc_datetime</th>\n",
       "      <th>lst_datetime</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>t_calc</th>\n",
       "      <th>t_hr_avg</th>\n",
       "      <th>t_max</th>\n",
       "      <th>...</th>\n",
       "      <th>sur_temp_type</th>\n",
       "      <th>sur_temp</th>\n",
       "      <th>sur_temp_flag</th>\n",
       "      <th>sur_temp_max</th>\n",
       "      <th>sur_temp_max_flag</th>\n",
       "      <th>sur_temp_min</th>\n",
       "      <th>sur_temp_min_flag</th>\n",
       "      <th>rh_hr_avg</th>\n",
       "      <th>rh_hr_avg_flag</th>\n",
       "      <th>date_added_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2040214</th>\n",
       "      <td>Metlakatla</td>\n",
       "      <td>25381.0</td>\n",
       "      <td>2.424</td>\n",
       "      <td>2022-12-05 03:00:00</td>\n",
       "      <td>2022-12-04 18:00:00</td>\n",
       "      <td>-131.59</td>\n",
       "      <td>55.05</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>...</td>\n",
       "      <td>C</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-02-17 22:33:04.077133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989977</th>\n",
       "      <td>Glennallen</td>\n",
       "      <td>56401.0</td>\n",
       "      <td>2.515</td>\n",
       "      <td>2022-03-11 22:00:00</td>\n",
       "      <td>2022-03-11 13:00:00</td>\n",
       "      <td>-145.50</td>\n",
       "      <td>63.03</td>\n",
       "      <td>-6.6</td>\n",
       "      <td>-6.4</td>\n",
       "      <td>-6.2</td>\n",
       "      <td>...</td>\n",
       "      <td>C</td>\n",
       "      <td>-4.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-02-17 22:33:04.077133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2155364</th>\n",
       "      <td>Sand_Point</td>\n",
       "      <td>25630.0</td>\n",
       "      <td>2.424</td>\n",
       "      <td>2023-02-10 15:00:00</td>\n",
       "      <td>2023-02-10 06:00:00</td>\n",
       "      <td>-160.47</td>\n",
       "      <td>55.35</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>...</td>\n",
       "      <td>C</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-02-17 22:33:04.077133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131917</th>\n",
       "      <td>Utqiagvik</td>\n",
       "      <td>27516.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>2007-06-26 03:00:00</td>\n",
       "      <td>2007-06-25 18:00:00</td>\n",
       "      <td>-156.61</td>\n",
       "      <td>71.32</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>...</td>\n",
       "      <td>R</td>\n",
       "      <td>8.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-02-17 22:33:04.077133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492562</th>\n",
       "      <td>St._Paul</td>\n",
       "      <td>25711.0</td>\n",
       "      <td>2.424</td>\n",
       "      <td>2019-06-22 07:00:00</td>\n",
       "      <td>2019-06-21 22:00:00</td>\n",
       "      <td>-170.21</td>\n",
       "      <td>57.16</td>\n",
       "      <td>8.5</td>\n",
       "      <td>8.7</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>C</td>\n",
       "      <td>9.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-02-17 22:33:04.077133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        station_location   wbanno  crx_vn        utc_datetime  \\\n",
       "2040214       Metlakatla  25381.0   2.424 2022-12-05 03:00:00   \n",
       "1989977       Glennallen  56401.0   2.515 2022-03-11 22:00:00   \n",
       "2155364       Sand_Point  25630.0   2.424 2023-02-10 15:00:00   \n",
       "131917         Utqiagvik  27516.0   1.301 2007-06-26 03:00:00   \n",
       "1492562         St._Paul  25711.0   2.424 2019-06-22 07:00:00   \n",
       "\n",
       "               lst_datetime  longitude  latitude  t_calc  t_hr_avg  t_max  \\\n",
       "2040214 2022-12-04 18:00:00    -131.59     55.05     0.2       0.2    0.3   \n",
       "1989977 2022-03-11 13:00:00    -145.50     63.03    -6.6      -6.4   -6.2   \n",
       "2155364 2023-02-10 06:00:00    -160.47     55.35     2.6       2.6    2.8   \n",
       "131917  2007-06-25 18:00:00    -156.61     71.32     2.2       2.4    2.8   \n",
       "1492562 2019-06-21 22:00:00    -170.21     57.16     8.5       8.7    9.0   \n",
       "\n",
       "         ...  sur_temp_type  sur_temp  sur_temp_flag  sur_temp_max  \\\n",
       "2040214  ...              C      -0.8            0.0          -0.7   \n",
       "1989977  ...              C      -4.8            0.0          -4.8   \n",
       "2155364  ...              C       2.0            0.0           2.1   \n",
       "131917   ...              R       8.7            0.0          10.2   \n",
       "1492562  ...              C       9.1            0.0           9.5   \n",
       "\n",
       "         sur_temp_max_flag  sur_temp_min  sur_temp_min_flag  rh_hr_avg  \\\n",
       "2040214                0.0          -0.9                0.0       74.0   \n",
       "1989977                0.0          -4.8                0.0       78.0   \n",
       "2155364                0.0           1.8                0.0        NaN   \n",
       "131917                 0.0           6.8                0.0        NaN   \n",
       "1492562                0.0           8.7                0.0       89.0   \n",
       "\n",
       "        rh_hr_avg_flag             date_added_utc  \n",
       "2040214            0.0 2023-02-17 22:33:04.077133  \n",
       "1989977            0.0 2023-02-17 22:33:04.077133  \n",
       "2155364            0.0 2023-02-17 22:33:04.077133  \n",
       "131917             0.0 2023-02-17 22:33:04.077133  \n",
       "1492562            0.0 2023-02-17 22:33:04.077133  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"../airflow/data/uscrn.csv\", index=False <-- Save finalized form of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving location names from our dataframe: \n",
    "locations = df[['station_location', 'wbanno', 'longitude', 'latitude']].drop_duplicates()\n",
    "# locations.to_csv(\"../airflow/data/locations.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recursive Re-Run \n",
    "\n",
    "Here's a refactored version of the above code using recursion and batch processing. You can re-run this cell-block to re-scrape and save the data in a more efficient fashion.\n",
    "\n",
    "Better yet, this block is available as a separate script in `notebooks/uscrn_scrape.py`. The fastest way to run this would be to open a terminal instance outside of VSCode and run it from the command line: \n",
    "\n",
    "```bash\n",
    "\n",
    "/notebooks$ python3.7 uscrn_scrape.py\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd \n",
    "import yaml \n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open (\"../airflow/data/sources.yaml\", \"r\") as yaml_file:\n",
    "  sources = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
    "\n",
    "columns = pd.read_csv(\"../airflow/data/column_descriptions.csv\")['col_name']\n",
    "\n",
    "base_url = sources[\"USCRN\"][\"index\"]\n",
    "base_soup = BeautifulSoup(requests.get(base_url).content, \"html.parser\")\n",
    "\n",
    "def process_rows(file_urls, row_limit, output_file):\n",
    "    \n",
    "    # Get rows for current batch\n",
    "    rows = []\n",
    "    regex = r\"([St.]*[A-Z][a-z]+_*[A-Za-z]*).*.txt\" \n",
    "    current_idx=0\n",
    "    for i, url in enumerate(file_urls[current_idx:]):\n",
    "      # Get location from url\n",
    "      file_name = re.search(regex, url).group(0)\n",
    "      station_location = re.sub(\"(_formerly_Barrow.*|_[0-9].*)\", \"\", file_name)\n",
    "      # Get results, add station location\n",
    "      response = requests.get(url)\n",
    "      soup = BeautifulSoup(response.content, 'html.parser')\n",
    "      soup_lines = [station_location + \" \" + line for line in str(soup).strip().split(\"\\n\")]\n",
    "      new_rows = [re.split('\\s+', row) for row in soup_lines]\n",
    "      # Add to list\n",
    "      rows.extend(new_rows)\n",
    "      if len(rows) >= row_limit:\n",
    "        current_idx=i\n",
    "        break\n",
    "\n",
    "    # Create dataframe for current batch\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "    # Transform dataframe\n",
    "    df = transform_dataframe(df)\n",
    "\n",
    "    # Write dataframe to CSV\n",
    "    if os.path.isfile(output_file):\n",
    "        df.to_csv(output_file, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(output_file, index=False)\n",
    "    if len(rows) >= row_limit:\n",
    "        # Recursively process remaining rows\n",
    "        remaining_urls = file_urls[current_idx:]\n",
    "        process_rows(remaining_urls, row_limit, output_file)\n",
    "    else:\n",
    "        rows.clear()\n",
    "\n",
    "process_rows(file_urls = getFileUrls(), row_limit=100000, output_file=\"../airflow/data/uscrn.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.) Upload to BigQuery "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"../airflow/data/uscrn.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'team-week3:alaska' successfully created.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "bq mk -d --location=us-east4 team-week3:alaska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadJob<project=team-week3, location=us-east4, id=8ce78ae7-e09e-43ed-989c-974ac9b52b51>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Setting certain numeric columns (e.g. crx_vn, the flag columns) as strings will indicate that they are not meant to have arithmetic calculations done on them\n",
    "schema = [\n",
    "  bigquery.SchemaField(\"station_location\", \"STRING\", mode=\"REQUIRED\"), \n",
    "  bigquery.SchemaField(\"wbanno\", \"STRING\", mode=\"REQUIRED\"), \n",
    "  bigquery.SchemaField(\"crx_vn\", \"STRING\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"utc_datetime\", \"DATETIME\", mode=\"REQUIRED\"), \n",
    "  bigquery.SchemaField(\"lst_datetime\", \"DATETIME\", mode=\"REQUIRED\"), \n",
    "  bigquery.SchemaField(\"longitude\", \"FLOAT\", mode=\"REQUIRED\"), \n",
    "  bigquery.SchemaField(\"latitude\", \"FLOAT\", mode=\"REQUIRED\"), \n",
    "  bigquery.SchemaField(\"t_calc\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"t_hr_avg\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"t_max\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"t_min\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"p_calc\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"solarad\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"solarad_flag\", \"STRING\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"solarad_max\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"solarad_max_flag\", \"STRING\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"solarad_min\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"solarad_min_flag\", \"STRING\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"sur_temp_type\", \"STRING\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"sur_temp\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"sur_temp_flag\", \"STRING\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"sur_temp_max\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"sur_temp_max_flag\", \"STRING\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"sur_temp_min\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"sur_temp_min_flag\", \"STRING\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"rh_hr_avg\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"rh_hr_avg_flag\", \"STRING\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"date_added_utc\", \"DATETIME\", mode=\"REQUIRED\")\n",
    "]\n",
    "\n",
    "key_path = \"/home/alex/.creds/alex-sa-tw3.json\"\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "   key_path, scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    ")\n",
    "\n",
    "client = bigquery.Client(credentials=credentials, project=credentials.project_id)\n",
    "\n",
    "table_id = f\"{credentials.project_id}.alaska.uscrn\"\n",
    "\n",
    "jc = bigquery.LoadJobConfig(\n",
    "   source_format = bigquery.SourceFormat.CSV,\n",
    "   autodetect=False,\n",
    "   schema=schema,\n",
    "   create_disposition=\"CREATE_IF_NEEDED\",\n",
    "   write_disposition=\"WRITE_TRUNCATE\", \n",
    "   destination_table_description=\"Historical weather data from USCRN stations in Alaska\"\n",
    ")\n",
    "\n",
    "job = client.load_table_from_dataframe(df, table_id, job_config=jc)\n",
    "\n",
    "job.result()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supplemental Data** (Locations and Column Description tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadJob<project=team-week3, location=us-east4, id=4394a74c-7da4-4d32-a3a9-811b40954a97>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Locations table\n",
    "table_id = f\"{credentials.project_id}.alaska.locations\"\n",
    "\n",
    "jc = bigquery.LoadJobConfig(\n",
    "  source_format = bigquery.SourceFormat.CSV,\n",
    "  autodetect=True,\n",
    "  create_disposition=\"CREATE_IF_NEEDED\",\n",
    "  write_disposition=\"WRITE_TRUNCATE\", \n",
    "  destination_table_description=\"Location names, WBANNO codes, and coordinates for USCRN stations in Alaska\"\n",
    ")\n",
    "\n",
    "with open(\"../airflow/data/locations.csv\", \"rb\") as fp: \n",
    "  job = client.load_table_from_file(fp, table_id, job_config=jc)\n",
    "job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadJob<project=team-week3, location=us-east4, id=5b735679-6080-479c-9175-6a95cc755735>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Column description table \n",
    "table_id = f\"{credentials.project_id}.alaska.column_descriptions\"\n",
    "\n",
    "schema = [ # Col headers not being autodetected\n",
    "  bigquery.SchemaField(\"col_name\", \"STRING\", mode=\"REQUIRED\"), \n",
    "  bigquery.SchemaField(\"description\", \"STRING\", mode=\"REQUIRED\"), \n",
    "  bigquery.SchemaField(\"units\", \"STRING\", mode=\"REQUIRED\"),\n",
    "  \n",
    "]\n",
    "jc = bigquery.LoadJobConfig(\n",
    "  source_format = bigquery.SourceFormat.CSV,\n",
    "  skip_leading_rows=1,\n",
    "  autodetect=False,\n",
    "  create_disposition=\"CREATE_IF_NEEDED\",\n",
    "  write_disposition=\"WRITE_TRUNCATE\", \n",
    "  destination_table_description=\"Column descriptions for fields in alaska.uscrn table\", \n",
    "  schema=schema\n",
    ")\n",
    "\n",
    "with open(\"../airflow/data/column_descriptions.csv\", \"rb\") as fp: \n",
    "  job = client.load_table_from_file(fp, table_id, job_config=jc)\n",
    "job.result()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2aaadf1bc696f0a460085bdecc593864560e074d547b0a9d06cbe82d2cb977eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
