{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USCRN Data: High-Octane Scraping\n",
    "\n",
    "This notebook explains and contains the initial scrape, transform, and upload of the USCRN data to BigQuery. The DAG contained in `airflow/dags/uscrn_dag.py` is set to periodically update the BigQuery table created by this notebook, as is the cloud function in `gcf/uscrn_update_cf.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from yaml import full_load\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open (\"../config/sources.yaml\", \"r\") as fp:\n",
    "  sources = full_load(fp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.) Column Headers and Descriptions\n",
    "\n",
    "To save on storage space, USCRN omits column names in its main data tables and stores them in a separate text file ([headers.txt](https://www.ncei.noaa.gov/pub/data/uscrn/products/hourly02/headers.txt)). We'll scrape these first before tackling the main data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The station WBAN number. The UTC date of the observation. The UTC time of the observation. Time is the end of the observed hour, so the 0000 hour is actually the last hour of the previous day's observation (starting just after 11:00 PM through midnight). The Local Standard Time (LST) date of the observation. The Local Standard Time (LST) time of the observation. Time is the end of the observed hour (see UTC_TIME description). The version number of the station datalogger program that was in effect at the time of the observation. Note: This field should be treated as text (i.e. string). Station longitude, using WGS-84. Station latitude, using WGS-84. Average air temperature, in degrees C, during the last 5 minutes of the hour. See Note F. Average air temperature, in degrees C, for the entire hour. See Note F. Maximum air temperature, in degrees C, during the hour. See Note F. Minimum air temperature, in degrees C, during the hour. See Note F. Total amount of precipitation, in mm, recorded during the hour. See Note F. Average global solar radiation, in watts/meter^2. QC flag for average global solar radiation. See Note G. Maximum global solar radiation, in watts/meter^2. QC flag for maximum global solar radiation. See Note G. Minimum global solar radiation, in watts/meter^2. QC flag for minimum global solar radiation. See Note G. Type of infrared surface temperature measurement: 'R' denotes raw (uncorrected), 'C' denotes corrected, and 'U' when unknown/missing. See Note H. Average infrared surface temperature, in degrees C. See Note H. QC flag for infrared surface temperature. See Note G. Maximum infrared surface temperature, in degrees C. QC flag for infrared surface temperature maximum. See Note G. Minimum infrared surface temperature, in degrees C. QC flag for infrared surface temperature minimum. See Note G. RH average for hour, in percentage. See Note I. QC flag for RH average. See Note G. Average soil moisture at 5 cm below the surface, in m^3/m^3. See Note K. Average soil moisture at 10 cm below the surface, in m^3/m^3. See Note K. Average soil moisture at 20 cm below the surface, in m^3/m^3. See Note K. Average soil moisture at 50 cm below the surface, in m^3/m^3. See Note K. Average soil moisture at 100 cm below the surface, in m^3/m^3. See Note K. Average soil temperature at 5 cm below the surface, in degrees C. See Note K. Average soil temperature at 10 cm below the surface, in degrees C. See Note K. Average soil temperature at 20 cm below the surface, in degrees C. See Note K. Average soil temperature at 50 cm below the surface, in degrees C. See Note K. Average soil temperature at 100 cm below the surface, in degrees C. See Note K. \""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = sources['USCRN']['headers']\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "columns = str(soup).split(\"\\n\")[1].strip(\" \").split(\" \")\n",
    "columns = [str.lower(c) for c in columns] \n",
    "columns.insert(0,'station_location')\n",
    "\n",
    "descrip_text = str(soup).split(\"\\n\")[2] # raw text block containing column descriptions\n",
    "descrip_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The descriptions of the columns are quite the mess, as there is no standard separator used. We will have to work our way through it step by step: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The station WBAN number. The UTC date of the observation. The UTC time of the observation. Time is the end of the observed hour, so the 0000 hour is actually the last hour of the previous day's observation (starting just after 11:00 PM through midnight)\",\n",
       " 'The Local Standard Time (LST) date of the observation. The Local Standard Time (LST) time of the observation. Time is the end of the observed hour (see UTC_TIME description)',\n",
       " 'The version number of the station datalogger program that was in effect at the time of the observation. Note: This field should be treated as text (i.e. string)',\n",
       " \"Station longitude, using WGS-84. Station latitude, using WGS-84. Average air temperature, in degrees C, during the last 5 minutes of the hour. Average air temperature, in degrees C, for the entire hour. Maximum air temperature, in degrees C, during the hour. Minimum air temperature, in degrees C, during the hour. Total amount of precipitation, in mm, recorded during the hour. Average global solar radiation, in watts/meter^2. QC flag for average global solar radiation. Maximum global solar radiation, in watts/meter^2. QC flag for maximum global solar radiation. Minimum global solar radiation, in watts/meter^2. QC flag for minimum global solar radiation. Type of infrared surface temperature measurement: 'R' denotes raw (uncorrected), 'C' denotes corrected, and 'U' when unknown/missing. Average infrared surface temperature, in degrees C. QC flag for infrared surface temperature. Maximum infrared surface temperature, in degrees C. QC flag for infrared surface temperature maximum. Minimum infrared surface temperature, in degrees C. QC flag for infrared surface temperature minimum. RH average for hour, in percentage. QC flag for RH average. Average soil moisture at 5 cm below the surface, in m^3/m^3. Average soil moisture at 10 cm below the surface, in m^3/m^3. Average soil moisture at 20 cm below the surface, in m^3/m^3. Average soil moisture at 50 cm below the surface, in m^3/m^3. Average soil moisture at 100 cm below the surface, in m^3/m^3. Average soil temperature at 5 cm below the surface, in degrees C. Average soil temperature at 10 cm below the surface, in degrees C. Average soil temperature at 20 cm below the surface, in degrees C. Average soil temperature at 50 cm below the surface, in degrees C. Average soil temperature at 100 cm below the surface, in degrees C. \"]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def close_parens(s:str):\n",
    "    \"\"\"uses regex to replace closing parenthesis ')' after it's removed from .split()\"\"\"\n",
    "    unclosed_paren = re.compile(r'(\\([^)]*)$') \n",
    "    return re.sub(unclosed_paren, r\"\\1)\", s) \n",
    "\n",
    "first_split = map(close_parens, descrip_text.split(\"). \"))\n",
    "\n",
    "no_notes = [re.sub(r' See Note [A-Z]\\.',\"\",s) for s in first_split]\n",
    "no_notes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third entry in `no_notes` is ready (it's a single string belonging to a single column). The last set of descriptions in `no_notes` can be split on `\". \"`, but the first two sets need special attention. We will pop the last set out and split it, then pop the third set out, and then address the first two sets. At that point we will recombine everything into one list while preserving the original order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_set = no_notes.pop().strip().split(\". \")\n",
    "third_set = no_notes.pop() # just a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Location name for USCRN station',\n",
       " 'The station WBAN number',\n",
       " 'The UTC date of the observation',\n",
       " \"The UTC time of the observation at the end of the observed hour, so the 0000 hour is actually the last hour of the previous day's observation (starting just after 11:00 PM through midnight)\",\n",
       " 'The Local Standard Time (LST) date of the observation']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flatten(ls:list): \n",
    "  \"\"\"Flattens/unnests a list of lists\"\"\"\n",
    "  return list(itertools.chain.from_iterable(ls)) \n",
    "\n",
    "no_notes = [re.sub(\". Time is\", \" at\", s) for s in no_notes] # rephrase description so we can split on sentences\n",
    "\n",
    "first_second = flatten([s.split(\". \") for s in no_notes]) \n",
    "\n",
    "# Finally:\n",
    "descriptions = flatten([first_second, [third_set], last_set]) \n",
    "descriptions.insert(0,\"Location name for USCRN station\") # Description added for \"station_location\" \n",
    "descriptions[0:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [readme](https://www.ncei.noaa.gov/pub/data/uscrn/products/hourly02/readme.txt) also contains information on the units of each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1    WBANNO                         XXXXX',\n",
       " '2    UTC_DATE                       YYYYMMDD',\n",
       " '3    UTC_TIME                       HHmm',\n",
       " '4    LST_DATE                       YYYYMMDD',\n",
       " '5    LST_TIME                       HHmm',\n",
       " '6    CRX_VN                         XXXXXX',\n",
       " '7    LONGITUDE                      Decimal_degrees',\n",
       " '8    LATITUDE                       Decimal_degrees',\n",
       " '9    T_CALC                         Celsius',\n",
       " '10   T_HR_AVG                       Celsius',\n",
       " '11   T_MAX                          Celsius',\n",
       " '12   T_MIN                          Celsius',\n",
       " '13   P_CALC                         mm',\n",
       " '14   SOLARAD                        W/m^2',\n",
       " '15   SOLARAD_FLAG                   X',\n",
       " '16   SOLARAD_MAX                    W/m^2',\n",
       " '17   SOLARAD_MAX_FLAG               X',\n",
       " '18   SOLARAD_MIN                    W/m^2',\n",
       " '19   SOLARAD_MIN_FLAG               X',\n",
       " '20   SUR_TEMP_TYPE                  X',\n",
       " '21   SUR_TEMP                       Celsius',\n",
       " '22   SUR_TEMP_FLAG                  X',\n",
       " '23   SUR_TEMP_MAX                   Celsius',\n",
       " '24   SUR_TEMP_MAX_FLAG              X',\n",
       " '25   SUR_TEMP_MIN                   Celsius',\n",
       " '26   SUR_TEMP_MIN_FLAG              X',\n",
       " '27   RH_HR_AVG                      %',\n",
       " '28   RH_HR_AVG_FLAG                 X',\n",
       " '29   SOIL_MOISTURE_5                m^3/m^3',\n",
       " '30   SOIL_MOISTURE_10               m^3/m^3',\n",
       " '31   SOIL_MOISTURE_20               m^3/m^3',\n",
       " '32   SOIL_MOISTURE_50               m^3/m^3',\n",
       " '33   SOIL_MOISTURE_100              m^3/m^3',\n",
       " '34   SOIL_TEMP_5                    Celsius',\n",
       " '35   SOIL_TEMP_10                   Celsius',\n",
       " '36   SOIL_TEMP_20                   Celsius',\n",
       " '37   SOIL_TEMP_50                   Celsius',\n",
       " '38   SOIL_TEMP_100                  Celsius']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = sources['USCRN']['readme']\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "lines = [line.strip() for line in str(soup).split(\"\\n\")]\n",
    "table_idx = lines.index(\"Field#  Name                           Units\") # 252\n",
    "table = lines[table_idx+2:table_idx+40]\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X+ (Various Lengths)', 'XXXXX', 'YYYYMMDD', 'HHmm', 'YYYYMMDD', 'HHmm', 'XXXXXX', 'Decimal_degrees', 'Decimal_degrees', 'Celsius', 'Celsius', 'Celsius', 'Celsius', 'mm', 'W/m^2', 'X', 'W/m^2', 'X', 'W/m^2', 'X', 'X', 'Celsius', 'X', 'Celsius', 'X', 'Celsius', 'X', '%', 'X', 'm^3/m^3', 'm^3/m^3', 'm^3/m^3', 'm^3/m^3', 'm^3/m^3', 'Celsius', 'Celsius', 'Celsius', 'Celsius', 'Celsius']\n"
     ]
    }
   ],
   "source": [
    "# Find with lookbehind\n",
    "regex = re.compile(r\"(?<=\\s{5})[^\\s']+\") # enough \\s to exclude column names\n",
    "units = re.findall(regex, str(table))\n",
    "\n",
    "units.insert(0, \"X+ (Various Lengths)\") \n",
    "print(units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_name</th>\n",
       "      <th>description</th>\n",
       "      <th>units</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>station_location</td>\n",
       "      <td>Location name for USCRN station</td>\n",
       "      <td>X+ (Various Lengths)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wbanno</td>\n",
       "      <td>The station WBAN number</td>\n",
       "      <td>XXXXX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>utc_date</td>\n",
       "      <td>The UTC date of the observation</td>\n",
       "      <td>YYYYMMDD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>utc_time</td>\n",
       "      <td>The UTC time of the observation at the end of ...</td>\n",
       "      <td>HHmm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lst_date</td>\n",
       "      <td>The Local Standard Time (LST) date of the obse...</td>\n",
       "      <td>YYYYMMDD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           col_name                                        description  \\\n",
       "0  station_location                    Location name for USCRN station   \n",
       "1            wbanno                            The station WBAN number   \n",
       "2          utc_date                    The UTC date of the observation   \n",
       "3          utc_time  The UTC time of the observation at the end of ...   \n",
       "4          lst_date  The Local Standard Time (LST) date of the obse...   \n",
       "\n",
       "                  units  \n",
       "0  X+ (Various Lengths)  \n",
       "1                 XXXXX  \n",
       "2              YYYYMMDD  \n",
       "3                  HHmm  \n",
       "4              YYYYMMDD  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header_info = {\n",
    "  'col_name': columns,\n",
    "  'description': descriptions, \n",
    "  'units': units\n",
    "}\n",
    "header_df = pd.DataFrame(header_info)\n",
    "\n",
    "header_df.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.) Main Data (>2 million rows)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least for me, bringing a dataframe with more than 2 million rows into memory risks crashing the Jupyter IPython kernel. The code from this section to scrape, transform, and save the main data to .csv is available as a helper script in our current directory (`notebooks/uscrn_scrape.py`\n",
    "). I've refactored it with recursion and batch processing to reduce memory load. \n",
    "\n",
    "I've included the main function from that script in the next cell. You can run it here, but it might be best to execute the script from the terminal (`$ python3.7 uscrn_scrape.py`). Refer to the rest of the script to see what the helper functions are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from uscrn_scrape import get_station_location, transform_dataframe, get_file_urls\n",
    "\n",
    "output_file=\"../data/uscrn.csv\"\n",
    "\n",
    "if os.path.isfile(output_file):\n",
    "  raise Exception(f\"{output_file} already exists\")\n",
    "\n",
    "def process_rows(file_urls, row_limit, output_file) -> None:\n",
    "  \"\"\"\n",
    "  Processes a batch of rows from a list of URLs to extract weather station data and save it to a CSV file.\n",
    "\n",
    "  Args:\n",
    "    file_urls (list): A list of URLs where weather station data can be found.\n",
    "    row_limit (int): The maximum number of rows to process per batch.\n",
    "    output_file (str): The path to the output CSV file.\n",
    "  Returns:\n",
    "    None\n",
    "  \"\"\"\n",
    "\n",
    "  # Define column names for dataframes\n",
    "  columns = ['station_location','wbanno','utc_date','utc_time','lst_date','lst_time','crx_vn','longitude','latitude',\n",
    "  't_calc','t_hr_avg','t_max','t_min','p_calc','solarad','solarad_flag','solarad_max','solarad_max_flag','solarad_min',\n",
    "  'solarad_min_flag','sur_temp_type','sur_temp','sur_temp_flag','sur_temp_max','sur_temp_max_flag','sur_temp_min',\n",
    "  'sur_temp_min_flag','rh_hr_avg','rh_hr_avg_flag','soil_moisture_5','soil_moisture_10','soil_moisture_20',\n",
    "  'soil_moisture_50','soil_moisture_100','soil_temp_5','soil_temp_10','soil_temp_20','soil_temp_50','soil_temp_100']\n",
    "\n",
    "  # Get rows for current batch\n",
    "  rows = []\n",
    "  current_idx=0\n",
    "  for i, url in enumerate(file_urls[current_idx:]):\n",
    "    # Get location from url\n",
    "    station_location = get_station_location(url)\n",
    "    # Get new rows \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    soup_lines = [station_location + \" \" + line for line in str(soup).strip().split(\"\\n\")]\n",
    "    new_rows = [re.split('\\s+', row) for row in soup_lines]\n",
    "    # Add to list\n",
    "    rows.extend(new_rows)\n",
    "    if len(rows) >= row_limit:\n",
    "      current_idx=i\n",
    "      break\n",
    "\n",
    "    # Create dataframe for current batch\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "    # Transform dataframe\n",
    "    df = transform_dataframe(df)\n",
    "\n",
    "    # Write dataframe to CSV\n",
    "    if os.path.isfile(output_file):\n",
    "        df.to_csv(output_file, mode='a', header=False, index=False)\n",
    "    else:\n",
    "      with open(output_file, \"w\") as fp:\n",
    "        df.to_csv(fp, index=False)\n",
    "    \n",
    "    # Recursively process remaining rows     \n",
    "    if len(rows) >= row_limit:\n",
    "        remaining_urls = file_urls[current_idx:]\n",
    "        process_rows(remaining_urls, row_limit, output_file)\n",
    "    else: \n",
    "        return \n",
    "    \n",
    "  process_rows(file_urls=get_file_urls(), row_limit=100000, output_file=output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save location information\n",
    "cols = ['station_location', 'wbanno', 'longitude', 'latitude']\n",
    "locations = pd.read_csv(\"../data/uscrn.csv\", usecols=cols)\n",
    "locations.drop_duplicates(inplace=True)\n",
    "locations.reset_index(drop=True, inplace=True)\n",
    "locations.to_csv(\"../data/locations.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    " \n",
    "The main data we're interested in is stored in a nested series of linked HTML pages accessible from [this](https://www.ncei.noaa.gov/pub/data/uscrn/products/hourly02/) index page. \n",
    "\n",
    "Pandas has a neat function for reading HTML tables to dataframes (`pd.read_html`). Unfortunately it doesn't work for our present task since these tables are stored as loose text within body elements -- `pd.read_html` relies on explicit HTML table syntax to work. It also isn't ideal for iterating through lots of HTML pages like our task calls for: iteratively creating and appending dataframes is very slow given the size of dataframe objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = sources[\"USCRN\"][\"index\"]\n",
    "base_soup = BeautifulSoup(requests.get(base_url).content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_urls(): \n",
    "  \"\"\"Retrieves the URLs for every file contained on each year's page\"\"\"\n",
    "  links = base_soup.find_all(\"a\") # 'links' in this notebook will refer to <a> elements, not urls\n",
    "  years = [str(x).zfill(1) for x in range(2000,2024)]\n",
    "  year_urls = [base_url + link['href'] for link in links if link['href'].rstrip('/') in years]\n",
    " \n",
    "  file_urls = []\n",
    "  for url in year_urls: \n",
    "    response = requests.get(url) \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    file_links = soup.find_all('a', href=re.compile(r'AK.*\\.txt'))\n",
    "    if file_links:\n",
    "      new_file_urls = [url + link.getText() for link in file_links]\n",
    "      file_urls.extend(new_file_urls)\n",
    "  return file_urls\n",
    "\n",
    "rows = []\n",
    "regex = r\"([St.]*[A-Z][a-z]+_*[A-Za-z]*).*.txt\" \n",
    "for url in get_file_urls():\n",
    "  # Get location from url\n",
    "  file_name = re.search(regex, url).group(0)\n",
    "  station_location = re.sub(\"(_formerly_Barrow.*|_[0-9].*)\", \"\", file_name)\n",
    "  # Get results, add station location\n",
    "  response = requests.get(url)\n",
    "  soup = BeautifulSoup(response.content,'html.parser')\n",
    "  soup_lines = [station_location + \" \" + line for line in str(soup).strip().split(\"\\n\")]\n",
    "  new_rows = [re.split('\\s+', row) for row in soup_lines]\n",
    "  # Add to list\n",
    "  rows.extend(new_rows)\n",
    "\n",
    "### DANGER ZONE ### \n",
    "# df = pd.DataFrame(rows, columns=columns) \n",
    "# df.to_csv(\"../data/uscrn.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_From the original data source [README](https://www.ncei.noaa.gov/pub/data/uscrn/products/hourly02/readme.txt):_  \n",
    "\n",
    "_\"Missing data are indicated by the lowest possible integer for a given column format, such as -9999.0 for 7-character fields with one decimal place or -99.000 for 7-character fields with three decimal places.\"_\n",
    "\n",
    "We can find these missing value indicators by getting the min of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'station_location': 'Aleknagik', 'wbanno': '23583', 'utc_date': '20230217', 'utc_time': '0000', 'lst_date': '20230217', 'lst_time': '1100', 'crx_vn': '2.424', 'longitude': '-131.59', 'latitude': '55.05', 't_calc': '-0.1', 't_hr_avg': '-0.3', 't_max': '-0.1', 't_min': '-0.1', 'p_calc': '-9999.0', 'solarad': '0', 'solarad_flag': '0', 'solarad_max': '0', 'solarad_max_flag': '0', 'solarad_min': '0', 'solarad_min_flag': '0', 'sur_temp_type': 'C', 'sur_temp': '-0.1', 'sur_temp_flag': '0', 'sur_temp_max': '-0.1', 'sur_temp_max_flag': '0', 'sur_temp_min': '-0.1', 'sur_temp_min_flag': '0', 'rh_hr_avg': '15', 'rh_hr_avg_flag': '0', 'soil_moisture_5': '-99.000', 'soil_moisture_10': '-99.000', 'soil_moisture_20': '-99.000', 'soil_moisture_50': '-99.000', 'soil_moisture_100': '-99.000', 'soil_temp_5': '-0.1', 'soil_temp_10': '-0.1', 'soil_temp_20': '-9999.0', 'soil_temp_50': '-9999.0', 'soil_temp_100': '-9999.0'}\n"
     ]
    }
   ],
   "source": [
    "def minMap(df):\n",
    "    min_values = {}\n",
    "    for col in df.columns:\n",
    "        mv = df[col].min()\n",
    "        min_values[col] = mv\n",
    "    return min_values\n",
    "\n",
    "print(minMap(df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will replace these values with `NaNs`, but we need to be careful: since the source does not normally have empty records, any `NaNs` entering our pipeline on read will likely come either from errors in the data source or errors in our attempts to read from it. When writing our update DAG, before we replace any values with `NaNs` we'll need to check for `NaNs` and log an alert if any are found. (**TO-DO**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace missing value designators\n",
    "df.replace([-99999,-9999], np.nan, inplace=True) # Can safely assume these are always missing values in every column they appear in\n",
    "df = df.filter(regex=\"^((?!soil).)*$\") # vast majority of soil columns have missing data\n",
    "df.replace({'crx_vn':{-9:np.nan}}, inplace=True)\n",
    "\n",
    "# convert to datetimes\n",
    "df['utc_datetime'] = pd.to_datetime(df['utc_date'].astype(int).astype(str) + df['utc_time'].astype(int).astype(str).str.zfill(4), format='%Y%m%d%H%M')\n",
    "df['lst_datetime'] = pd.to_datetime(df['lst_date'].astype(int).astype(str) + df['lst_time'].astype(int).astype(str).str.zfill(4), format='%Y%m%d%H%M')\n",
    "\n",
    "# drop old date and time columns\n",
    "df.drop(['utc_date', 'utc_time', 'lst_date', 'lst_time'], axis=1, inplace=True)\n",
    "\n",
    "# reorder columns \n",
    "cols = ['station_location','wbanno','crx_vn','utc_datetime','lst_datetime'] + list(df.columns)[3:-2]\n",
    "df = df[cols]\n",
    "\n",
    "# add date-added column\n",
    "df['date_added_utc'] = datetime.utcnow() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_location</th>\n",
       "      <th>wbanno</th>\n",
       "      <th>crx_vn</th>\n",
       "      <th>utc_datetime</th>\n",
       "      <th>lst_datetime</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>t_calc</th>\n",
       "      <th>t_hr_avg</th>\n",
       "      <th>t_max</th>\n",
       "      <th>...</th>\n",
       "      <th>sur_temp_type</th>\n",
       "      <th>sur_temp</th>\n",
       "      <th>sur_temp_flag</th>\n",
       "      <th>sur_temp_max</th>\n",
       "      <th>sur_temp_max_flag</th>\n",
       "      <th>sur_temp_min</th>\n",
       "      <th>sur_temp_min_flag</th>\n",
       "      <th>rh_hr_avg</th>\n",
       "      <th>rh_hr_avg_flag</th>\n",
       "      <th>date_added_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2040214</th>\n",
       "      <td>Metlakatla</td>\n",
       "      <td>25381.0</td>\n",
       "      <td>2.424</td>\n",
       "      <td>2022-12-05 03:00:00</td>\n",
       "      <td>2022-12-04 18:00:00</td>\n",
       "      <td>-131.59</td>\n",
       "      <td>55.05</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>...</td>\n",
       "      <td>C</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-02-17 22:33:04.077133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989977</th>\n",
       "      <td>Glennallen</td>\n",
       "      <td>56401.0</td>\n",
       "      <td>2.515</td>\n",
       "      <td>2022-03-11 22:00:00</td>\n",
       "      <td>2022-03-11 13:00:00</td>\n",
       "      <td>-145.50</td>\n",
       "      <td>63.03</td>\n",
       "      <td>-6.6</td>\n",
       "      <td>-6.4</td>\n",
       "      <td>-6.2</td>\n",
       "      <td>...</td>\n",
       "      <td>C</td>\n",
       "      <td>-4.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-02-17 22:33:04.077133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2155364</th>\n",
       "      <td>Sand_Point</td>\n",
       "      <td>25630.0</td>\n",
       "      <td>2.424</td>\n",
       "      <td>2023-02-10 15:00:00</td>\n",
       "      <td>2023-02-10 06:00:00</td>\n",
       "      <td>-160.47</td>\n",
       "      <td>55.35</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>...</td>\n",
       "      <td>C</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-02-17 22:33:04.077133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131917</th>\n",
       "      <td>Utqiagvik</td>\n",
       "      <td>27516.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>2007-06-26 03:00:00</td>\n",
       "      <td>2007-06-25 18:00:00</td>\n",
       "      <td>-156.61</td>\n",
       "      <td>71.32</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>...</td>\n",
       "      <td>R</td>\n",
       "      <td>8.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-02-17 22:33:04.077133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492562</th>\n",
       "      <td>St._Paul</td>\n",
       "      <td>25711.0</td>\n",
       "      <td>2.424</td>\n",
       "      <td>2019-06-22 07:00:00</td>\n",
       "      <td>2019-06-21 22:00:00</td>\n",
       "      <td>-170.21</td>\n",
       "      <td>57.16</td>\n",
       "      <td>8.5</td>\n",
       "      <td>8.7</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>C</td>\n",
       "      <td>9.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-02-17 22:33:04.077133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        station_location   wbanno  crx_vn        utc_datetime  \\\n",
       "2040214       Metlakatla  25381.0   2.424 2022-12-05 03:00:00   \n",
       "1989977       Glennallen  56401.0   2.515 2022-03-11 22:00:00   \n",
       "2155364       Sand_Point  25630.0   2.424 2023-02-10 15:00:00   \n",
       "131917         Utqiagvik  27516.0   1.301 2007-06-26 03:00:00   \n",
       "1492562         St._Paul  25711.0   2.424 2019-06-22 07:00:00   \n",
       "\n",
       "               lst_datetime  longitude  latitude  t_calc  t_hr_avg  t_max  \\\n",
       "2040214 2022-12-04 18:00:00    -131.59     55.05     0.2       0.2    0.3   \n",
       "1989977 2022-03-11 13:00:00    -145.50     63.03    -6.6      -6.4   -6.2   \n",
       "2155364 2023-02-10 06:00:00    -160.47     55.35     2.6       2.6    2.8   \n",
       "131917  2007-06-25 18:00:00    -156.61     71.32     2.2       2.4    2.8   \n",
       "1492562 2019-06-21 22:00:00    -170.21     57.16     8.5       8.7    9.0   \n",
       "\n",
       "         ...  sur_temp_type  sur_temp  sur_temp_flag  sur_temp_max  \\\n",
       "2040214  ...              C      -0.8            0.0          -0.7   \n",
       "1989977  ...              C      -4.8            0.0          -4.8   \n",
       "2155364  ...              C       2.0            0.0           2.1   \n",
       "131917   ...              R       8.7            0.0          10.2   \n",
       "1492562  ...              C       9.1            0.0           9.5   \n",
       "\n",
       "         sur_temp_max_flag  sur_temp_min  sur_temp_min_flag  rh_hr_avg  \\\n",
       "2040214                0.0          -0.9                0.0       74.0   \n",
       "1989977                0.0          -4.8                0.0       78.0   \n",
       "2155364                0.0           1.8                0.0        NaN   \n",
       "131917                 0.0           6.8                0.0        NaN   \n",
       "1492562                0.0           8.7                0.0       89.0   \n",
       "\n",
       "        rh_hr_avg_flag             date_added_utc  \n",
       "2040214            0.0 2023-02-17 22:33:04.077133  \n",
       "1989977            0.0 2023-02-17 22:33:04.077133  \n",
       "2155364            0.0 2023-02-17 22:33:04.077133  \n",
       "131917             0.0 2023-02-17 22:33:04.077133  \n",
       "1492562            0.0 2023-02-17 22:33:04.077133  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save finalized form of dataframe\n",
    "# df.to_csv(\"../data/uscrn.csv\", index=False \n",
    "\n",
    "## Save location information\n",
    "# locations = df[['station_location', 'wbanno', 'longitude', 'latitude']].drop_duplicates()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.) Upload to BigQuery "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'alaska-scrape:weather' successfully created.\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/bq-config.yaml\", \"r\") as fp:\n",
    "  bq_config = full_load(fp) \n",
    "\n",
    "!bq mk -d --location=us-east4 {bq_config['project-id']}:{bq_config['dataset-id']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadJob<project=team-week3, location=us-east4, id=8ce78ae7-e09e-43ed-989c-974ac9b52b51>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "\n",
    "# Setting certain numeric columns (e.g. crx_vn, the flag columns) as strings will indicate that they are not meant to have arithmetic calculations done on them\n",
    "schema = [\n",
    "  bigquery.SchemaField(\"station_location\", \"STRING\", mode=\"REQUIRED\"), \n",
    "  bigquery.SchemaField(\"wbanno\", \"STRING\", mode=\"REQUIRED\"), \n",
    "  bigquery.SchemaField(\"crx_vn\", \"STRING\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"utc_datetime\", \"DATETIME\", mode=\"REQUIRED\"), \n",
    "  bigquery.SchemaField(\"lst_datetime\", \"DATETIME\", mode=\"REQUIRED\"), \n",
    "  bigquery.SchemaField(\"longitude\", \"FLOAT\", mode=\"REQUIRED\"), \n",
    "  bigquery.SchemaField(\"latitude\", \"FLOAT\", mode=\"REQUIRED\"), \n",
    "  bigquery.SchemaField(\"t_calc\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"t_hr_avg\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"t_max\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"t_min\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"p_calc\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"solarad\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"solarad_flag\", \"STRING\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"solarad_max\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"solarad_max_flag\", \"STRING\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"solarad_min\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"solarad_min_flag\", \"STRING\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"sur_temp_type\", \"STRING\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"sur_temp\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"sur_temp_flag\", \"STRING\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"sur_temp_max\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"sur_temp_max_flag\", \"STRING\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"sur_temp_min\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"sur_temp_min_flag\", \"STRING\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"rh_hr_avg\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"rh_hr_avg_flag\", \"STRING\", mode=\"NULLABLE\"), \n",
    "  bigquery.SchemaField(\"date_added_utc\", \"DATETIME\", mode=\"REQUIRED\")\n",
    "]\n",
    "\n",
    "key_path = bq_config['credentials']\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "   key_path, scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    ")\n",
    "\n",
    "client = bigquery.Client(credentials=credentials, project=credentials.project_id)\n",
    "\n",
    "table_id = f\"{credentials.project_id}.alaska.uscrn\"\n",
    "\n",
    "jc = bigquery.LoadJobConfig(\n",
    "   source_format = bigquery.SourceFormat.CSV,\n",
    "   autodetect=False,\n",
    "   schema=schema,\n",
    "   create_disposition=\"CREATE_IF_NEEDED\",\n",
    "   write_disposition=\"WRITE_TRUNCATE\", \n",
    "   destination_table_description=\"Historical weather data from USCRN stations in Alaska\"\n",
    ")\n",
    "\n",
    "job = client.load_table_from_dataframe(df, table_id, job_config=jc)\n",
    "\n",
    "job.result()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supplemental Data** (Locations and Column Description tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadJob<project=team-week3, location=us-east4, id=4394a74c-7da4-4d32-a3a9-811b40954a97>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Locations table\n",
    "table_id = f\"{credentials.project_id}.alaska.locations\"\n",
    "\n",
    "jc = bigquery.LoadJobConfig(\n",
    "  source_format = bigquery.SourceFormat.CSV,\n",
    "  autodetect=True,\n",
    "  create_disposition=\"CREATE_IF_NEEDED\",\n",
    "  write_disposition=\"WRITE_TRUNCATE\", \n",
    "  destination_table_description=\"Location names, WBANNO codes, and coordinates for USCRN stations in Alaska\"\n",
    ")\n",
    "\n",
    "with open(\"../data/locations.csv\", \"rb\") as fp: \n",
    "  job = client.load_table_from_file(fp, table_id, job_config=jc)\n",
    "job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadJob<project=team-week3, location=us-east4, id=5b735679-6080-479c-9175-6a95cc755735>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column description table \n",
    "table_id = f\"{credentials.project_id}.alaska.column_descriptions\"\n",
    "\n",
    "schema = [ # Col headers not being autodetected\n",
    "  bigquery.SchemaField(\"col_name\", \"STRING\", mode=\"REQUIRED\"), \n",
    "  bigquery.SchemaField(\"description\", \"STRING\", mode=\"REQUIRED\"), \n",
    "  bigquery.SchemaField(\"units\", \"STRING\", mode=\"REQUIRED\"),\n",
    "  \n",
    "]\n",
    "jc = bigquery.LoadJobConfig(\n",
    "  source_format = bigquery.SourceFormat.CSV,\n",
    "  skip_leading_rows=1,\n",
    "  autodetect=False,\n",
    "  create_disposition=\"CREATE_IF_NEEDED\",\n",
    "  write_disposition=\"WRITE_TRUNCATE\", \n",
    "  destination_table_description=\"Column descriptions for fields in alaska.uscrn table\", \n",
    "  schema=schema\n",
    ")\n",
    "\n",
    "with open(\"../data/column_descriptions.csv\", \"rb\") as fp: \n",
    "  job = client.load_table_from_file(fp, table_id, job_config=jc)\n",
    "job.result()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "70623b801652781c2389d9f74154af1ef3dd8a50bfe8b7cd6824c1648ddc5ad0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
