{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NWS Data  \n",
    "\n",
    "This notebook shows how to create and host an automated web-scraping script in Google Cloud Functions (GCF), repurposed from an Airflow DAG (`../airflow/nws_dag.py`). The GCF package directory this notebook creates (`notebooks/nws_update_gcf/`) is already included in this repo, so there is no need for you to run the code in the first two sections. The third will show how to upload the package to GCF.\n",
    "\n",
    "---\n",
    "\n",
    "Airflow is extremely useful for coordinating complex pipelines in distributed systems and can be run in the cloud with [Google Cloud Composer](https://cloud.google.com/composer). The downside with this approach is that the increased overhead with Airflow results in a more expensive project. By contrast, GCF is dedicated to simple stateless functions and is much more affordable. Thankfully, our use case is simple enough for GCF to handle. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.) Create directory for GCF scripts\n",
    "\n",
    "First we need to make a project directory which we can upload to GCF. We will make it in our current directory `notebooks`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nws_update_gcf\n",
      "├── main.py\n",
      "├── README.md\n",
      "├── requirements.txt\n",
      "└── utils\n",
      "    ├── __init__.py\n",
      "    └── utils.py\n",
      "\n",
      "1 directory, 5 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cp: -r not specified; omitting directory '../../airflow/dags/utils/__pycache__'\n"
     ]
    }
   ],
   "source": [
    "# %%bash -- I've already run this for you  \n",
    "# mkdir nws_update_gcf && cd $_\n",
    "# mkdir utils\n",
    "# touch main.py\n",
    "# touch README.md requirements.txt \n",
    "# cp ../../airflow/dags/utils/* utils/\n",
    "\n",
    "# echo \"Script to scrape, transform, and upload NWS forecast information to BigQuery\" >> README.md\n",
    "\n",
    "# echo \"# This file is required to mark the 'utils' directory as a package\" >> utils/__init__.py\n",
    "\n",
    "# # The GCF container will automatically install any dependencies contained in requirements.txt.\n",
    "# echo -e \"datetime\\npandas\\nnumpy\\nrequests\\nbeautifulsoup4\\ngoogle-cloud-bigquery\\ngoogle-cloud-storage\\ngoogle-cloud-logging\\ngoogle-auth\\ngoogle-auth-oauthlib\\ngoogle-auth-httplib2\" > requirements.txt\n",
    "\n",
    "# cd .. && tree nws_update_gcf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.) Refactor Airflow Script\n",
    "\n",
    "Next we modify our Airflow DAG to work as a regular python script, which we will add to `main.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "import io \n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "# GCP imports: \n",
    "from google.cloud import bigquery, storage, logging as cloud_logging \n",
    "from google.oauth2 import service_account\n",
    "# Utils\n",
    "from nws_update_gcf.utils.utils import nws_url, get_table, table_to_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon creating a function in Google Cloud Functions, it is automatically associated with a single Google Cloud project. When creating various GCP client (e.g. `storage.Client()`) we would not have to specify the project or our credentials. We define these here just to test-run our code locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating GCP Connection \n",
    "from yaml import full_load\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "with open(\"../config/gcp-config.yaml\", \"r\") as fp: \n",
    "    gcp_config = full_load(fp)\n",
    "\n",
    "PROJECT_ID = gcp_config[\"project-id\"]\n",
    "DATASET_ID = gcp_config[\"dataset-id\"]\n",
    "\n",
    "key_path = gcp_config[\"credentials\"]\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "   key_path, scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    "\n",
    ")\n",
    "\n",
    "## ---------- LOGGING ---------- ## \n",
    "# Cloud logging client\n",
    "logger_client = cloud_logging.Client(credentials=credentials, project=credentials.project_id)\n",
    "\n",
    "# Cloud logging handler\n",
    "handler = logger_client.get_default_handler()\n",
    "\n",
    "# Create logger with cloud handler\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "# Set logging levels \n",
    "logger.setLevel(logging.INFO)\n",
    "handler.setLevel(logging.INFO)\n",
    "\n",
    "# Format logger \n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "# Confirm logger is working  \n",
    "logger.info(f\"Running daily scrape of NWS Weather Forecasts in Alaska\")\n",
    "\n",
    "## ---------- CLOUD STORAGE ---------- ## \n",
    "storage_client = storage.Client(credentials=credentials, project=credentials.project_id)\n",
    "bucket = storage_client.bucket(f\"{PROJECT_ID}-bucket\")\n",
    "\n",
    "# Locations \n",
    "blob = bucket.blob(\"locations.csv\")\n",
    "content = blob.download_as_bytes()\n",
    "locations_df = pd.read_csv(io.BytesIO(content))\n",
    "\n",
    "## ---------- BIGQUERY ---------- ## \n",
    "bq_client = bigquery.Client(credentials=credentials, project=credentials.project_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the actual code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_forecast_df() -> pd.DataFrame:\n",
    "  \"\"\"Get dataframe of forecast data for next 48 hours from various points in Alaska\"\"\"\n",
    "\n",
    "  logger.info(f\"Scraping forecast data\")\n",
    "\n",
    "  nws_urls = locations_df.apply(nws_url, axis=1)\n",
    "  loc_dict = dict(zip(locations_df['station_location'], nws_urls))\n",
    "\n",
    "  combined_table = []\n",
    "  for location, url in loc_dict.items():\n",
    "    result = requests.get(url)\n",
    "    soup = BeautifulSoup(result.content, \"html.parser\")\n",
    "\n",
    "    tr_list = soup.find_all(\"table\")[5].find_all(\"tr\") # <tr> elements\n",
    "    table = get_table(tr_list, location)   \n",
    "    combined_table.extend(table)\n",
    "  \n",
    "  return table_to_dict(combined_table)\n",
    "\n",
    "\n",
    "def transform_df(fcast_dict:dict) -> pd.DataFrame: \n",
    "  \"\"\"Cast dictionary from get_forecast() to a dataframe and transform\"\"\"\n",
    "  df = pd.DataFrame(fcast_dict)\n",
    "  df.columns = [col.lower() for col in df.columns] \n",
    "\n",
    "  # Check for missing values\n",
    "  empty_values = df.isna().any(axis=1)\n",
    "  if empty_values.any():\n",
    "    bad_rows = df[empty_values]\n",
    "    logging.warning(f\"When creating dataframe, failed to parse values in the following rows:\\n{bad_rows}\")\n",
    "    logging.info(f\"Dropping bad rows from dataframe\")\n",
    "\n",
    "\n",
    "  # Replace missing value indicators with Nan\n",
    "  df.replace({'':np.NaN, '--':np.NaN}, inplace=True)\n",
    "\n",
    "  ## Datetime Transformations\n",
    "  cur_year = datetime.now().year\n",
    "  dt_strings = df['date'] + '/' + str(cur_year) + ' ' + df['hour (akst)'] + ':00 AKST'\n",
    "  # Local time (AKST)\n",
    "  df['lst_datetime'] = pd.to_datetime(dt_strings, format='%m/%d/%Y %H:%M AKST')\n",
    "  # UTC time\n",
    "  akst_offset = timedelta(hours=9)\n",
    "  df['utc_datetime'] = df['lst_datetime'] + akst_offset\n",
    "\n",
    "  # Reorder columns \n",
    "  col_names = ['location','utc_datetime','lst_datetime'] + list(df.columns)[3:-2]\n",
    "  df = df[col_names]\n",
    "\n",
    "  # Timestamp column: track when forecast was accessed\n",
    "  df['date_added_utc'] = datetime.utcnow()\n",
    "\n",
    "  # Edit column headers \n",
    "  df.rename(columns=lambda x: re.sub('°|\\(|\\)', '', x), inplace=True)\n",
    "  df.rename(columns=lambda x: re.sub('%', 'pct', x), inplace=True)\n",
    "  df.rename(columns=lambda x: re.sub(' ', '_', x.strip()), inplace=True)\n",
    "\n",
    "  # Log result\n",
    "  logger.info(f\"Created dataframe: \")\n",
    "\n",
    "  return df \n",
    "\n",
    "\n",
    "def load_df_to_bq(df) -> None:\n",
    "  \"\"\"Load dataframe to BigQuery\"\"\"\n",
    "\n",
    "  # Set schema and job_config\n",
    "  schema = [\n",
    "    bigquery.SchemaField(\"location\", \"STRING\", mode=\"REQUIRED\"), \n",
    "    bigquery.SchemaField(\"utc_datetime\", \"DATETIME\", mode=\"REQUIRED\"), \n",
    "    bigquery.SchemaField(\"lst_datetime\", \"DATETIME\", mode=\"REQUIRED\"), \n",
    "    bigquery.SchemaField(\"temperature_f\", \"INTEGER\", mode=\"REQUIRED\"), \n",
    "    bigquery.SchemaField(\"dewpoint_f\", \"INTEGER\", mode=\"REQUIRED\"), \n",
    "    bigquery.SchemaField(\"wind_chill_f\", \"INTEGER\", mode=\"REQUIRED\"), \n",
    "    bigquery.SchemaField(\"surface_wind_mph\", \"INTEGER\", mode=\"REQUIRED\"), \n",
    "    bigquery.SchemaField(\"wind_dir\", \"STRING\", mode=\"REQUIRED\"), \n",
    "    bigquery.SchemaField(\"gust\", \"INTEGER\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"sky_cover_pct\", \"INTEGER\", mode=\"REQUIRED\"), \n",
    "    bigquery.SchemaField(\"precipitation_potential_pct\", \"FLOAT\", mode=\"REQUIRED\"), \n",
    "    bigquery.SchemaField(\"relative_humidity_pct\", \"FLOAT\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"rain\", \"STRING\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"thunder\", \"STRING\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"snow\", \"STRING\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"freezing_rain\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"sleet\", \"STRING\", mode=\"NULLABLE\")\n",
    "  ]\n",
    "  \n",
    "  jc = bigquery.LoadJobConfig(\n",
    "    source_format = bigquery.SourceFormat.CSV,\n",
    "    skip_leading_rows=1,\n",
    "    autodetect=False,\n",
    "    schema=schema,\n",
    "    create_disposition=\"CREATE_IF_NEEDED\",\n",
    "    write_disposition=\"WRITE_APPEND\"   \n",
    "  )\n",
    " \n",
    "  # Set target table in BigQuery\n",
    "  full_table_id = f\"{PROJECT_ID}.{DATASET_ID}.nws_forecasts\"\n",
    "\n",
    "  # Load from dataframe -- if any columns are missing, include names of column in the error message\n",
    "  try: \n",
    "    job = bq_client.load_table_from_dataframe(df, full_table_id, job_config=jc)\n",
    "    job.result()\n",
    "  except Exception as e:\n",
    "    error_message = str(e)\n",
    "    # modify error message to include the name of the missing column\n",
    "    if 'Required column value for column index' in error_message:\n",
    "      start_index = error_message.index('Required column value for column index') + len('Required column value for column index: ')\n",
    "      end_index = error_message.index(' is missing', start_index)\n",
    "      missing_column_index = int(error_message[start_index:end_index])\n",
    "      # get the name of the missing column based on its index\n",
    "      missing_column_name = list(df.columns)[missing_column_index]\n",
    "      # modify the error message to include the name of the missing column\n",
    "      error_message = error_message[:start_index] + f'{missing_column_name} ({missing_column_index})' + error_message[end_index:]\n",
    "    raise Exception(error_message) \n",
    "\n",
    "  # Log result \n",
    "  table = bq_client.get_table(full_table_id)\n",
    "  logger.info(f\"Loaded {table.num_rows} rows and {table.schema} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(get_forecast_df())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>location</th>\n",
       "      <th>Hour (AKST)</th>\n",
       "      <th>Temperature (°F)</th>\n",
       "      <th>Dewpoint (°F)</th>\n",
       "      <th>Wind Chill (°F)</th>\n",
       "      <th>Surface Wind (mph)</th>\n",
       "      <th>Wind Dir</th>\n",
       "      <th>Gust</th>\n",
       "      <th>Sky Cover (%)</th>\n",
       "      <th>Precipitation Potential (%)</th>\n",
       "      <th>Relative Humidity (%)</th>\n",
       "      <th>Rain</th>\n",
       "      <th>Thunder</th>\n",
       "      <th>Snow</th>\n",
       "      <th>Freezing Rain</th>\n",
       "      <th>Sleet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Date, location, Hour (AKST), Temperature (°F), Dewpoint (°F), Wind Chill (°F), Surface Wind (mph), Wind Dir, Gust, Sky Cover (%), Precipitation Potential (%), Relative Humidity (%), Rain, Thunder, Snow, Freezing Rain, Sleet]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_values = df2.isna().any(axis=1)\n",
    "if empty_values.any():\n",
    "  bad_rows = df2[empty_values]\n",
    "  # logging.warning(f\"When creating dataframe, failed to parse values in the following rows:\\n{bad_rows}\")\n",
    "  for index, row in bad_rows.iterrows():\n",
    "      cols_with_nans = list(row[row.isna()].index)\n",
    "      cols_with_nans\n",
    "      # log_data = {index: cols_with_nans}\n",
    "      # logging.warning(f\"When creating dataframe, failed to parse values in the following columns of row {index}:\\n{json.dumps(log_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>location</th>\n",
       "      <th>Hour (AKST)</th>\n",
       "      <th>Temperature (°F)</th>\n",
       "      <th>Gust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02/27</td>\n",
       "      <td>Fairbanks</td>\n",
       "      <td>10</td>\n",
       "      <td>-15</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02/27</td>\n",
       "      <td>Fairbanks</td>\n",
       "      <td>11</td>\n",
       "      <td>-14</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02/27</td>\n",
       "      <td>Fairbanks</td>\n",
       "      <td>12</td>\n",
       "      <td>-12</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02/27</td>\n",
       "      <td>Fairbanks</td>\n",
       "      <td>13</td>\n",
       "      <td>-10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>02/27</td>\n",
       "      <td>Fairbanks</td>\n",
       "      <td>14</td>\n",
       "      <td>-9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>03/01</td>\n",
       "      <td>Aleknagik</td>\n",
       "      <td>05</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>03/01</td>\n",
       "      <td>Aleknagik</td>\n",
       "      <td>06</td>\n",
       "      <td>22</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>03/01</td>\n",
       "      <td>Aleknagik</td>\n",
       "      <td>07</td>\n",
       "      <td>20</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1102</th>\n",
       "      <td>03/01</td>\n",
       "      <td>Aleknagik</td>\n",
       "      <td>08</td>\n",
       "      <td>19</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>03/01</td>\n",
       "      <td>Aleknagik</td>\n",
       "      <td>09</td>\n",
       "      <td>19</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1104 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date   location Hour (AKST) Temperature (°F) Gust\n",
       "0     02/27  Fairbanks          10              -15  NaN\n",
       "1     02/27  Fairbanks          11              -14  NaN\n",
       "2     02/27  Fairbanks          12              -12  NaN\n",
       "3     02/27  Fairbanks          13              -10  NaN\n",
       "4     02/27  Fairbanks          14               -9  NaN\n",
       "...     ...        ...         ...              ...  ...\n",
       "1099  03/01  Aleknagik          05               22  NaN\n",
       "1100  03/01  Aleknagik          06               22   29\n",
       "1101  03/01  Aleknagik          07               20   29\n",
       "1102  03/01  Aleknagik          08               19   29\n",
       "1103  03/01  Aleknagik          09               19   31\n",
       "\n",
       "[1104 rows x 5 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[['Date', 'location', 'Hour (AKST)', 'Temperature (°F)', 'Gust']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://forecast.weather.gov/MapClick.php?lat=64.97&lon=-147.51&unit=0&lg=english&FcstType=digital&menu=1'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(locations_df.apply(nws_url, axis=1)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "70623b801652781c2389d9f74154af1ef3dd8a50bfe8b7cd6824c1648ddc5ad0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
