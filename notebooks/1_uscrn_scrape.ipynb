{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USCRN Data: Historical Baseline\n",
    "\n",
    "This notebook explains and contains the initial scrape, transform, and upload of the USCRN weather data to BigQuery and Google Cloud Storage. This dataset will serve as the benchmark for the NWS forecast data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from yaml import full_load\n",
    "import datetime as dt\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open (\"../airflow/dags/config/sources.yaml\", \"r\") as fp:\n",
    "  sources = full_load(fp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.) Column Headers and Descriptions\n",
    "\n",
    "To save on storage space, USCRN omits column names in its main data tables and stores them in a separate text file ([headers.txt](https://www.ncei.noaa.gov/pub/data/uscrn/products/hourly02/headers.txt)). We'll scrape these first before tackling the main data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The station WBAN number. The UTC date of the observation. The UTC time of the observation. Time is the end of the observed hour, so the 0000 hour is actually the last hour of the previous day's observation (starting just after 11:00 PM through midnight). The Local Standard Time (LST) date of the observation. The Local Standard Time (LST) time of the observation. Time is the end of the observed hour (see UTC_TIME description). The version number of the station datalogger program that was in effect at the time of the observation. Note: This field should be treated as text (i.e. string). Station longitude, using WGS-84. Station latitude, using WGS-84. Average air temperature, in degrees C, during the last 5 minutes of the hour. See Note F. Average air temperature, in degrees C, for the entire hour. See Note F. Maximum air temperature, in degrees C, during the hour. See Note F. Minimum air temperature, in degrees C, during the hour. See Note F. Total amount of precipitation, in mm, recorded during the hour. See Note F. Average global solar radiation, in watts/meter^2. QC flag for average global solar radiation. See Note G. Maximum global solar radiation, in watts/meter^2. QC flag for maximum global solar radiation. See Note G. Minimum global solar radiation, in watts/meter^2. QC flag for minimum global solar radiation. See Note G. Type of infrared surface temperature measurement: 'R' denotes raw (uncorrected), 'C' denotes corrected, and 'U' when unknown/missing. See Note H. Average infrared surface temperature, in degrees C. See Note H. QC flag for infrared surface temperature. See Note G. Maximum infrared surface temperature, in degrees C. QC flag for infrared surface temperature maximum. See Note G. Minimum infrared surface temperature, in degrees C. QC flag for infrared surface temperature minimum. See Note G. RH average for hour, in percentage. See Note I. QC flag for RH average. See Note G. Average soil moisture at 5 cm below the surface, in m^3/m^3. See Note K. Average soil moisture at 10 cm below the surface, in m^3/m^3. See Note K. Average soil moisture at 20 cm below the surface, in m^3/m^3. See Note K. Average soil moisture at 50 cm below the surface, in m^3/m^3. See Note K. Average soil moisture at 100 cm below the surface, in m^3/m^3. See Note K. Average soil temperature at 5 cm below the surface, in degrees C. See Note K. Average soil temperature at 10 cm below the surface, in degrees C. See Note K. Average soil temperature at 20 cm below the surface, in degrees C. See Note K. Average soil temperature at 50 cm below the surface, in degrees C. See Note K. Average soil temperature at 100 cm below the surface, in degrees C. See Note K. \""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = sources['USCRN']['headers']\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "columns = str(soup).split(\"\\n\")[1].strip(\" \").split(\" \")\n",
    "columns = [str.lower(c) for c in columns] \n",
    "columns.insert(0,'station_location')\n",
    "\n",
    "descrip_text = str(soup).split(\"\\n\")[2] # raw text block containing column descriptions\n",
    "descrip_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The descriptions of the columns are quite the mess, as there is no standard separator used. We will have to work our way through it step by step: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The station WBAN number. The UTC date of the observation. The UTC time of the observation. Time is the end of the observed hour, so the 0000 hour is actually the last hour of the previous day's observation (starting just after 11:00 PM through midnight)\",\n",
       " 'The Local Standard Time (LST) date of the observation. The Local Standard Time (LST) time of the observation. Time is the end of the observed hour (see UTC_TIME description)',\n",
       " 'The version number of the station datalogger program that was in effect at the time of the observation. Note: This field should be treated as text (i.e. string)',\n",
       " \"Station longitude, using WGS-84. Station latitude, using WGS-84. Average air temperature, in degrees C, during the last 5 minutes of the hour. Average air temperature, in degrees C, for the entire hour. Maximum air temperature, in degrees C, during the hour. Minimum air temperature, in degrees C, during the hour. Total amount of precipitation, in mm, recorded during the hour. Average global solar radiation, in watts/meter^2. QC flag for average global solar radiation. Maximum global solar radiation, in watts/meter^2. QC flag for maximum global solar radiation. Minimum global solar radiation, in watts/meter^2. QC flag for minimum global solar radiation. Type of infrared surface temperature measurement: 'R' denotes raw (uncorrected), 'C' denotes corrected, and 'U' when unknown/missing. Average infrared surface temperature, in degrees C. QC flag for infrared surface temperature. Maximum infrared surface temperature, in degrees C. QC flag for infrared surface temperature maximum. Minimum infrared surface temperature, in degrees C. QC flag for infrared surface temperature minimum. RH average for hour, in percentage. QC flag for RH average. Average soil moisture at 5 cm below the surface, in m^3/m^3. Average soil moisture at 10 cm below the surface, in m^3/m^3. Average soil moisture at 20 cm below the surface, in m^3/m^3. Average soil moisture at 50 cm below the surface, in m^3/m^3. Average soil moisture at 100 cm below the surface, in m^3/m^3. Average soil temperature at 5 cm below the surface, in degrees C. Average soil temperature at 10 cm below the surface, in degrees C. Average soil temperature at 20 cm below the surface, in degrees C. Average soil temperature at 50 cm below the surface, in degrees C. Average soil temperature at 100 cm below the surface, in degrees C. \"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def close_parens(s:str):\n",
    "    \"\"\"uses regex to replace closing parenthesis ')' after it's removed from .split()\"\"\"\n",
    "    unclosed_paren = re.compile(r'(\\([^)]*)$') \n",
    "    return re.sub(unclosed_paren, r\"\\1)\", s) \n",
    "\n",
    "first_split = map(close_parens, descrip_text.split(\"). \"))\n",
    "\n",
    "no_notes = [re.sub(r' See Note [A-Z]\\.',\"\",s) for s in first_split]\n",
    "no_notes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third entry in `no_notes` is ready (it's a single string belonging to a single column). The last set of descriptions in `no_notes` can be split on `\". \"`, but the first two sets need special attention. We will pop the last set out and split it, then pop the third set out, and then address the first two sets. At that point we will recombine everything into one list while preserving the original order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_set = no_notes.pop().strip().split(\". \")\n",
    "third_set = no_notes.pop() # just a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Location name for USCRN station',\n",
       " 'The station WBAN number',\n",
       " 'The UTC date of the observation',\n",
       " \"The UTC time of the observation at the end of the observed hour, so the 0000 hour is actually the last hour of the previous day's observation (starting just after 11:00 PM through midnight)\",\n",
       " 'The Local Standard Time (LST) date of the observation']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flatten(ls:list): \n",
    "  \"\"\"Flattens/unnests a list of lists\"\"\"\n",
    "  return list(itertools.chain.from_iterable(ls)) \n",
    "\n",
    "no_notes = [re.sub(\". Time is\", \" at\", s) for s in no_notes] # rephrase description so we can split on sentences\n",
    "\n",
    "first_second = flatten([s.split(\". \") for s in no_notes]) \n",
    "\n",
    "# Finally:\n",
    "descriptions = flatten([first_second, [third_set], last_set]) \n",
    "descriptions.insert(0,\"Location name for USCRN station\") # Description added for \"station_location\" \n",
    "descriptions[0:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [readme](https://www.ncei.noaa.gov/pub/data/uscrn/products/hourly02/readme.txt) also contains information on the units of each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1    WBANNO                         XXXXX',\n",
       " '2    UTC_DATE                       YYYYMMDD',\n",
       " '3    UTC_TIME                       HHmm',\n",
       " '4    LST_DATE                       YYYYMMDD',\n",
       " '5    LST_TIME                       HHmm',\n",
       " '6    CRX_VN                         XXXXXX',\n",
       " '7    LONGITUDE                      Decimal_degrees',\n",
       " '8    LATITUDE                       Decimal_degrees',\n",
       " '9    T_CALC                         Fahrenheit',\n",
       " '10   T_HR_AVG                       Fahrenheit',\n",
       " '11   T_MAX                          Fahrenheit',\n",
       " '12   T_MIN                          Fahrenheit',\n",
       " '13   P_CALC                         mm',\n",
       " '14   SOLARAD                        W/m^2',\n",
       " '15   SOLARAD_FLAG                   X',\n",
       " '16   SOLARAD_MAX                    W/m^2',\n",
       " '17   SOLARAD_MAX_FLAG               X',\n",
       " '18   SOLARAD_MIN                    W/m^2',\n",
       " '19   SOLARAD_MIN_FLAG               X',\n",
       " '20   SUR_TEMP_TYPE                  X',\n",
       " '21   SUR_TEMP                       Fahrenheit',\n",
       " '22   SUR_TEMP_FLAG                  X',\n",
       " '23   SUR_TEMP_MAX                   Fahrenheit',\n",
       " '24   SUR_TEMP_MAX_FLAG              X',\n",
       " '25   SUR_TEMP_MIN                   Fahrenheit',\n",
       " '26   SUR_TEMP_MIN_FLAG              X',\n",
       " '27   RH_HR_AVG                      %',\n",
       " '28   RH_HR_AVG_FLAG                 X',\n",
       " '29   SOIL_MOISTURE_5                m^3/m^3',\n",
       " '30   SOIL_MOISTURE_10               m^3/m^3',\n",
       " '31   SOIL_MOISTURE_20               m^3/m^3',\n",
       " '32   SOIL_MOISTURE_50               m^3/m^3',\n",
       " '33   SOIL_MOISTURE_100              m^3/m^3',\n",
       " '34   SOIL_TEMP_5                    Fahrenheit',\n",
       " '35   SOIL_TEMP_10                   Fahrenheit',\n",
       " '36   SOIL_TEMP_20                   Fahrenheit',\n",
       " '37   SOIL_TEMP_50                   Fahrenheit',\n",
       " '38   SOIL_TEMP_100                  Fahrenheit']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = sources['USCRN']['readme']\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "lines = [line.strip() for line in str(soup).split(\"\\n\")]\n",
    "table_idx = lines.index(\"Field#  Name                           Units\") # 252\n",
    "table = lines[table_idx+2:table_idx+40]\n",
    "table = [re.sub(\"Celsius\", \"Fahrenheit\", l) for l in table]\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>units</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>station_location</td>\n",
       "      <td>Location name for USCRN station</td>\n",
       "      <td>X+ (Various Lengths)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wbanno</td>\n",
       "      <td>The station WBAN number</td>\n",
       "      <td>XXXXX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>utc_date</td>\n",
       "      <td>The UTC date of the observation</td>\n",
       "      <td>YYYYMMDD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>utc_time</td>\n",
       "      <td>The UTC time of the observation at the end of ...</td>\n",
       "      <td>HHmm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lst_date</td>\n",
       "      <td>The Local Standard Time (LST) date of the obse...</td>\n",
       "      <td>YYYYMMDD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name                                        description  \\\n",
       "0  station_location                    Location name for USCRN station   \n",
       "1            wbanno                            The station WBAN number   \n",
       "2          utc_date                    The UTC date of the observation   \n",
       "3          utc_time  The UTC time of the observation at the end of ...   \n",
       "4          lst_date  The Local Standard Time (LST) date of the obse...   \n",
       "\n",
       "                  units  \n",
       "0  X+ (Various Lengths)  \n",
       "1                 XXXXX  \n",
       "2              YYYYMMDD  \n",
       "3                  HHmm  \n",
       "4              YYYYMMDD  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get units with regex lookbehind\n",
    "regex = re.compile(r\"(?<=\\s{5})[^\\s']+\") # enough \\s to exclude column names\n",
    "units = re.findall(regex, str(table))\n",
    "\n",
    "# Add unit for \"station_location\" column\n",
    "units.insert(0, \"X+ (Various Lengths)\") \n",
    "\n",
    "header_info = {\n",
    "  'name': columns,\n",
    "  'description': descriptions, \n",
    "  'units': units\n",
    "}\n",
    "header_df = pd.DataFrame(header_info)\n",
    "\n",
    "header_df.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.) Main Data (>2 million rows)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might find that trying to create a dataframe from a nested list of 2 million rows risks crashing the Jupyter IPython Kernel. I've since refactored my code with recursion and batch processing to reduce memory load.\n",
    "\n",
    "The full script to scrape, transform, and save the data is available in `notebooks/uscrn_helper_scripts/uscrn_scrape_main.py`. I've imported the main function from there in the next cell so you can run it here. But it's likely better to execute the script in a separate terminal (`$ python3.7 uscrn_scrape.py`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "from utils.utils import get_station_location, get_soup, get_file_urls\n",
    "# Refer to the script to see what these helper functions are doing\n",
    "\n",
    "output_file=\"../airflow/dags/data/uscrn.csv\"\n",
    "\n",
    "columns = header_df['name']\n",
    "\n",
    "if os.path.isfile(output_file):\n",
    "  raise Exception(f\"{output_file} already exists\")\n",
    "\n",
    "def process_rows(file_urls, row_limit, output_file) -> None:\n",
    "  \"\"\"\n",
    "  Processes a batch of rows from a list of URLs to extract weather station data and save it to a CSV file.\n",
    "\n",
    "  Args:\n",
    "    file_urls (list): A list of URLs where weather station data can be found.\n",
    "    row_limit (int): The maximum number of rows to process per batch.\n",
    "    output_file (str): The path to the output CSV file.\n",
    "  Returns:\n",
    "    None\n",
    "  \"\"\"\n",
    "  # Get rows for current batch\n",
    "  rows = []\n",
    "  current_idx=0\n",
    "  for i, url in enumerate(file_urls[current_idx:]):\n",
    "    # Get location from url\n",
    "    station_location = get_station_location(url)\n",
    "    # Get new rows \n",
    "    soup = get_soup(url, delay=1)\n",
    "    soup_lines = [station_location + \" \" + line for line in str(soup).strip().split(\"\\n\")]\n",
    "    new_rows = [re.split('\\s+', row) for row in soup_lines]\n",
    "    # Add to list\n",
    "    rows.extend(new_rows)\n",
    "    if len(rows) >= row_limit:\n",
    "      current_idx=i\n",
    "      break\n",
    "\n",
    "  # Define column names -- same as from header_df['name']\n",
    "  columns = ['station_location','wbanno','utc_date','utc_time','lst_date','lst_time','crx_vn','longitude','latitude',\n",
    "  't_calc','t_hr_avg','t_max','t_min','p_calc','solarad','solarad_flag','solarad_max','solarad_max_flag','solarad_min',\n",
    "  'solarad_min_flag','sur_temp_type','sur_temp','sur_temp_flag','sur_temp_max','sur_temp_max_flag','sur_temp_min',\n",
    "  'sur_temp_min_flag','rh_hr_avg','rh_hr_avg_flag','soil_moisture_5','soil_moisture_10','soil_moisture_20',\n",
    "  'soil_moisture_50','soil_moisture_100','soil_temp_5','soil_temp_10','soil_temp_20','soil_temp_50','soil_temp_100']\n",
    "  \n",
    "  # Create dataframe for current batch\n",
    "  df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "  #### --- Transform dataframe --- #### \n",
    "\n",
    "  # Convert to fahrenheit \n",
    "\n",
    "  df['t_calc'] = df['t_calc'].astype(float)\n",
    "  df['t_hr_avg'] = df['t_hr_avg'].astype(float)\n",
    "  df['t_max'] = df['t_max'].astype(float)\n",
    "  df['t_min'] = df['t_min'].astype(float)\n",
    "\n",
    "  df[['t_calc','t_hr_avg', 't_max', 't_min']].apply(lambda celsius: np.where(celsius > -90, celsius * 9/5 + 32, celsius))\n",
    "\n",
    "  # Drop soil columns -- vast majority have missing data \n",
    "  df = df.filter(regex=\"^((?!soil).)*$\")\n",
    "\n",
    "  # convert to datetimes\n",
    "  df['utc_datetime'] = pd.to_datetime(df['utc_date'].astype(int).astype(str) + df['utc_time'].astype(int).astype(str).str.zfill(4), format='%Y%m%d%H%M')\n",
    "  df['lst_datetime'] = pd.to_datetime(df['lst_date'].astype(int).astype(str) + df['lst_time'].astype(int).astype(str).str.zfill(4), format='%Y%m%d%H%M')\n",
    "\n",
    "  # drop old date and time columns\n",
    "  df.drop(['utc_date', 'utc_time', 'lst_date', 'lst_time'], axis=1, inplace=True)\n",
    "\n",
    "  # reorder columns \n",
    "  cols = ['station_location','wbanno','crx_vn','utc_datetime','lst_datetime'] + list(df.columns)[3:-2]\n",
    "  df = df[cols]\n",
    "\n",
    "  # add date added column\n",
    "  df['date_added_utc'] = dt.datetime.utcnow()\n",
    "\n",
    "  #### -------------------------- #####\n",
    "\n",
    "  # Write dataframe to CSV\n",
    "  hdr = False if os.path.isfile(output_file) else True\n",
    "  df.to_csv(\"../airflow/dags/data/uscrn.csv\", mode=\"a+\", header=hdr, index=False)\n",
    "  del df\n",
    "  gc.collect()\n",
    "  \n",
    "  # Recursively process remaining rows     \n",
    "  if len(rows) >= row_limit:\n",
    "      remaining_urls = file_urls[current_idx:]\n",
    "      process_rows(remaining_urls, row_limit, output_file)\n",
    "  else: \n",
    "      return \n",
    "    \n",
    "process_rows(file_urls=get_file_urls(\"hourly02\"), row_limit=100000, output_file=output_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also save tables containing supplemental information:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Location information*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_cols = ['station_location', 'wbanno', 'longitude', 'latitude']\n",
    "df = pd.read_csv(\"../airflow/dags/data/uscrn.csv\", usecols=location_cols)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.to_csv(\"../airflow/dags/data/locations.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Column descriptions* -- we made `header_df` earlier but transformed the columns since. Easiest way to update this is to join with the transformed dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = pd.read_csv(\"../airflow/dags/data/uscrn.csv\", nrows=1)\n",
    "df = pd.DataFrame(list(names.columns), columns=[\"name\"]) # transposing \n",
    "\n",
    "# Join to column description dataframe we made in previous section\n",
    "df = df.merge(header_df, how=\"left\")\n",
    "\n",
    "# Adding information for datetime columns\n",
    "df['units'].fillna(\"YYYY-MM-DD HH:MM:SS\", inplace=True) \n",
    "df['description'][df['name'] == 'utc_datetime'] = \"UTC datetime of observation\"\n",
    "df['description'][df['name'] == 'lst_datetime'] = \"Local standard datetime of observation (AKST)\"\n",
    "df['description'][df['name'] == 'date_added_utc'] = \"Datetime added to usrcn.csv (UTC)\"\n",
    "\n",
    "# Add 'type' columns -- useful for setting table schema in next section\n",
    "def map_type(unit:str):\n",
    "  \"\"\"Map unit to datatype\"\"\"\n",
    "  if \"X\" in unit:\n",
    "    return \"STRING\"\n",
    "  elif \"Y\" in unit:\n",
    "    return \"DATETIME\"\n",
    "  else: \n",
    "    return \"FLOAT\"\n",
    "  \n",
    "df['type'] = df['units'].map(map_type)\n",
    "\n",
    "# Write to .csv\n",
    "df.to_csv(\"../airflow/dags/data/column_descriptions.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.) Wind Data (*20 million* rows!)\n",
    "\n",
    "For some reason, the USCRN hourly database (`hourly02`) does not include windspeed measurements. This information is only included in the sub-hourly database (`subhourly01`). \n",
    "\n",
    "The amount of the data is so large here that even the batch processing method we just used isn't enough to avoid exceeding memory limits -- the garbage collector is not adequately clearing the memory on every pass that's used from creating each dataframe. We instead write all the raw data directly to .csv before reading it back into memory and transforming it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def write_raw_rows(file_urls:list, output_file:str) -> None:\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    file_urls (list): List of text file urls\n",
    "    output_file (str): The path to the output CSV file.\n",
    "\n",
    "  Returns:\n",
    "    None\n",
    "  \"\"\"\n",
    "\n",
    "  if os.path.isfile(output_file):\n",
    "    raise Exception(f\"{output_file} already exists\")\n",
    "\n",
    "  for url in file_urls:\n",
    "    # Get location from url\n",
    "    station_location = get_station_location(url)\n",
    "    # Get new rows \n",
    "    soup = get_soup(url, delay=.5)\n",
    "    lines = [re.split('\\s+', line) for line in str(soup).strip().splitlines()]\n",
    "    # We're only scraping this data for the wind information, so we ignore rows that don't have any (i.e wind < 0)\n",
    "    wind_cols = [[station_location] + line[:5] + line[-2:] for line in lines if float(line[-2]) >= 0]\n",
    "    # Write rows to CSV\n",
    "    if wind_cols:\n",
    "      with open(output_file, \"a+\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(wind_cols)\n",
    "      del wind_cols\n",
    "\n",
    "subhourly_files = get_file_urls(\"subhourly01\")\n",
    "\n",
    "write_raw_rows(file_urls=subhourly_files, output_file=\"../airflow/dags/data/uscrn_wind_raw.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to aggregate the sub-hourly measurements by hour, we can't read the data in chunks (otherwise we will skew averages for hours that were partially read-in)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the data  \n",
    "colnames = ['station_location','wbanno','utc_date','utc_time',\n",
    "  'lst_date','lst_time',\"wind_1_5\", \"wind_flag\"]\n",
    "\n",
    "df = pd.read_csv(\"../airflow/dags/data/uscrn_wind_raw.csv\", names=colnames) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transformations\n",
    "df['wind_1_5'] = df['wind_1_5'].astype(float)\n",
    "\n",
    "# convert to datetimes\n",
    "df['utc_datetime'] = pd.to_datetime(df['utc_date'].astype(int).astype(str) + df['utc_time'].astype(int).astype(str).str.zfill(4), format='%Y%m%d%H%M')\n",
    "df['lst_datetime'] = pd.to_datetime(df['lst_date'].astype(int).astype(str) + df['lst_time'].astype(int).astype(str).str.zfill(4), format='%Y%m%d%H%M')\n",
    "\n",
    "# round to nearest hour \n",
    "df['utc_datetime'] = df['utc_datetime'].dt.floor(\"H\")\n",
    "df['lst_datetime'] = df['lst_datetime'].dt.floor(\"H\")\n",
    "\n",
    "# drop poor quality data (wind_flag == 3: roughly 1.9% of rows)\n",
    "df = df[df['wind_flag'] == 0]\n",
    "df.drop(\"wind_flag\", axis=1, inplace=True)\n",
    "\n",
    "# aggregate by hour\n",
    "df = df.groupby(['station_location','wbanno','utc_datetime','lst_datetime'])['wind_1_5'].mean().reset_index()\n",
    "df.rename({\"wind_1_5\":\"wind_hr_avg\"}, axis=1, inplace=True)\n",
    "\n",
    "# sort by date\n",
    "df.sort_values(\"utc_datetime\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write to csv\n",
    "df.to_csv(\"../airflow/dags/data/uscrn_wind_agg.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.) Upload to BigQuery "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset \n",
    "with open(\"../airflow/dags/config/gcp-config.yaml\", \"r\") as fp:\n",
    "  gcp_config = full_load(fp) \n",
    "# !bq mk -d --location={gcp_config['location']} {gcp_config['project-id']}:{gcp_config['dataset-id']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'station_location', 'description': 'Location name for USCRN station', 'type': 'STRING', 'mode': 'REQUIRED'}, {'name': 'wbanno', 'description': 'The station WBAN number', 'type': 'STRING', 'mode': 'REQUIRED'}, {'name': 'crx_vn', 'description': 'The version number of the station datalogger program that was in effect at the time of the observation. Note: This field should be treated as text (i.e. string)', 'type': 'STRING', 'mode': 'NULLABLE'}, {'name': 'utc_datetime', 'description': 'UTC datetime of observation', 'type': 'DATETIME', 'mode': 'REQUIRED'}, {'name': 'lst_datetime', 'description': 'Local standard datetime of observation (AKST)', 'type': 'DATETIME', 'mode': 'REQUIRED'}, {'name': 'longitude', 'description': 'Station longitude, using WGS-84', 'type': 'FLOAT', 'mode': 'REQUIRED'}]\n"
     ]
    }
   ],
   "source": [
    "# Set schema \n",
    "header_df = pd.read_csv(\"../airflow/dags/data/column_descriptions.csv\")\n",
    "header_df.drop(\"units\", axis=1, inplace=True)\n",
    "\n",
    "required_fields = ['station_location', 'wbanno', 'utc_datetime', 'lst_datetime', 'longitude', 'latitude', 'date_added_utc']\n",
    "\n",
    "header_df['mode'] = np.where(header_df['name'].isin(required_fields), \"REQUIRED\", \"NULLABLE\")\n",
    "\n",
    "schema = header_df.to_dict(orient='records')\n",
    "\n",
    "print(schema[0:6]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadJob<project=alaska-scrape, location=us-east4, id=74f9a78c-29ef-4efc-8ef8-19c597e7f13b>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "key_path = gcp_config['credentials']\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "   key_path, scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    ")\n",
    "\n",
    "bq_client = bigquery.Client(credentials=credentials, project=credentials.project_id)\n",
    "\n",
    "PROJECT_ID = gcp_config['project-id']\n",
    "DATASET_ID = gcp_config['dataset-id']\n",
    "table_id = f\"{PROJECT_ID}.{DATASET_ID}.uscrn\"\n",
    "\n",
    "jc = bigquery.LoadJobConfig(\n",
    "   source_format = bigquery.SourceFormat.CSV,\n",
    "   skip_leading_rows=1,\n",
    "   autodetect=False,\n",
    "   schema=schema,\n",
    "   create_disposition=\"CREATE_IF_NEEDED\",\n",
    "   write_disposition=\"WRITE_TRUNCATE\", \n",
    "   destination_table_description=\"Historical weather data from USCRN stations in Alaska\"\n",
    ")\n",
    "\n",
    "df = pd.read_csv(\"../airflow/dags/data/uscrn.csv\")\n",
    "df['date_added_utc'] = dt.datetime.utcnow()\n",
    "job = bq_client.load_table_from_dataframe(df, table_id, job_config=jc)\n",
    "\n",
    "job.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wind Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'station_location',\n",
       "  'description': 'Location name for USCRN station',\n",
       "  'type': 'STRING',\n",
       "  'mode': 'REQUIRED'},\n",
       " {'name': 'wbanno',\n",
       "  'description': 'The station WBAN number',\n",
       "  'type': 'STRING',\n",
       "  'mode': 'REQUIRED'},\n",
       " {'name': 'utc_datetime',\n",
       "  'description': 'UTC datetime of observation',\n",
       "  'type': 'DATETIME',\n",
       "  'mode': 'REQUIRED'},\n",
       " {'name': 'lst_datetime',\n",
       "  'description': 'Local standard datetime of observation (AKST)',\n",
       "  'type': 'DATETIME',\n",
       "  'mode': 'REQUIRED'},\n",
       " {'name': 'wind_hr_avg',\n",
       "  'description': 'Average wind speed over the course of an hour',\n",
       "  'type': 'FLOAT',\n",
       "  'mode': 'NULLABLE'}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wind_hr_avg = {'name':'wind_hr_avg', \n",
    "  'description':'Average wind speed over the course of an hour', \n",
    "  'type':'FLOAT', \n",
    "  'mode':'NULLABLE'}\n",
    "\n",
    "wind_schema = schema[0:2] + schema[3:5] + [wind_hr_avg]\n",
    "wind_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadJob<project=alaska-scrape, location=us-east4, id=30e48cb9-55c3-44ff-bab8-1e02cab80b6b>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../airflow/dags/data/uscrn_wind_agg.csv\")\n",
    "\n",
    "jc = bigquery.LoadJobConfig(\n",
    "   source_format = bigquery.SourceFormat.CSV,\n",
    "   skip_leading_rows=1,\n",
    "   autodetect=False,\n",
    "   schema=wind_schema,\n",
    "   create_disposition=\"CREATE_IF_NEEDED\",\n",
    "   write_disposition=\"WRITE_TRUNCATE\", \n",
    "   destination_table_description=\"Hourly wind data from USCRN stations, aggregated from 5 minute measurements\"\n",
    ")\n",
    "\n",
    "table_id = f\"{PROJECT_ID}.{DATASET_ID}.uscrn_wind\" \n",
    "\n",
    "job = bq_client.load_table_from_dataframe(df, table_id, job_config=jc)\n",
    "job.result()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supplemental Data** (Locations and Column Description tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadJob<project=alaska-scrape, location=us-east4, id=e2fe472f-ee3b-424f-ac0f-a14ccc5ff26b>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Locations table\n",
    "table_id = f\"{PROJECT_ID}.{DATASET_ID}.locations\"\n",
    "\n",
    "jc = bigquery.LoadJobConfig(\n",
    "  source_format = bigquery.SourceFormat.CSV,\n",
    "  autodetect=True,\n",
    "  create_disposition=\"CREATE_IF_NEEDED\",\n",
    "  write_disposition=\"WRITE_TRUNCATE\", \n",
    "  destination_table_description=\"Location names, WBANNO codes, and coordinates for USCRN stations in Alaska\"\n",
    ")\n",
    "\n",
    "with open(\"../airflow/dags/data/locations.csv\", \"rb\") as fp: \n",
    "  job = bq_client.load_table_from_file(fp, table_id, job_config=jc)\n",
    "job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadJob<project=alaska-scrape, location=us-east4, id=0a834079-f8cf-484b-96dc-1ee70a55c464>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column description table \n",
    "table_id = f\"{PROJECT_ID}.{DATASET_ID}.column_descriptions\"\n",
    "\n",
    "schema = [ # Col headers not being autodetected\n",
    "  bigquery.SchemaField(\"name\", \"STRING\", mode=\"REQUIRED\"), \n",
    "  bigquery.SchemaField(\"description\", \"STRING\", mode=\"REQUIRED\"), \n",
    "  bigquery.SchemaField(\"units\", \"STRING\", mode=\"REQUIRED\"),\n",
    "  bigquery.SchemaField(\"type\", \"STRING\", mode=\"REQUIRED\")\n",
    "]\n",
    "\n",
    "jc = bigquery.LoadJobConfig(\n",
    "  source_format = bigquery.SourceFormat.CSV,\n",
    "  skip_leading_rows=1,\n",
    "  autodetect=False,\n",
    "  create_disposition=\"CREATE_IF_NEEDED\",\n",
    "  write_disposition=\"WRITE_TRUNCATE\", \n",
    "  destination_table_description=f\"Column descriptions for fields in {DATASET_ID}.uscrn\", \n",
    "  schema=schema\n",
    ")\n",
    "\n",
    "with open(\"../airflow/dags/data/column_descriptions.csv\", \"rb\") as fp: \n",
    "  job = bq_client.load_table_from_file(fp, table_id, job_config=jc)\n",
    "job.result()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's also upload these smaller tables to Google Cloud Storage. When we create our Google Cloud Functions (see `gcf/`) it will be easier to read them from there than from BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil mb -p {PROJECT_ID} -b on -l us-east4 gs://{PROJECT_ID}-bucket\n",
    "!gsutil cp ../airflow/dags/data/locations.csv gs://{PROJECT_ID}-bucket\n",
    "!gsutil cp ../airflow/dags/data/column_descriptions.csv gs://{PROJECT_ID}-bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2252  2023-02-26T17:52:09Z  gs://alaska-scrape-bucket/column_descriptions.csv\n",
      "       706  2023-02-26T17:51:54Z  gs://alaska-scrape-bucket/locations.csv\n",
      "TOTAL: 2 objects, 2958 bytes (2.89 KiB)\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls -l gs://{PROJECT_ID}-bucket "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.) Updating Data \n",
    "\n",
    "So far we've scraped, transformed, and uploaded all available data from the USCRN database. For our regularly-running script we'll only want to scrape the newest data we don't have yet. See `airflow/dags` for examples of how to do this.\n",
    "\n",
    "\n",
    "`uscrn_dag.py`\n",
    "\n",
    "`uscrn_wind_dag.py`\n",
    "\n",
    "![uscrn_wind_dag_success](../img/uscrn_wind_dag_success.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "70623b801652781c2389d9f74154af1ef3dd8a50bfe8b7cd6824c1648ddc5ad0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
