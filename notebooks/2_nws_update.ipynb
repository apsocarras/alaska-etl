{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NWS Forecast Data\n",
    "\n",
    "This notebook explains how to scrape, transform, and upload the NWS's hourly weather forecast for the next 6 days** into a dataset in BigQuery. The resulting script can be set to run at regular intervals as an Airflow DAG or a function in Cloud Functions. \n",
    "\n",
    "** *For some reason, the hourly forecast doesn't quite extend to a full week, but only 6.5 days. To keep the math easier, we will only scrape the next 6 days -- in the end, this won't affect our pipeline once we have it updating continuously.*\n",
    "\n",
    "We will collect hourly forecasts for the locations of 23 USCRN data collection stations in Alaska -- the presence of these stations will enable ourselves and any other users of our dataset to evaluate the accuracy of the forecasts.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why use scraping over `api.weather.gov`?\n",
    "\n",
    "Generally speaking, if a website offers an API to access its data then it's a good bet to use it.\n",
    "\n",
    " So why not just use `api.weather.gov`?\n",
    "\n",
    "The main reason I've chose webscraping for the NWS part of the project is that at times the `api.weather.gov` has given me `500: Internal Server Error` responses even when the HTML data interface is still accessible. The level of information provided is essentially the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "import datetime as dt \n",
    "import itertools\n",
    "import time\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_df = pd.read_csv(\"../data/locations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "station_location    Deadhorse\n",
      "wbanno                  26565\n",
      "longitude             -148.46\n",
      "latitude                70.16\n",
      "Name: 13, dtype: object\n",
      "\n",
      "{'number': 1, 'name': '', 'startTime': '2023-02-28T05:00:00-09:00', 'endTime': '2023-02-28T06:00:00-09:00', 'isDaytime': False, 'temperature': -24, 'temperatureUnit': 'F', 'temperatureTrend': None, 'probabilityOfPrecipitation': {'unitCode': 'wmoUnit:percent', 'value': 3}, 'dewpoint': {'unitCode': 'wmoUnit:degC', 'value': -33.333333333333336}, 'relativeHumidity': {'unitCode': 'wmoUnit:percent', 'value': 79}, 'windSpeed': '5 mph', 'windDirection': 'SW', 'icon': 'https://api.weather.gov/icons/land/night/sct,3?size=small', 'shortForecast': 'Partly Cloudy', 'detailedForecast': ''}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Date</td>\n",
       "      <td>02/28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>03/01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hour (AKST)</td>\n",
       "      <td>05</td>\n",
       "      <td>06</td>\n",
       "      <td>07</td>\n",
       "      <td>08</td>\n",
       "      <td>09</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>00</td>\n",
       "      <td>01</td>\n",
       "      <td>02</td>\n",
       "      <td>03</td>\n",
       "      <td>04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Temperature (°F)</td>\n",
       "      <td>-24</td>\n",
       "      <td>-24</td>\n",
       "      <td>-25</td>\n",
       "      <td>-25</td>\n",
       "      <td>-24</td>\n",
       "      <td>-22</td>\n",
       "      <td>-19</td>\n",
       "      <td>-17</td>\n",
       "      <td>-16</td>\n",
       "      <td>...</td>\n",
       "      <td>-16</td>\n",
       "      <td>-17</td>\n",
       "      <td>-18</td>\n",
       "      <td>-20</td>\n",
       "      <td>-21</td>\n",
       "      <td>-22</td>\n",
       "      <td>-22</td>\n",
       "      <td>-21</td>\n",
       "      <td>-21</td>\n",
       "      <td>-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dewpoint (°F)</td>\n",
       "      <td>-28</td>\n",
       "      <td>-28</td>\n",
       "      <td>-29</td>\n",
       "      <td>-30</td>\n",
       "      <td>-30</td>\n",
       "      <td>-28</td>\n",
       "      <td>-25</td>\n",
       "      <td>-22</td>\n",
       "      <td>-21</td>\n",
       "      <td>...</td>\n",
       "      <td>-20</td>\n",
       "      <td>-23</td>\n",
       "      <td>-22</td>\n",
       "      <td>-24</td>\n",
       "      <td>-24</td>\n",
       "      <td>-25</td>\n",
       "      <td>-25</td>\n",
       "      <td>-25</td>\n",
       "      <td>-26</td>\n",
       "      <td>-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wind Chill (°F)</td>\n",
       "      <td>-38</td>\n",
       "      <td>-41</td>\n",
       "      <td>-41</td>\n",
       "      <td>-41</td>\n",
       "      <td>-44</td>\n",
       "      <td>-42</td>\n",
       "      <td>-39</td>\n",
       "      <td>-41</td>\n",
       "      <td>-39</td>\n",
       "      <td>...</td>\n",
       "      <td>-45</td>\n",
       "      <td>-46</td>\n",
       "      <td>-47</td>\n",
       "      <td>-49</td>\n",
       "      <td>-51</td>\n",
       "      <td>-51</td>\n",
       "      <td>-51</td>\n",
       "      <td>-50</td>\n",
       "      <td>-48</td>\n",
       "      <td>-48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Surface Wind (mph)</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Wind Dir</td>\n",
       "      <td>SW</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>...</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Gust</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sky Cover (%)</td>\n",
       "      <td>28</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Precipitation Potential (%)</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Relative Humidity (%)</td>\n",
       "      <td>80</td>\n",
       "      <td>82</td>\n",
       "      <td>79</td>\n",
       "      <td>74</td>\n",
       "      <td>71</td>\n",
       "      <td>72</td>\n",
       "      <td>76</td>\n",
       "      <td>78</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>85</td>\n",
       "      <td>76</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>84</td>\n",
       "      <td>85</td>\n",
       "      <td>83</td>\n",
       "      <td>80</td>\n",
       "      <td>78</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Rain</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>...</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Thunder</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>...</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Snow</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>...</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Freezing Rain</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>...</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Sleet</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>...</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             0      1    2    3    4    5    6    7    8   \\\n",
       "1                          Date  02/28  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "2                   Hour (AKST)     05   06   07   08   09   10   11   12   \n",
       "3              Temperature (°F)    -24  -24  -25  -25  -24  -22  -19  -17   \n",
       "4                 Dewpoint (°F)    -28  -28  -29  -30  -30  -28  -25  -22   \n",
       "5               Wind Chill (°F)    -38  -41  -41  -41  -44  -42  -39  -41   \n",
       "6            Surface Wind (mph)      5    6    6    6    9    9    9   15   \n",
       "7                      Wind Dir     SW    S    S    S    E    E    E    E   \n",
       "8                          Gust    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "9                 Sky Cover (%)     28   22   22   22   14   14   14    5   \n",
       "10  Precipitation Potential (%)      3    3    3    3    2    2    2    1   \n",
       "11        Relative Humidity (%)     80   82   79   74   71   72   76   78   \n",
       "12                         Rain     --   --   --   --   --   --   --   --   \n",
       "13                      Thunder     --   --   --   --   --   --   --   --   \n",
       "14                         Snow     --   --   --   --   --   --   --   --   \n",
       "15                Freezing Rain     --   --   --   --   --   --   --   --   \n",
       "16                        Sleet     --   --   --   --   --   --   --   --   \n",
       "\n",
       "     9   ...   15   16   17   18   19     20   21   22   23   24  \n",
       "1   NaN  ...  NaN  NaN  NaN  NaN  NaN  03/01  NaN  NaN  NaN  NaN  \n",
       "2    13  ...   19   20   21   22   23     00   01   02   03   04  \n",
       "3   -16  ...  -16  -17  -18  -20  -21    -22  -22  -21  -21  -21  \n",
       "4   -21  ...  -20  -23  -22  -24  -24    -25  -25  -25  -26  -26  \n",
       "5   -39  ...  -45  -46  -47  -49  -51    -51  -51  -50  -48  -48  \n",
       "6    15  ...   22   22   22   22   22     21   21   21   18   18  \n",
       "7     E  ...    E    E    E    E    E      E    E    E    E    E  \n",
       "8   NaN  ...  NaN  NaN  NaN  NaN  NaN    NaN  NaN  NaN  NaN  NaN  \n",
       "9     5  ...   32   32   28   28   28     65   65   65   73   73  \n",
       "10    1  ...    0    0    0    0    0      0    0    0    0    0  \n",
       "11   76  ...   85   76   82   82   84     85   83   80   78   79  \n",
       "12   --  ...   --   --   --   --   --     --   --   --   --   --  \n",
       "13   --  ...   --   --   --   --   --     --   --   --   --   --  \n",
       "14   --  ...   --   --   --   --   --     --   --   --   --   --  \n",
       "15   --  ...   --   --   --   --   --     --   --   --   --   --  \n",
       "16   --  ...   --   --   --   --   --     --   --   --   --   --  \n",
       "\n",
       "[16 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_location = locations_df.sample(1).iloc[0] \n",
    "\n",
    "print(f\"{random_location}\\n\")\n",
    "\n",
    "lat, lon = random_location['latitude'], random_location['longitude']  \n",
    "\n",
    "## API results\n",
    "url = f\"https://api.weather.gov/points/{lat},{lon}\"\n",
    "\n",
    "response = requests.get(url)\n",
    "main_data = response.json()\n",
    "\n",
    "response = requests.get(main_data['properties']['forecastHourly'])\n",
    "hourly_data = response.json()\n",
    "fields = hourly_data['properties']['periods'][0]\n",
    "\n",
    "print(f\"{fields}\\n\") \n",
    "\n",
    "## Webscraping results \n",
    "url = f\"https://forecast.weather.gov/MapClick.php?lat={lat}&lon={lon}&unit=0&lg=english&FcstType=digital&menu=1\"\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "df = pd.read_html(str(soup.find_all(\"table\")[5]))[0]\n",
    "df = df.iloc[1:17,:]\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results need cleaning (e.g. the column names are in the first row), but you can see that the information is roughly equivalent, with the API offering an `isDaytime` field and the HTML interface offering an actual percentage for sky cover (rather than a snippet like \"Partly Cloudy\"). Depending on how `isDaytime` is measured, it might be more accurate than any estimate of daylight hours we could make based off the scraped data...\n",
    "\n",
    "![alaska-sun](../img/alaska_suntimes.png)\n",
    "\n",
    "Welp -- we'll exclude it for now. Later on we can either access it separately from the API or estimate it ourselves based off standard time tables.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.) Scraping the Data "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each location, the forecast for the next 48 hours is stored in a tabular data table like this: \n",
    "\n",
    "<img src=\"../img/nws_p1.png\" height=400px>\n",
    "\n",
    "The rest of the forecast can be accessed by jumping ahead in 48 hour increments. We do this by adding `&AheadHour=` on the end of the URL and specifying how many hours (48 and 96). \n",
    "\n",
    "The series of transformations required here is quite complex -- a downside of scraping vs using the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.utils # get_last_update_nws, ff_list, get_soup, flatten  -- check utils.py to see what these do\n",
    "\n",
    "def extract_table_data(soup:BeautifulSoup, location:str) -> list:\n",
    "  \"\"\"\n",
    "  Extracts 48hr forecast table data from a Beautiful Soup object as a list of lists\n",
    "\n",
    "  Args: \n",
    "  table_records (list): List of <tr> elements containing NWS forecast data\n",
    "\n",
    "  location (str): The name of the place the forecast is for; used for filling out added \"location\" column \n",
    "\n",
    "  Returns:\n",
    "  table (list): List of lists containing table data \n",
    "  \"\"\"\n",
    "  table_records = soup.find_all(\"table\")[5].find_all(\"tr\")\n",
    "\n",
    "  colspan = table_records[0] # 48hr data is divided into two tables by two colspan elements\n",
    "  table = [tr for  tr in table_records if tr != colspan] # vertically concat tables by removing colspan elements\n",
    "\n",
    "  table = [[ele.getText() for ele in tr.find_all(\"font\")] for tr in table] \n",
    "\n",
    "  # Add location column \n",
    "  location_col = ['location']\n",
    "  location_col.extend([location]*24) # fill out to match length of other columns\n",
    "  table.insert(1, location_col)  # for first half of table\n",
    "  table.insert(19, location_col) # for second half of table\n",
    "\n",
    "  # Add last_update_nws column \n",
    "  last_update_nws = [\"last_update_nws\"]\n",
    "  last_update_nws.extend([utils.get_last_update_nws(soup)] * 24)\n",
    "  table.insert(1, last_update_nws)\n",
    "  table.insert(19, last_update_nws) \n",
    "\n",
    "  return table\n",
    "\n",
    "def transpose_as_dict(table:list) -> dict:\n",
    "  \"\"\"\n",
    "  Takes the list of lists generated by extract_table_data() and transposes it (flip orientation) by casting as a dictionary\n",
    "  \n",
    "  Args:\n",
    "  table (list): list of lists of columnar data generated by extract_table_data()\n",
    "\n",
    "  Returns: \n",
    "  data_map (dict): Dictionary representation of table, transposed and ready to be made into a dataframe\n",
    "  \"\"\"\n",
    "  data_map = {}\n",
    "  for col in table: # Table is still \"landscape-oriented\"\n",
    "    if col[0] not in data_map.keys(): # cols from first half of table\n",
    "      data_map[col[0]] = col[1:]\n",
    "    else: # cols from second half\n",
    "      data_map[col[0]].extend(col[1:])\n",
    "\n",
    "  data_map['Date'] = utils.ff_list(data_map['Date'])\n",
    "\n",
    "  return data_map\n",
    "\n",
    "def transform_df(forecast_dict:dict) -> pd.DataFrame: \n",
    "  \"\"\"Cast dictionary from transpose_as_dict() to a dataframe and transform\"\"\"\n",
    "  ## Create dataframe\n",
    "  df = pd.DataFrame(forecast_dict)\n",
    "  \n",
    "  ## Edit column headers \n",
    "  df.columns = [col.lower() for col in df.columns] \n",
    "  df.rename(columns=lambda x: re.sub('°|\\(|\\)', '', x), inplace=True)\n",
    "  df.rename(columns=lambda x: re.sub('%', 'pct', x), inplace=True)\n",
    "  df.rename(columns=lambda x: re.sub(' ', '_', x.strip()), inplace=True)\n",
    "  \n",
    "  ## Replace missing values\n",
    "  # Replace missing values in gust with zero -- gust is never *actually* 0 so no masking\n",
    "  # Replace missing values in windchill with an explicity np.NaN\n",
    "  df.replace({'gust':{'':0}, 'wind_chill_f':{'':np.nan}}, inplace=True)\n",
    "\n",
    "  ## Datetime Transformations\n",
    "  cur_year = dt.datetime.now().year\n",
    "  dt_strings = df['date'] + '/' + str(cur_year) + ' ' + df['hour_akst'] + ':00 AKST'\n",
    "  # Local time (AKST)\n",
    "  df['lst_datetime'] = pd.to_datetime(dt_strings, format='%m/%d/%Y %H:%M AKST')\n",
    "  # UTC time\n",
    "  akst_offset = dt.timedelta(hours=9)\n",
    "  df['utc_datetime'] = df['lst_datetime'] + akst_offset\n",
    "\n",
    "  ## Drop duplicates in composite key columns \n",
    "  duplicates = df.duplicated(subset=[\"location\", \"lst_datetime\"], keep=False)\n",
    "  duplicate_rows = df[duplicates]\n",
    "  if not duplicate_rows.empty:\n",
    "    print(f\"Warning: {len(duplicate_rows)} rows have duplicate values in location and lst_datetime\")\n",
    "    print(f\"Dropping\")\n",
    "    df.drop_duplicates(subset=['location', 'lst_datetime'], inplace=True, ignore_index=True)\n",
    "\n",
    "  ## Reorder columns \n",
    "  col_names = ['location', 'utc_datetime', 'lst_datetime'] + list(df.columns[4:-2]) + [\"last_update_nws\"]\n",
    "  df = df[col_names]\n",
    "\n",
    "  return df "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the main function to scrape the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forecast_df() -> pd.DataFrame:\n",
    "  \"\"\"Get a dataframe of NWS forecast data for the next 6 days from various points in Alaska\"\"\"\n",
    "\n",
    "  nws_urls = locations_df.apply(utils.get_nws_url, axis=1)\n",
    "  url_map = dict(zip(locations_df['station_location'], nws_urls))\n",
    "\n",
    "  combined_table = []\n",
    "  for location, url in url_map.items():\n",
    "    soup_list = [utils.get_soup(url + f\"&AheadHour={hr}\") for hr in (0,48,96)]\n",
    "    table_list = utils.flatten([extract_table_data(soup, location) for soup in soup_list])\n",
    "    combined_table.extend(table_list)\n",
    "  \n",
    "  forecast_dict = transpose_as_dict(combined_table)\n",
    "\n",
    "  return transform_df(forecast_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>utc_datetime</th>\n",
       "      <th>lst_datetime</th>\n",
       "      <th>temperature_f</th>\n",
       "      <th>dewpoint_f</th>\n",
       "      <th>wind_chill_f</th>\n",
       "      <th>surface_wind_mph</th>\n",
       "      <th>wind_dir</th>\n",
       "      <th>gust</th>\n",
       "      <th>sky_cover_pct</th>\n",
       "      <th>precipitation_potential_pct</th>\n",
       "      <th>relative_humidity_pct</th>\n",
       "      <th>rain</th>\n",
       "      <th>thunder</th>\n",
       "      <th>snow</th>\n",
       "      <th>freezing_rain</th>\n",
       "      <th>sleet</th>\n",
       "      <th>fog</th>\n",
       "      <th>last_update_nws</th>\n",
       "      <th>hash_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fairbanks</td>\n",
       "      <td>2023-03-01 15:00:00</td>\n",
       "      <td>2023-03-01 06:00:00</td>\n",
       "      <td>-4</td>\n",
       "      <td>-8</td>\n",
       "      <td>-17</td>\n",
       "      <td>7</td>\n",
       "      <td>E</td>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>16</td>\n",
       "      <td>82</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>SChc</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>2023-03-01 04:51:00</td>\n",
       "      <td>595ce723d483bcc5136b598d75c8106a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fairbanks</td>\n",
       "      <td>2023-03-01 16:00:00</td>\n",
       "      <td>2023-03-01 07:00:00</td>\n",
       "      <td>-4</td>\n",
       "      <td>-9</td>\n",
       "      <td>-18</td>\n",
       "      <td>7</td>\n",
       "      <td>E</td>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>16</td>\n",
       "      <td>80</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>SChc</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>2023-03-01 04:51:00</td>\n",
       "      <td>eb60aefaa763787eccd79cfb3d2cb150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fairbanks</td>\n",
       "      <td>2023-03-01 17:00:00</td>\n",
       "      <td>2023-03-01 08:00:00</td>\n",
       "      <td>-4</td>\n",
       "      <td>-10</td>\n",
       "      <td>-18</td>\n",
       "      <td>7</td>\n",
       "      <td>E</td>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>16</td>\n",
       "      <td>77</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>SChc</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>2023-03-01 04:51:00</td>\n",
       "      <td>7ac1be4ecb6e583d98328f7ee8ccd618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fairbanks</td>\n",
       "      <td>2023-03-01 18:00:00</td>\n",
       "      <td>2023-03-01 09:00:00</td>\n",
       "      <td>-2</td>\n",
       "      <td>-9</td>\n",
       "      <td>-12</td>\n",
       "      <td>5</td>\n",
       "      <td>NE</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>21</td>\n",
       "      <td>73</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>SChc</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>2023-03-01 04:51:00</td>\n",
       "      <td>2edc0876c91456a32d481bcf5a91456a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fairbanks</td>\n",
       "      <td>2023-03-01 19:00:00</td>\n",
       "      <td>2023-03-01 10:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>-6</td>\n",
       "      <td>-8</td>\n",
       "      <td>5</td>\n",
       "      <td>NE</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>21</td>\n",
       "      <td>69</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>SChc</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>2023-03-01 04:51:00</td>\n",
       "      <td>ab8ea23d1af79efd227d782ecc077149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3307</th>\n",
       "      <td>Aleknagik</td>\n",
       "      <td>2023-03-07 10:00:00</td>\n",
       "      <td>2023-03-07 01:00:00</td>\n",
       "      <td>27</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>E</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>31</td>\n",
       "      <td>98</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>Chc</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>2023-03-01 04:21:00</td>\n",
       "      <td>24a8c0ca1f720e1ca2db6b2af46d7312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3308</th>\n",
       "      <td>Aleknagik</td>\n",
       "      <td>2023-03-07 11:00:00</td>\n",
       "      <td>2023-03-07 02:00:00</td>\n",
       "      <td>27</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>E</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>31</td>\n",
       "      <td>99</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>Chc</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>2023-03-01 04:21:00</td>\n",
       "      <td>354f18cf14430bca0021ec2de3cedceb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3309</th>\n",
       "      <td>Aleknagik</td>\n",
       "      <td>2023-03-07 12:00:00</td>\n",
       "      <td>2023-03-07 03:00:00</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>E</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>33</td>\n",
       "      <td>100</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>Chc</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>2023-03-01 04:21:00</td>\n",
       "      <td>5ee9a517e5c05a1c982429d11e8cc26d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3310</th>\n",
       "      <td>Aleknagik</td>\n",
       "      <td>2023-03-07 13:00:00</td>\n",
       "      <td>2023-03-07 04:00:00</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>E</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>33</td>\n",
       "      <td>98</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>Chc</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>2023-03-01 04:21:00</td>\n",
       "      <td>000fa28b4b24e3ef952ef56e8c551c95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3311</th>\n",
       "      <td>Aleknagik</td>\n",
       "      <td>2023-03-07 14:00:00</td>\n",
       "      <td>2023-03-07 05:00:00</td>\n",
       "      <td>26</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>E</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>33</td>\n",
       "      <td>95</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>Chc</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>2023-03-01 04:21:00</td>\n",
       "      <td>180bb02e7c8eac6dafcf86cb8d4a33f5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3312 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       location        utc_datetime        lst_datetime temperature_f  \\\n",
       "0     Fairbanks 2023-03-01 15:00:00 2023-03-01 06:00:00            -4   \n",
       "1     Fairbanks 2023-03-01 16:00:00 2023-03-01 07:00:00            -4   \n",
       "2     Fairbanks 2023-03-01 17:00:00 2023-03-01 08:00:00            -4   \n",
       "3     Fairbanks 2023-03-01 18:00:00 2023-03-01 09:00:00            -2   \n",
       "4     Fairbanks 2023-03-01 19:00:00 2023-03-01 10:00:00             2   \n",
       "...         ...                 ...                 ...           ...   \n",
       "3307  Aleknagik 2023-03-07 10:00:00 2023-03-07 01:00:00            27   \n",
       "3308  Aleknagik 2023-03-07 11:00:00 2023-03-07 02:00:00            27   \n",
       "3309  Aleknagik 2023-03-07 12:00:00 2023-03-07 03:00:00            26   \n",
       "3310  Aleknagik 2023-03-07 13:00:00 2023-03-07 04:00:00            26   \n",
       "3311  Aleknagik 2023-03-07 14:00:00 2023-03-07 05:00:00            26   \n",
       "\n",
       "     dewpoint_f wind_chill_f surface_wind_mph wind_dir gust sky_cover_pct  \\\n",
       "0            -8          -17                7        E    0            82   \n",
       "1            -9          -18                7        E    0            82   \n",
       "2           -10          -18                7        E    0            82   \n",
       "3            -9          -12                5       NE    0            84   \n",
       "4            -6           -8                5       NE    0            84   \n",
       "...         ...          ...              ...      ...  ...           ...   \n",
       "3307         26            0               11        E    0            73   \n",
       "3308         26            0               11        E    0            73   \n",
       "3309         26            0                8        E    0            70   \n",
       "3310         26            0                8        E    0            70   \n",
       "3311         25            0                8        E    0            70   \n",
       "\n",
       "     precipitation_potential_pct relative_humidity_pct rain thunder  snow  \\\n",
       "0                             16                    82   --      --  SChc   \n",
       "1                             16                    80   --      --  SChc   \n",
       "2                             16                    77   --      --  SChc   \n",
       "3                             21                    73   --      --  SChc   \n",
       "4                             21                    69   --      --  SChc   \n",
       "...                          ...                   ...  ...     ...   ...   \n",
       "3307                          31                    98   --      --   Chc   \n",
       "3308                          31                    99   --      --   Chc   \n",
       "3309                          33                   100   --      --   Chc   \n",
       "3310                          33                    98   --      --   Chc   \n",
       "3311                          33                    95   --      --   Chc   \n",
       "\n",
       "     freezing_rain sleet fog     last_update_nws  \\\n",
       "0               --    --  -- 2023-03-01 04:51:00   \n",
       "1               --    --  -- 2023-03-01 04:51:00   \n",
       "2               --    --  -- 2023-03-01 04:51:00   \n",
       "3               --    --  -- 2023-03-01 04:51:00   \n",
       "4               --    --  -- 2023-03-01 04:51:00   \n",
       "...            ...   ...  ..                 ...   \n",
       "3307            --    --  -- 2023-03-01 04:21:00   \n",
       "3308            --    --  -- 2023-03-01 04:21:00   \n",
       "3309            --    --  -- 2023-03-01 04:21:00   \n",
       "3310            --    --  -- 2023-03-01 04:21:00   \n",
       "3311            --    --  -- 2023-03-01 04:21:00   \n",
       "\n",
       "                               hash_id  \n",
       "0     595ce723d483bcc5136b598d75c8106a  \n",
       "1     eb60aefaa763787eccd79cfb3d2cb150  \n",
       "2     7ac1be4ecb6e583d98328f7ee8ccd618  \n",
       "3     2edc0876c91456a32d481bcf5a91456a  \n",
       "4     ab8ea23d1af79efd227d782ecc077149  \n",
       "...                                ...  \n",
       "3307  24a8c0ca1f720e1ca2db6b2af46d7312  \n",
       "3308  354f18cf14430bca0021ec2de3cedceb  \n",
       "3309  5ee9a517e5c05a1c982429d11e8cc26d  \n",
       "3310  000fa28b4b24e3ef952ef56e8c551c95  \n",
       "3311  180bb02e7c8eac6dafcf86cb8d4a33f5  \n",
       "\n",
       "[3312 rows x 20 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_forecast_df()\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 2.) Uploading the Data \n",
    "\n",
    "![TO-DO_ERD]()\n",
    "\n",
    "The end goal of our pipeline is for the NWS forecasts to be easily evaluated against the historic data from the USCRN. We also want to make it easy for data scientists and ML engineers forecasting from the USCRN data to evaluate the performance of their models against the NWS forecasts, with how long the NWS forecasts were made in advance (`utc_datetime - last_update_nws`) being a key parameter. \n",
    "\n",
    "By taking a daily snapshot of the NWS forecast data we keep the data highly denormalized, making this sort of analysis much easier and our pipeline simpler to manage. We could reduce data duplication by using a nested history column (e.g. a JSON array) but memory is less of a concern for us than ease of analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yaml import full_load\n",
    "from google.cloud import bigquery \n",
    "from google.oauth2 import service_account \n",
    "from google.api_core.exceptions import NotFound\n",
    "\n",
    "# GCP/BigQuery information\n",
    "with open(\"../airflow/dags/config/gcp-config.yaml\", \"r\") as fp:\n",
    "  gcp_config = full_load(fp)\n",
    "  \n",
    "PROJECT_ID = gcp_config['project-id']\n",
    "DATASET_ID = gcp_config['dataset-id']\n",
    "STAGING_TABLE_ID = 'nws_staging'\n",
    "MAIN_TABLE_ID = 'nws' \n",
    "\n",
    "# Set credentials\n",
    "key_path = gcp_config['credentials']\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "  key_path, scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    ")\n",
    "\n",
    "# Create client\n",
    "client = bigquery.Client(credentials=credentials, project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_staging_table(df:pd.DataFrame) -> None: \n",
    "  \"\"\"Upload dataframe from get_forecast_df() to BigQuery staging table\"\"\"\n",
    "\n",
    "  # Set Schema\n",
    "  schema = [\n",
    "    bigquery.SchemaField(\"location\", \"STRING\", mode=\"REQUIRED\"), \n",
    "    bigquery.SchemaField(\"utc_datetime\", \"DATETIME\", mode=\"REQUIRED\"), \n",
    "    bigquery.SchemaField(\"lst_datetime\", \"DATETIME\", mode=\"REQUIRED\"), \n",
    "    bigquery.SchemaField(\"temperature_f\", \"INTEGER\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"dewpoint_f\", \"INTEGER\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"wind_chill_f\", \"INTEGER\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"surface_wind_mph\", \"INTEGER\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"wind_dir\", \"STRING\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"gust\", \"INTEGER\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"sky_cover_pct\", \"INTEGER\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"precipitation_potential_pct\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"relative_humidity_pct\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"rain\", \"STRING\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"thunder\", \"STRING\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"snow\", \"STRING\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"freezing_rain\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"sleet\", \"STRING\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"fog\", \"STRING\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"last_update_nws\", \"DATETIME\", mode=\"NULLABLE\")\n",
    "  ] \n",
    "\n",
    "  jc = bigquery.LoadJobConfig(\n",
    "    source_format = bigquery.SourceFormat.CSV,\n",
    "    skip_leading_rows=1,\n",
    "    autodetect=False,\n",
    "    schema=schema,\n",
    "    create_disposition=\"CREATE_IF_NEEDED\",\n",
    "    write_disposition=\"WRITE_TRUNCATE\"   \n",
    "  )\n",
    "\n",
    "  # Set target table in BigQuery\n",
    "  full_table_id = f\"{PROJECT_ID}.{DATASET_ID}.{STAGING_TABLE_ID}\"\n",
    "\n",
    "  # Upload to BigQuery\n",
    "  ## If any required columns are missing values, include name of column in error message\n",
    "  try: \n",
    "    job = client.load_table_from_dataframe(df, full_table_id, job_config=jc)\n",
    "    job.result()\n",
    "  except Exception as e:\n",
    "    error_message = str(e)\n",
    "    if 'Required column value for column index' in error_message:\n",
    "      start_index = error_message.index('Required column value for column index') + len('Required column value for column index: ')\n",
    "      end_index = error_message.index(' is missing', start_index)\n",
    "      missing_column_index = int(error_message[start_index:end_index])\n",
    "      missing_column_name = list(df.columns)[missing_column_index]\n",
    "      error_message = error_message[:start_index] + f'{missing_column_name} ({missing_column_index})' + error_message[end_index:]\n",
    "    raise Exception(error_message) \n",
    "  \n",
    "  # Log result \n",
    "  table = client.get_table(full_table_id)\n",
    "  print(f\"Loaded {table.num_rows} rows and {len(table.schema)} columns into {full_table_id}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3311 rows and 19 columns into alaska-scrape.weather.nws_staging\n",
      "\n"
     ]
    }
   ],
   "source": [
    "load_staging_table(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_table() -> None: \n",
    "  \"\"\"Insert staging table into the main data table -- creates the table if it doesn't exist yet\"\"\"\n",
    "  \n",
    "  insert_query=f\"\"\"\n",
    "    INSERT INTO {DATASET_ID}.{MAIN_TABLE_ID} \n",
    "    SELECT *, CURRENT_TIMESTAMP() as date_added\n",
    "    FROM {DATASET_ID}.{STAGING_TABLE_ID}\n",
    "    \"\"\"\n",
    "\n",
    "  try: \n",
    "    query_job = client.query(insert_query) \n",
    "    query_job.result()\n",
    "  except NotFound:\n",
    "    print(f\"Table {DATASET_ID}.{MAIN_TABLE_ID} does not exist. Creating.\")\n",
    "    create_query = f\"\"\"\n",
    "      CREATE TABLE {DATASET_ID}.{MAIN_TABLE_ID}\n",
    "      AS\n",
    "      SELECT *, CURRENT_TIMESTAMP() as date_added\n",
    "      FROM {DATASET_ID}.{STAGING_TABLE_ID}\n",
    "    \"\"\"\n",
    "    query_job = client.query(create_query)\n",
    "    query_job.result()\n",
    "    \n",
    "  full_table_id = f\"{PROJECT_ID}.{DATASET_ID}.{MAIN_TABLE_ID}\"\n",
    "  table = client.get_table(full_table_id)\n",
    "  print(f\"Loaded {table.num_rows} rows and {len(table.schema)} columns into {full_table_id}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table weather.nws does not exist. Creating.\n",
      "Loaded 3311 rows and 20 columns into alaska-scrape.weather.nws\n",
      "\n"
     ]
    }
   ],
   "source": [
    "insert_table()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![success](../img/success_nws_staging.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Automating this pipeline\n",
    "\n",
    "**Airflow**\n",
    "\n",
    "![nws_dag_success](../img/nws_dag_success.png)\n",
    "\n",
    "See `../airflow/dags/` for how I ported this over to an automatable DAG in Airflow. `get_forecast_df()` does quite a lot for a single task, but we kept the port super simple since ultimately this pipeline will be run in Google Cloud Functions. Airflow logs our print statements so we don't even need to change those.\n",
    "\n",
    "```python \n",
    "## Changes made \n",
    "@task \n",
    "def get_forecast_dict() -> dict:\n",
    "  \"\"\"Get forecast data for next 6 days as dict\"\"\"\n",
    "  #...do stuff...#\n",
    "  return forecast_dict\n",
    "\n",
    "@task\n",
    "def transform_forecast(forecast_dict:dict) -> dict: \n",
    "  \"\"\"Cast dictionary from transpose_as_dict() to a dataframe, transform, and cast back to dict\"\"\"\n",
    "  #...do stuff...#\n",
    "  return transformed_dict \n",
    "\n",
    "@task \n",
    "def load_staging_table(transformed_dict:dict) -> None: \n",
    "  \"\"\"Read dict from transform_dict() load to staging table in BigQuery\"\"\"\n",
    "```\n",
    "<br>\n",
    "\n",
    "A main restriction for Airflow is that tasks are not meant to transfer data. We can pass data as XCOMs, but: \n",
    "1. Dataframes are too large "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3130472 vs. 656 !!!\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "print(f\"{sys.getsizeof(df)} vs. {sys.getsizeof(df.to_dict())} bytes!!!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Types which are not JSON serializable (e.g. datetimes) need to be converted before they can be encoded. This makes passing around dictionaries with datetime information awkward. For purposes of keeping the port simple, we will modify `get_last_update_nws()` to return a string and convert any datetime columns to string and back again between XCOM pushes/pulls. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Google Cloud Functions**\n",
    "\n",
    "See `./notebooks/3_gcf_export.ipynb` for how I ported this over to a Google Cloud Function. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ![bq_result](../img/success_nws_staging.png) -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "70623b801652781c2389d9f74154af1ef3dd8a50bfe8b7cd6824c1648ddc5ad0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
