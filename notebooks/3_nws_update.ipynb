{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NWS Forecast Data\n",
    "\n",
    "This notebook explains how to scrape, transform, and upload the NWS's hourly weather forecast for the next 6 days** into a dataset in BigQuery. The resulting script can be set to run at regular intervals as an Airflow DAG or a function in Cloud Functions. \n",
    "\n",
    "** *For some reason, the hourly forecast doesn't quite extend to a full week, but only 6.5 days. To keep the math easier, we will only scrape the next 6 days -- in the end, this won't affect our pipeline once we have it updating continuously.*\n",
    "\n",
    "We will collect hourly forecasts for the locations of 23 USCRN data collection stations in Alaska -- the presence of these stations will enable ourselves and any other users of our dataset to evaluate the accuracy of the forecasts.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why use scraping over `api.weather.gov`?\n",
    "\n",
    "Generally speaking, if a website offers an API to access its data then it's a good bet to use it. So why not just use `api.weather.gov`?\n",
    "\n",
    "The main reason I've chose webscraping for the NWS part of the project is that at times the `api.weather.gov` has given me `500: Internal Server Error` responses even when the HTML data interface is still accessible. The level of information provided is essentially the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "import datetime as dt \n",
    "import itertools\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_df = pd.read_csv(\"../data/locations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "station_location    Deadhorse\n",
      "wbanno                  26565\n",
      "longitude             -148.46\n",
      "latitude                70.16\n",
      "Name: 13, dtype: object\n",
      "\n",
      "{'number': 1, 'name': '', 'startTime': '2023-02-28T05:00:00-09:00', 'endTime': '2023-02-28T06:00:00-09:00', 'isDaytime': False, 'temperature': -24, 'temperatureUnit': 'F', 'temperatureTrend': None, 'probabilityOfPrecipitation': {'unitCode': 'wmoUnit:percent', 'value': 3}, 'dewpoint': {'unitCode': 'wmoUnit:degC', 'value': -33.333333333333336}, 'relativeHumidity': {'unitCode': 'wmoUnit:percent', 'value': 79}, 'windSpeed': '5 mph', 'windDirection': 'SW', 'icon': 'https://api.weather.gov/icons/land/night/sct,3?size=small', 'shortForecast': 'Partly Cloudy', 'detailedForecast': ''}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Date</td>\n",
       "      <td>02/28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>03/01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hour (AKST)</td>\n",
       "      <td>05</td>\n",
       "      <td>06</td>\n",
       "      <td>07</td>\n",
       "      <td>08</td>\n",
       "      <td>09</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>00</td>\n",
       "      <td>01</td>\n",
       "      <td>02</td>\n",
       "      <td>03</td>\n",
       "      <td>04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Temperature (°F)</td>\n",
       "      <td>-24</td>\n",
       "      <td>-24</td>\n",
       "      <td>-25</td>\n",
       "      <td>-25</td>\n",
       "      <td>-24</td>\n",
       "      <td>-22</td>\n",
       "      <td>-19</td>\n",
       "      <td>-17</td>\n",
       "      <td>-16</td>\n",
       "      <td>...</td>\n",
       "      <td>-16</td>\n",
       "      <td>-17</td>\n",
       "      <td>-18</td>\n",
       "      <td>-20</td>\n",
       "      <td>-21</td>\n",
       "      <td>-22</td>\n",
       "      <td>-22</td>\n",
       "      <td>-21</td>\n",
       "      <td>-21</td>\n",
       "      <td>-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dewpoint (°F)</td>\n",
       "      <td>-28</td>\n",
       "      <td>-28</td>\n",
       "      <td>-29</td>\n",
       "      <td>-30</td>\n",
       "      <td>-30</td>\n",
       "      <td>-28</td>\n",
       "      <td>-25</td>\n",
       "      <td>-22</td>\n",
       "      <td>-21</td>\n",
       "      <td>...</td>\n",
       "      <td>-20</td>\n",
       "      <td>-23</td>\n",
       "      <td>-22</td>\n",
       "      <td>-24</td>\n",
       "      <td>-24</td>\n",
       "      <td>-25</td>\n",
       "      <td>-25</td>\n",
       "      <td>-25</td>\n",
       "      <td>-26</td>\n",
       "      <td>-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wind Chill (°F)</td>\n",
       "      <td>-38</td>\n",
       "      <td>-41</td>\n",
       "      <td>-41</td>\n",
       "      <td>-41</td>\n",
       "      <td>-44</td>\n",
       "      <td>-42</td>\n",
       "      <td>-39</td>\n",
       "      <td>-41</td>\n",
       "      <td>-39</td>\n",
       "      <td>...</td>\n",
       "      <td>-45</td>\n",
       "      <td>-46</td>\n",
       "      <td>-47</td>\n",
       "      <td>-49</td>\n",
       "      <td>-51</td>\n",
       "      <td>-51</td>\n",
       "      <td>-51</td>\n",
       "      <td>-50</td>\n",
       "      <td>-48</td>\n",
       "      <td>-48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Surface Wind (mph)</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Wind Dir</td>\n",
       "      <td>SW</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>...</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Gust</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sky Cover (%)</td>\n",
       "      <td>28</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Precipitation Potential (%)</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Relative Humidity (%)</td>\n",
       "      <td>80</td>\n",
       "      <td>82</td>\n",
       "      <td>79</td>\n",
       "      <td>74</td>\n",
       "      <td>71</td>\n",
       "      <td>72</td>\n",
       "      <td>76</td>\n",
       "      <td>78</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>85</td>\n",
       "      <td>76</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>84</td>\n",
       "      <td>85</td>\n",
       "      <td>83</td>\n",
       "      <td>80</td>\n",
       "      <td>78</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Rain</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>...</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Thunder</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>...</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Snow</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>...</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Freezing Rain</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>...</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Sleet</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>...</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             0      1    2    3    4    5    6    7    8   \\\n",
       "1                          Date  02/28  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "2                   Hour (AKST)     05   06   07   08   09   10   11   12   \n",
       "3              Temperature (°F)    -24  -24  -25  -25  -24  -22  -19  -17   \n",
       "4                 Dewpoint (°F)    -28  -28  -29  -30  -30  -28  -25  -22   \n",
       "5               Wind Chill (°F)    -38  -41  -41  -41  -44  -42  -39  -41   \n",
       "6            Surface Wind (mph)      5    6    6    6    9    9    9   15   \n",
       "7                      Wind Dir     SW    S    S    S    E    E    E    E   \n",
       "8                          Gust    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "9                 Sky Cover (%)     28   22   22   22   14   14   14    5   \n",
       "10  Precipitation Potential (%)      3    3    3    3    2    2    2    1   \n",
       "11        Relative Humidity (%)     80   82   79   74   71   72   76   78   \n",
       "12                         Rain     --   --   --   --   --   --   --   --   \n",
       "13                      Thunder     --   --   --   --   --   --   --   --   \n",
       "14                         Snow     --   --   --   --   --   --   --   --   \n",
       "15                Freezing Rain     --   --   --   --   --   --   --   --   \n",
       "16                        Sleet     --   --   --   --   --   --   --   --   \n",
       "\n",
       "     9   ...   15   16   17   18   19     20   21   22   23   24  \n",
       "1   NaN  ...  NaN  NaN  NaN  NaN  NaN  03/01  NaN  NaN  NaN  NaN  \n",
       "2    13  ...   19   20   21   22   23     00   01   02   03   04  \n",
       "3   -16  ...  -16  -17  -18  -20  -21    -22  -22  -21  -21  -21  \n",
       "4   -21  ...  -20  -23  -22  -24  -24    -25  -25  -25  -26  -26  \n",
       "5   -39  ...  -45  -46  -47  -49  -51    -51  -51  -50  -48  -48  \n",
       "6    15  ...   22   22   22   22   22     21   21   21   18   18  \n",
       "7     E  ...    E    E    E    E    E      E    E    E    E    E  \n",
       "8   NaN  ...  NaN  NaN  NaN  NaN  NaN    NaN  NaN  NaN  NaN  NaN  \n",
       "9     5  ...   32   32   28   28   28     65   65   65   73   73  \n",
       "10    1  ...    0    0    0    0    0      0    0    0    0    0  \n",
       "11   76  ...   85   76   82   82   84     85   83   80   78   79  \n",
       "12   --  ...   --   --   --   --   --     --   --   --   --   --  \n",
       "13   --  ...   --   --   --   --   --     --   --   --   --   --  \n",
       "14   --  ...   --   --   --   --   --     --   --   --   --   --  \n",
       "15   --  ...   --   --   --   --   --     --   --   --   --   --  \n",
       "16   --  ...   --   --   --   --   --     --   --   --   --   --  \n",
       "\n",
       "[16 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_location = locations_df.sample(1).iloc[0] \n",
    "\n",
    "print(f\"{random_location}\\n\")\n",
    "\n",
    "lat, lon = random_location['latitude'], random_location['longitude']  \n",
    "\n",
    "## API results\n",
    "url = f\"https://api.weather.gov/points/{lat},{lon}\"\n",
    "\n",
    "response = requests.get(url)\n",
    "main_data = response.json()\n",
    "\n",
    "response = requests.get(main_data['properties']['forecastHourly'])\n",
    "hourly_data = response.json()\n",
    "fields = hourly_data['properties']['periods'][0]\n",
    "\n",
    "print(f\"{fields}\\n\") \n",
    "\n",
    "## Webscraping results \n",
    "url = f\"https://forecast.weather.gov/MapClick.php?lat={lat}&lon={lon}&unit=0&lg=english&FcstType=digital&menu=1\"\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "df = pd.read_html(str(soup.find_all(\"table\")[5]))[0]\n",
    "df = df.iloc[1:17,:]\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results need cleaning (e.g. the column names are in the first row), but you can see that the information is roughly equivalent, with the API offering an `isDaytime` field and the HTML interface offering an actual percentage for sky cover (rather than a snippet like \"Partly Cloudy\"). Depending on how `isDaytime` is measured, it might be more accurate than any estimate of daylight hours we could make based off the scraped data...\n",
    "\n",
    "![alaska-sun](../img/alaska_suntimes.png)\n",
    "\n",
    "Welp -- we'll exclude it for now. Later on we can either access it separately from the API or estimate it ourselves based off standard time tables.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.) Scraping the Data "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each location, the forecast for the next 48 hours is stored in a tabular data table like this: \n",
    "\n",
    "<img src=\"../img/nws_p1.png\" height=400px>\n",
    "\n",
    "The rest of the forecast can be accessed by jumping ahead in 48 hour increments. We do this by adding `&AheadHour=` on the end of the URL and specifying how many hours (48 and 96). \n",
    "\n",
    "The series of transformations required here is quite complex -- a downside of scraping vs using the API -- so I've broken it up into many modular functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## General Utilities \n",
    "def get_soup(url:str) -> BeautifulSoup:\n",
    "  \"\"\"Simple wrapper for getting beautiful soup object from url\"\"\"\n",
    "  result = requests.get(url)\n",
    "  return BeautifulSoup(result.content, \"html.parser\") \n",
    "\n",
    "def flatten(ls:list): \n",
    "  \"\"\"Flattens/unnests a list of lists by one layer\"\"\"\n",
    "  return list(itertools.chain.from_iterable(ls)) \n",
    "\n",
    "def ff_list(ls:list) -> list:\n",
    "  \"\"\"Forward fill the values in a list\"\"\"\n",
    "  for i in range(len(ls)):\n",
    "    if not ls[i] and i > 0:\n",
    "        ls[i] = ls[i-1]\n",
    "  return ls\n",
    "\n",
    "## Specific Utilities\n",
    "def get_nws_url(row:pd.Series) -> str:\n",
    "  \"\"\"\n",
    "  Get url for the next 48 hours of forecasts from latitude and longitude columns\n",
    "  \n",
    "  Args: \n",
    "  row (pd.Series): The current row of the dataframe\n",
    "\n",
    "  Returns: \n",
    "  url (str): The url for the next 48 hours of forecasts\n",
    "  \"\"\"\n",
    "  lat, lon = row[\"latitude\"], row[\"longitude\"]\n",
    "  url = f\"https://forecast.weather.gov/MapClick.php?w0=t&w1=td&w2=wc&w3=sfcwind&w3u=1&w4=sky&w5=pop&w6=rh&w7=rain&w8=thunder&w9=snow&w10=fzg&w11=sleet&w12=fog&AheadHour=0&Submit=Submit&FcstType=digital&textField1={lat}&textField2={lon}&site=all&unit=0&dd=&bw=&menu=1\"\n",
    "  return url\n",
    "\n",
    "def get_last_update(soup:BeautifulSoup) -> dt.datetime:\n",
    "  \"\"\"\n",
    "  Find the \"Last Updated\" value from a BeautifulSoup object, transform to a datetime in AKST\n",
    "\n",
    "  Args:\n",
    "  soup (BeautifulSoup): A Beautiful Soup representation of a particular NWS forecast page\n",
    "\n",
    "  Returns: \n",
    "  last_update_dt (datetime): Datetime representation of time page was last updated (AKST)\n",
    "  \"\"\"\n",
    "  last_update_tag = soup.find('td', string=lambda text: text and 'Last Update:' in text)\n",
    "  last_update_text = re.sub(\"Last Update: |\\s(?=pm|am)|AKST |,\", \"\", last_update_tag.getText())\n",
    "  last_update_dt = dt.datetime.strptime(last_update_text, \"%I:%M%p %b %d %Y\")\n",
    "  return last_update_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Core helper functions\n",
    "def extract_table_data(soup:BeautifulSoup, location:str) -> list:\n",
    "  \"\"\"\n",
    "  Extracts 48hr forecast table data from a Beautiful Soup object as a list of lists\n",
    "\n",
    "  Args: \n",
    "  table_records (list): List of <tr> elements containing NWS forecast data\n",
    "\n",
    "  location (str): The name of the place the forecast is for; used for filling out added \"location\" column \n",
    "\n",
    "  Returns:\n",
    "  table (list): List of lists containing table data \n",
    "  \"\"\"\n",
    "  table_records = soup.find_all(\"table\")[5].find_all(\"tr\")\n",
    "\n",
    "  colspan = table_records[0] # 48hr data is divided into two tables by two colspan elements\n",
    "  table = [tr for  tr in table_records if tr != colspan] # vertically concat tables by removing colspan elements\n",
    "\n",
    "  table = [[ele.getText() for ele in tr.find_all(\"font\")] for tr in table] \n",
    "\n",
    "  # Add location column \n",
    "  location_col = ['location']\n",
    "  location_col.extend([location]*24) # fill out to match length of other columns\n",
    "  table.insert(1, location_col)  # for first half of table\n",
    "  table.insert(19, location_col) # for second half of table\n",
    "\n",
    "  # Add last_update_nws column \n",
    "  last_update_nws = [\"last_update_nws\"]\n",
    "  last_update_nws.extend([get_last_update(soup)] * 24)\n",
    "  table.insert(1, last_update_nws)\n",
    "  table.insert(19, last_update_nws) \n",
    "\n",
    "  return table\n",
    "\n",
    "def transpose_as_dict(table:list) -> dict:\n",
    "  \"\"\"\n",
    "  Takes the list of lists generated by extract_table_data() and transposes it (flip orientation) by casting as a dictionary\n",
    "  \n",
    "  Args:\n",
    "  table (list): list of lists of columnar data generated by extract_table_data()\n",
    "\n",
    "  Returns: \n",
    "  data_map (dict): Dictionary representation of table, transposed and ready to be made into a dataframe\n",
    "  \"\"\"\n",
    "  data_map = {}\n",
    "  for col in table: # Table is still \"landscape-oriented\"\n",
    "    if col[0] not in data_map.keys(): # cols from first half of table\n",
    "      data_map[col[0]] = col[1:]\n",
    "    else: # cols from second half\n",
    "      data_map[col[0]].extend(col[1:])\n",
    "\n",
    "  data_map['Date'] = ff_list(data_map['Date'])\n",
    "\n",
    "  return data_map\n",
    "\n",
    "def transform_df(fcast_dict:dict) -> pd.DataFrame: \n",
    "  \"\"\"\n",
    "  Cast dictionary from transpose_as_dict() to a dataframe and transform\n",
    "\n",
    "  Args: \n",
    "  table (list)\n",
    "  \"\"\"\n",
    "  # Create dataframe\n",
    "  df = pd.DataFrame(fcast_dict)\n",
    "  \n",
    "  # Edit column headers \n",
    "  df.columns = [col.lower() for col in df.columns] \n",
    "  df.rename(columns=lambda x: re.sub('°|\\(|\\)', '', x), inplace=True)\n",
    "  df.rename(columns=lambda x: re.sub('%', 'pct', x), inplace=True)\n",
    "  df.rename(columns=lambda x: re.sub(' ', '_', x.strip()), inplace=True)\n",
    "  \n",
    "  # Replace missing value with Nan\n",
    "  # df.replace({'':np.NaN}, inplace=True)\n",
    "  \n",
    "  ## Datetime Transformations\n",
    "  cur_year = dt.datetime.now().year\n",
    "  dt_strings = df['date'] + '/' + str(cur_year) + ' ' + df['hour_akst'] + ':00 AKST'\n",
    "  # Local time (AKST)\n",
    "  df['lst_datetime'] = pd.to_datetime(dt_strings, format='%m/%d/%Y %H:%M AKST')\n",
    "  # UTC time\n",
    "  akst_offset = dt.timedelta(hours=9)\n",
    "  df['utc_datetime'] = df['lst_datetime'] + akst_offset\n",
    "  \n",
    "  ## Reorder columns \n",
    "  col_names = ['location', 'utc_datetime', 'lst_datetime'] + list(df.columns[4:-2]) + [\"last_update_nws\"]\n",
    "  df = df[col_names]\n",
    "\n",
    "\n",
    "  return df "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the main function to scrape the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forecast_df() -> pd.DataFrame:\n",
    "  \"\"\"Get a dataframe of NWS forecast data for the next 6 days from various points in Alaska\"\"\"\n",
    "\n",
    "  nws_urls = locations_df.apply(get_nws_url, axis=1)\n",
    "  url_map = dict(zip(locations_df['station_location'], nws_urls))\n",
    "\n",
    "  combined_table = []\n",
    "  for location, url in url_map.items():\n",
    "    soup_list = [get_soup(url + f\"&AheadHour={hr}\") for hr in (0,48,96)]\n",
    "    table_list = flatten([extract_table_data(soup, location) for soup in soup_list])\n",
    "    combined_table.extend(table_list)\n",
    "  \n",
    "  forecast_dict = transpose_as_dict(combined_table)\n",
    "\n",
    "  return transform_df(forecast_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_forecast_df()\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.) Uploading the Data \n",
    "\n",
    "Here's the structure of our ETL pipeline as an Entity Relationship Diagram (ERD): \n",
    "\n",
    "![TO-DO_ERD]()\n",
    "\n",
    "The end goal of our pipeline is for the NWS forecasts to be easily evaluated against the historic data from the USCRN. We want to track how the forecast accuracy improves as the forecast gets closer to the current time and to make this easily accessible for analysis later on. We also want to make it easy for data scientists and ML engineers forecasting from the USCRN data to evaluate the performance of their models against the NWS forecasts, with how long the NWS forecasts were made in advance (`utc_datetime - last_update_nws`) being a key parameter. \n",
    "\n",
    "By taking a daily snapshot of the NWS forecast data we keep the data highly denormalized, making this sort of analysis much easier and our pipeline simpler to manage. The downside is that we duplicate a lot of data, especially if a forecast for a given time has not changed at all since the day before.\n",
    "\n",
    "Based on the example table we just made, `3312 * 365 = 1208880` rows added to our main data table per year. We could reduce this greatly by using a nested history column (e.g. a JSON array) but memory is less of a concern for us than ease of analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yaml import full_load\n",
    "from google.cloud import bigquery \n",
    "from google.oauth2 import service_account \n",
    "from google.api_core.exceptions import NotFound\n",
    "\n",
    "# GCP/BigQuery information\n",
    "with open(\"../config/gcp-config.yaml\", \"r\") as fp:\n",
    "  gcp_config = full_load(fp)\n",
    "  \n",
    "PROJECT_ID = gcp_config['project-id']\n",
    "DATASET_ID = gcp_config['dataset-id']\n",
    "STAGING_TABLE_ID = 'nws_staging'\n",
    "MAIN_TABLE_ID = 'nws' \n",
    "\n",
    "# Set credentials\n",
    "key_path = gcp_config['credentials']\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "  key_path, scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    ")\n",
    "\n",
    "# Create client\n",
    "client = bigquery.Client(credentials=credentials, project=PROJECT_ID)\n",
    "\n",
    "def load_staging_table(df:pd.DataFrame) -> None: \n",
    "  \"\"\"Upload dataframe from get_forecast_df() to BigQuery staging table\"\"\"\n",
    "\n",
    "  # Set Schema\n",
    "  schema = [\n",
    "    bigquery.SchemaField(\"location\", \"STRING\", mode=\"REQUIRED\"), \n",
    "    bigquery.SchemaField(\"utc_datetime\", \"DATETIME\", mode=\"REQUIRED\"), \n",
    "    bigquery.SchemaField(\"lst_datetime\", \"DATETIME\", mode=\"REQUIRED\"), \n",
    "    bigquery.SchemaField(\"temperature_f\", \"INTEGER\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"dewpoint_f\", \"INTEGER\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"wind_chill_f\", \"INTEGER\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"surface_wind_mph\", \"INTEGER\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"wind_dir\", \"STRING\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"gust\", \"INTEGER\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"sky_cover_pct\", \"INTEGER\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"precipitation_potential_pct\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"relative_humidity_pct\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"rain\", \"STRING\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"thunder\", \"STRING\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"snow\", \"STRING\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"freezing_rain\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"sleet\", \"STRING\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"fog\", \"STRING\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"last_update_nws\", \"DATETIME\", mode=\"NULLABLE\")\n",
    "  ] \n",
    "\n",
    "  jc = bigquery.LoadJobConfig(\n",
    "    source_format = bigquery.SourceFormat.CSV,\n",
    "    skip_leading_rows=1,\n",
    "    autodetect=False,\n",
    "    schema=schema,\n",
    "    create_disposition=\"CREATE_IF_NEEDED\",\n",
    "    write_disposition=\"WRITE_TRUNCATE\"   \n",
    "  )\n",
    "\n",
    "  # Set target table in BigQuery\n",
    "  full_table_id = f\"{PROJECT_ID}.{DATASET_ID}.{STAGING_TABLE_ID}\"\n",
    "\n",
    "  # Upload to BigQuery\n",
    "  ## If any required columns are missing values, include name of column in error message\n",
    "  try: \n",
    "    job = client.load_table_from_dataframe(df, full_table_id, job_config=jc)\n",
    "    job.result()\n",
    "  except Exception as e:\n",
    "    error_message = str(e)\n",
    "    if 'Required column value for column index' in error_message:\n",
    "      start_index = error_message.index('Required column value for column index') + len('Required column value for column index: ')\n",
    "      end_index = error_message.index(' is missing', start_index)\n",
    "      missing_column_index = int(error_message[start_index:end_index])\n",
    "      missing_column_name = list(df.columns)[missing_column_index]\n",
    "      error_message = error_message[:start_index] + f'{missing_column_name} ({missing_column_index})' + error_message[end_index:]\n",
    "    raise Exception(error_message) \n",
    "  \n",
    "  job = client.load_table_from_dataframe(df, full_table_id, job_config=jc)\n",
    "  job.result()\n",
    "\n",
    "  # Log result \n",
    "  table = client.get_table(full_table_id)\n",
    "  print(f\"Loaded {table.num_rows} rows and {len(table.schema)} columns into {full_table_id}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we make some basic data validations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_check(max_nan_in_row:int) -> None: \n",
    "  \"\"\"\n",
    "  Validates data in staging table by checking for NaNs and logs bad rows\n",
    "  \n",
    "  Args: \n",
    "  max_nan_in_row (int): Sets the threshold (>) for bad rows \n",
    "  \"\"\"\n",
    "\n",
    "  cols = [\"temperature_f\", \"dewpoint_f\", \"wind_chill_f\", \"surface_wind_mph\", \"wind_dir\", \n",
    "  \"gust\", \"sky_cover_pct\", \"precipitation_potential_pct\", \"relative_humidity_pct\", \"rain\", \n",
    "  \"thunder\", \"snow\", \"freezing_rain\", \"sleet\", \"fog\", \"last_update_nws\"]\n",
    "\n",
    "  # select_clause = \",\\n\".join([f\"COUNTIF({col} IS NULL) AS {col}\" for col in cols])\n",
    "  where_clause = \"OR \\n\".join([f\"{col} IS NULL \" for col in cols]).rstrip(\"OR \\n\") # don't need it for last line\n",
    "\n",
    "  nan_query = f\"\"\"\n",
    "    SELECT * \n",
    "    FROM {DATASET_ID}.{STAGING_TABLE_ID}\n",
    "    WHERE {where_clause}\n",
    "  \"\"\"\n",
    "\n",
    "  df = client.query(nan_query).to_dataframe()\n",
    "\n",
    "  # Log results\n",
    "  print(\"NaN Column Counts: \")\n",
    "  print(df[df.isna().sum() > 2])\n",
    "  print(\"\\n\")\n",
    "  \n",
    "  ## Some fields might be null a lot of the time (e.g. gust)\n",
    "  ## We're only really concerned if there are truly bad rows in our dataset with lots of NaNs\n",
    "  \n",
    "  # Log problem records \n",
    "  problem_rows = df[df.isna().sum(axis=1) > max_nan_in_row]\n",
    "  if not problem_rows.empty:\n",
    "    print(f\"Warning: {len(problem_rows)} rows exceed the nan threshold\")\n",
    "    print(problem_rows)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dup_check() -> None: \n",
    "  \"\"\"Validates data in staging table by checking for duplicates in location, utc_datetime, and lst_datetime\"\"\"\n",
    "\n",
    "  dup_query = f\"\"\"\n",
    "  SELECT location, utc_datetime, lst_datetime, COUNT(*) as count\n",
    "  FROM {DATASET_ID}.{STAGING_TABLE_ID}\n",
    "  GROUP BY location, utc_datetime, lst_datetime\n",
    "  HAVING count > 1\n",
    "  \"\"\"\n",
    "\n",
    "  df = client.query(dup_query).to_dataframe()\n",
    "\n",
    "  # Log problem records and drop from staging\n",
    "  if not df.empty:\n",
    "    print(f\"Warning: {len(df)} rows have duplicated time and location values\\n\")\n",
    "    print(df) \n",
    "\n",
    "    # Store snapshot of staging table in a temporary backup table\n",
    "    backup_table = f\"{STAGING_TABLE_ID}_duplicates\"\n",
    "    client.delete_table(f\"{DATASET_ID}.{backup_table}\", not_found_ok=True)  # Delete the table if it already exists\n",
    "    client.copy_table(f\"{DATASET_ID}.{STAGING_TABLE_ID}\", f\"{DATASET_ID}.{backup_table}\")  # Create backup table\n",
    "\n",
    "    # Drop duplicated rows \n",
    "    rows_to_delete = []\n",
    "    for _, row in df.iterrows():\n",
    "      rows_to_delete.append((row['location'], row['utc_datetime'], row['lst_datetime']))\n",
    "\n",
    "    delete_query = f\"\"\"\n",
    "    DELETE FROM {DATASET_ID}.{STAGING_TABLE_ID}\n",
    "    WHERE (location, utc_datetime, lst_datetime) IN ({','.join(map(str, rows_to_delete))})\n",
    "    \"\"\"\n",
    "    client.query(delete_query)\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having validated the data and removed any duplicated rows, we can now insert from our staging table into our main table. In so doing we make two minor changes: \n",
    "1. We replace empty values (`\"\"`) with (`0`) in `gust`\n",
    "2. We add a timestamp column indicating the time of the append (useful if we need to undo something)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_table() -> None: \n",
    "  \"\"\"Insert staging table into the main data table (copies itself if it doesn't exist yet)\"\"\"\n",
    "  \n",
    "  insert_query=f\"\"\"\n",
    "    INSERT INTO {DATASET_ID}.{MAIN_TABLE_ID} (location, utc_datetime, lst_datetime, temperature_f, \n",
    "    dewpoint_f, wind_chill_f, surface_wind_mph, wind_dir, gust, sky_cover_pct, precipitation_potential_pct, \n",
    "    relative_humidity_pct, rain, thunder, snow, freezing_rain, sleet, fog, last_update_nws, date_added)\n",
    "    SELECT \n",
    "      location, utc_datetime, lst_datetime, temperature_f, dewpoint_f, wind_chill_f, surface_wind_mph, \n",
    "      wind_dir, IFNULL(gust, 0) AS gust, sky_cover_pct, precipitation_potential_pct, relative_humidity_pct, rain, thunder, \n",
    "      snow, freezing_rain, sleet, fog, last_update_nws, TIME_STAMP() as date_added\n",
    "    FROM {DATASET_ID}.{STAGING_TABLE_ID}\n",
    "    \"\"\"\n",
    "\n",
    "  try: \n",
    "    query_job = client.query(insert_query) \n",
    "    query_job.result()\n",
    "  except NotFound:\n",
    "    print(f\"Table {DATASET_ID}.{MAIN_TABLE_ID} does not exist. Creating.\")\n",
    "    create_query = f\"\"\"\n",
    "      CREATE TABLE {DATASET_ID}.{MAIN_TABLE_ID}\n",
    "      AS\n",
    "      SELECT \n",
    "        location, utc_datetime, lst_datetime, temperature_f, dewpoint_f, wind_chill_f, surface_wind_mph, \n",
    "        wind_dir, IFNULL(gust, 0) AS gust, sky_cover_pct, precipitation_potential_pct, relative_humidity_pct, rain, thunder, \n",
    "        snow, freezing_rain, sleet, fog, last_update_nws, TIME_STAMP() as date_added\n",
    "      FROM {DATASET_ID}.{STAGING_TABLE_ID}\n",
    "    \"\"\"\n",
    "    client.query(create_query)\n",
    "    print(f\"Create table {DATASET_ID}.{MAIN_TABLE_ID}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final test**: Let's now call all the functions in sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = get_forecast_df()\n",
    "## Duplicating some rows to test dup_check()\n",
    "# rows_to_dup = df.sample(10)\n",
    "\n",
    "# df = pd.concat([df, rows_to_dup])\n",
    "\n",
    "load_staging_table(df)\n",
    "nan_check(2)\n",
    "dup_check()\n",
    "insert_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "location                       0\n",
       "utc_datetime                   0\n",
       "lst_datetime                   0\n",
       "temperature_f                  0\n",
       "dewpoint_f                     0\n",
       "wind_chill_f                   0\n",
       "surface_wind_mph               0\n",
       "wind_dir                       0\n",
       "gust                           0\n",
       "sky_cover_pct                  0\n",
       "precipitation_potential_pct    0\n",
       "relative_humidity_pct          0\n",
       "rain                           0\n",
       "thunder                        0\n",
       "snow                           0\n",
       "freezing_rain                  0\n",
       "sleet                          0\n",
       "fog                            0\n",
       "last_update_nws                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_table() -> None: \n",
    "  \"\"\"Insert staging table into the main data table (copies itself if it doesn't exist yet)\"\"\"\n",
    "  \n",
    "  insert_query=f\"\"\"\n",
    "    INSERT INTO {DATASET_ID}.{MAIN_TABLE_ID} (location, utc_datetime, lst_datetime, temperature_f, \n",
    "    dewpoint_f, wind_chill_f, surface_wind_mph, wind_dir, gust, sky_cover_pct, precipitation_potential_pct, \n",
    "    relative_humidity_pct, rain, thunder, snow, freezing_rain, sleet, fog, last_update_nws, date_added)\n",
    "    SELECT \n",
    "      location, utc_datetime, lst_datetime, temperature_f, dewpoint_f, wind_chill_f, surface_wind_mph, \n",
    "      wind_dir, IFNULL(gust, 0) AS gust, sky_cover_pct, precipitation_potential_pct, relative_humidity_pct, rain, thunder, \n",
    "      snow, freezing_rain, sleet, fog, last_update_nws, TIME_STAMP() as date_added\n",
    "    FROM {DATASET_ID}.{STAGING_TABLE_ID}\n",
    "    \"\"\"\n",
    "\n",
    "  try: \n",
    "    query_job = client.query(insert_query) \n",
    "    query_job.result()\n",
    "  except NotFound:\n",
    "    print(f\"Table {DATASET_ID}.{MAIN_TABLE_ID} does not exist. Creating.\")\n",
    "    create_query = f\"\"\"\n",
    "      CREATE TABLE {DATASET_ID}.{MAIN_TABLE_ID}\n",
    "      AS\n",
    "      SELECT \n",
    "        location, utc_datetime, lst_datetime, temperature_f, dewpoint_f, wind_chill_f, surface_wind_mph, \n",
    "        wind_dir, IFNULL(gust, 0) AS gust, sky_cover_pct, precipitation_potential_pct, relative_humidity_pct, rain, thunder, \n",
    "        snow, freezing_rain, sleet, fog, last_update_nws, TIME_STAMP() as date_added\n",
    "      FROM {DATASET_ID}.{STAGING_TABLE_ID}\n",
    "    \"\"\"\n",
    "    client.query(create_query)\n",
    "    print(f\"Create table {DATASET_ID}.{MAIN_TABLE_ID}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bq_result](../img/success_nws_staging.png)\n",
    "\n",
    "\n",
    "Let's also add some duplicate rows to df just make sure it works correctly: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "70623b801652781c2389d9f74154af1ef3dd8a50bfe8b7cd6824c1648ddc5ad0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
