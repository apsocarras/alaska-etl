{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NWS Forecast Data\n",
    "\n",
    "This notebook explains how to scrape, transform, and upload the NWS's hourly weather forecast for the next 6 days** into a dataset in BigQuery. The resulting script can be set to run at regular intervals as an Airflow DAG or a function in Cloud Functions. \n",
    "\n",
    "** *For some reason, the hourly forecast doesn't quite extend to a full week, but only 6.5 days. To keep the math easier, we will only scrape the next 6 days -- in the end, this won't affect our pipeline once we have it updating continuously.*\n",
    "\n",
    "We will collect hourly forecasts for the locations of 23 USCRN data collection stations in Alaska -- the presence of these stations will enable ourselves and any other users of our dataset to evaluate the accuracy of the forecasts.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why use scraping over `api.weather.gov`?\n",
    "\n",
    "Generally speaking, if a website offers an API to access its data then it's a good bet to use it. So why not just use `api.weather.gov`?\n",
    "\n",
    "The main reason I've chose webscraping for the NWS part of the project is that at times the `api.weather.gov` has given me `500: Internal Server Error` responses even when the HTML data interface is still accessible. The level of information provided is essentially the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "import datetime as dt \n",
    "import itertools\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_df = pd.read_csv(\"../data/locations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "station_location    Deadhorse\n",
      "wbanno                  26565\n",
      "longitude             -148.46\n",
      "latitude                70.16\n",
      "Name: 13, dtype: object\n",
      "\n",
      "{'number': 1, 'name': '', 'startTime': '2023-02-28T05:00:00-09:00', 'endTime': '2023-02-28T06:00:00-09:00', 'isDaytime': False, 'temperature': -24, 'temperatureUnit': 'F', 'temperatureTrend': None, 'probabilityOfPrecipitation': {'unitCode': 'wmoUnit:percent', 'value': 3}, 'dewpoint': {'unitCode': 'wmoUnit:degC', 'value': -33.333333333333336}, 'relativeHumidity': {'unitCode': 'wmoUnit:percent', 'value': 79}, 'windSpeed': '5 mph', 'windDirection': 'SW', 'icon': 'https://api.weather.gov/icons/land/night/sct,3?size=small', 'shortForecast': 'Partly Cloudy', 'detailedForecast': ''}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Date</td>\n",
       "      <td>02/28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>03/01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hour (AKST)</td>\n",
       "      <td>05</td>\n",
       "      <td>06</td>\n",
       "      <td>07</td>\n",
       "      <td>08</td>\n",
       "      <td>09</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>00</td>\n",
       "      <td>01</td>\n",
       "      <td>02</td>\n",
       "      <td>03</td>\n",
       "      <td>04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Temperature (°F)</td>\n",
       "      <td>-24</td>\n",
       "      <td>-24</td>\n",
       "      <td>-25</td>\n",
       "      <td>-25</td>\n",
       "      <td>-24</td>\n",
       "      <td>-22</td>\n",
       "      <td>-19</td>\n",
       "      <td>-17</td>\n",
       "      <td>-16</td>\n",
       "      <td>...</td>\n",
       "      <td>-16</td>\n",
       "      <td>-17</td>\n",
       "      <td>-18</td>\n",
       "      <td>-20</td>\n",
       "      <td>-21</td>\n",
       "      <td>-22</td>\n",
       "      <td>-22</td>\n",
       "      <td>-21</td>\n",
       "      <td>-21</td>\n",
       "      <td>-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dewpoint (°F)</td>\n",
       "      <td>-28</td>\n",
       "      <td>-28</td>\n",
       "      <td>-29</td>\n",
       "      <td>-30</td>\n",
       "      <td>-30</td>\n",
       "      <td>-28</td>\n",
       "      <td>-25</td>\n",
       "      <td>-22</td>\n",
       "      <td>-21</td>\n",
       "      <td>...</td>\n",
       "      <td>-20</td>\n",
       "      <td>-23</td>\n",
       "      <td>-22</td>\n",
       "      <td>-24</td>\n",
       "      <td>-24</td>\n",
       "      <td>-25</td>\n",
       "      <td>-25</td>\n",
       "      <td>-25</td>\n",
       "      <td>-26</td>\n",
       "      <td>-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wind Chill (°F)</td>\n",
       "      <td>-38</td>\n",
       "      <td>-41</td>\n",
       "      <td>-41</td>\n",
       "      <td>-41</td>\n",
       "      <td>-44</td>\n",
       "      <td>-42</td>\n",
       "      <td>-39</td>\n",
       "      <td>-41</td>\n",
       "      <td>-39</td>\n",
       "      <td>...</td>\n",
       "      <td>-45</td>\n",
       "      <td>-46</td>\n",
       "      <td>-47</td>\n",
       "      <td>-49</td>\n",
       "      <td>-51</td>\n",
       "      <td>-51</td>\n",
       "      <td>-51</td>\n",
       "      <td>-50</td>\n",
       "      <td>-48</td>\n",
       "      <td>-48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Surface Wind (mph)</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Wind Dir</td>\n",
       "      <td>SW</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>...</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Gust</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sky Cover (%)</td>\n",
       "      <td>28</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Precipitation Potential (%)</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Relative Humidity (%)</td>\n",
       "      <td>80</td>\n",
       "      <td>82</td>\n",
       "      <td>79</td>\n",
       "      <td>74</td>\n",
       "      <td>71</td>\n",
       "      <td>72</td>\n",
       "      <td>76</td>\n",
       "      <td>78</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>85</td>\n",
       "      <td>76</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>84</td>\n",
       "      <td>85</td>\n",
       "      <td>83</td>\n",
       "      <td>80</td>\n",
       "      <td>78</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Rain</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>...</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Thunder</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>...</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Snow</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>...</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Freezing Rain</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>...</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Sleet</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>...</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             0      1    2    3    4    5    6    7    8   \\\n",
       "1                          Date  02/28  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "2                   Hour (AKST)     05   06   07   08   09   10   11   12   \n",
       "3              Temperature (°F)    -24  -24  -25  -25  -24  -22  -19  -17   \n",
       "4                 Dewpoint (°F)    -28  -28  -29  -30  -30  -28  -25  -22   \n",
       "5               Wind Chill (°F)    -38  -41  -41  -41  -44  -42  -39  -41   \n",
       "6            Surface Wind (mph)      5    6    6    6    9    9    9   15   \n",
       "7                      Wind Dir     SW    S    S    S    E    E    E    E   \n",
       "8                          Gust    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "9                 Sky Cover (%)     28   22   22   22   14   14   14    5   \n",
       "10  Precipitation Potential (%)      3    3    3    3    2    2    2    1   \n",
       "11        Relative Humidity (%)     80   82   79   74   71   72   76   78   \n",
       "12                         Rain     --   --   --   --   --   --   --   --   \n",
       "13                      Thunder     --   --   --   --   --   --   --   --   \n",
       "14                         Snow     --   --   --   --   --   --   --   --   \n",
       "15                Freezing Rain     --   --   --   --   --   --   --   --   \n",
       "16                        Sleet     --   --   --   --   --   --   --   --   \n",
       "\n",
       "     9   ...   15   16   17   18   19     20   21   22   23   24  \n",
       "1   NaN  ...  NaN  NaN  NaN  NaN  NaN  03/01  NaN  NaN  NaN  NaN  \n",
       "2    13  ...   19   20   21   22   23     00   01   02   03   04  \n",
       "3   -16  ...  -16  -17  -18  -20  -21    -22  -22  -21  -21  -21  \n",
       "4   -21  ...  -20  -23  -22  -24  -24    -25  -25  -25  -26  -26  \n",
       "5   -39  ...  -45  -46  -47  -49  -51    -51  -51  -50  -48  -48  \n",
       "6    15  ...   22   22   22   22   22     21   21   21   18   18  \n",
       "7     E  ...    E    E    E    E    E      E    E    E    E    E  \n",
       "8   NaN  ...  NaN  NaN  NaN  NaN  NaN    NaN  NaN  NaN  NaN  NaN  \n",
       "9     5  ...   32   32   28   28   28     65   65   65   73   73  \n",
       "10    1  ...    0    0    0    0    0      0    0    0    0    0  \n",
       "11   76  ...   85   76   82   82   84     85   83   80   78   79  \n",
       "12   --  ...   --   --   --   --   --     --   --   --   --   --  \n",
       "13   --  ...   --   --   --   --   --     --   --   --   --   --  \n",
       "14   --  ...   --   --   --   --   --     --   --   --   --   --  \n",
       "15   --  ...   --   --   --   --   --     --   --   --   --   --  \n",
       "16   --  ...   --   --   --   --   --     --   --   --   --   --  \n",
       "\n",
       "[16 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_location = locations_df.sample(1).iloc[0] \n",
    "\n",
    "print(f\"{random_location}\\n\")\n",
    "\n",
    "lat, lon = random_location['latitude'], random_location['longitude']  \n",
    "\n",
    "## API results\n",
    "url = f\"https://api.weather.gov/points/{lat},{lon}\"\n",
    "\n",
    "response = requests.get(url)\n",
    "main_data = response.json()\n",
    "\n",
    "response = requests.get(main_data['properties']['forecastHourly'])\n",
    "hourly_data = response.json()\n",
    "fields = hourly_data['properties']['periods'][0]\n",
    "\n",
    "print(f\"{fields}\\n\") \n",
    "\n",
    "## Webscraping results \n",
    "url = f\"https://forecast.weather.gov/MapClick.php?lat={lat}&lon={lon}&unit=0&lg=english&FcstType=digital&menu=1\"\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "df = pd.read_html(str(soup.find_all(\"table\")[5]))[0]\n",
    "df = df.iloc[1:17,:]\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results need cleaning (e.g. the column names are in the first row), but you can see that the information is roughly equivalent, with the API offering an `isDaytime` field and the HTML interface offering an actual percentage for sky cover (rather than a snippet like \"Partly Cloudy\"). Depending on how `isDaytime` is measured, it might be more accurate than any estimate of daylight hours we could make based off the scraped data...\n",
    "\n",
    "![alaska-sun](../img/alaska_suntimes.png)\n",
    "\n",
    "Welp -- we'll exclude it for now. Later on we can either access it separately from the API or estimate it ourselves based off standard time tables.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.) Scraping the Data "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each location, the forecast for the next 48 hours is stored in a tabular data table like this: \n",
    "\n",
    "<img src=\"../img/nws_p1.png\" height=400px>\n",
    "\n",
    "The rest of the forecast can be accessed by jumping ahead in 48 hour increments. We do this by adding `&AheadHour=` on the end of the URL and specifying how many hours (48 and 96). \n",
    "\n",
    "The series of transformations required here is quite complex -- a downside of scraping vs using the API -- so I've broken it up into many modular functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## General Utilities \n",
    "def get_soup(url:str) -> BeautifulSoup:\n",
    "  \"\"\"Simple wrapper for getting beautiful soup object from url\"\"\"\n",
    "  result = requests.get(url)\n",
    "  return BeautifulSoup(result.content, \"html.parser\") \n",
    "\n",
    "def flatten(ls:list): \n",
    "  \"\"\"Flattens/unnests a list of lists by one layer\"\"\"\n",
    "  return list(itertools.chain.from_iterable(ls)) \n",
    "\n",
    "def ff_list(ls:list) -> list:\n",
    "  \"\"\"Forward fill the values in a list\"\"\"\n",
    "  for i in range(len(ls)):\n",
    "    if not ls[i] and i > 0:\n",
    "        ls[i] = ls[i-1]\n",
    "  return ls\n",
    "\n",
    "## Specific Utilities\n",
    "def get_nws_url(row:pd.Series) -> str:\n",
    "  \"\"\"\n",
    "  Get url for the next 48 hours of forecasts from latitude and longitude columns\n",
    "  \n",
    "  Args: \n",
    "  row (pd.Series): The current row of the dataframe\n",
    "\n",
    "  Returns: \n",
    "  url (str): The url for the next 48 hours of forecasts\n",
    "  \"\"\"\n",
    "  lat, lon = row[\"latitude\"], row[\"longitude\"]\n",
    "  url = f\"https://forecast.weather.gov/MapClick.php?w0=t&w1=td&w2=wc&w3=sfcwind&w3u=1&w4=sky&w5=pop&w6=rh&w7=rain&w8=thunder&w9=snow&w10=fzg&w11=sleet&w12=fog&AheadHour=0&Submit=Submit&FcstType=digital&textField1={lat}&textField2={lon}&site=all&unit=0&dd=&bw=&menu=1\"\n",
    "  return url\n",
    "\n",
    "def get_last_update(soup:BeautifulSoup) -> dt.datetime:\n",
    "  \"\"\"\n",
    "  Find the \"Last Updated\" value from a BeautifulSoup object, transform to a datetime in AKST\n",
    "\n",
    "  Args:\n",
    "  soup (BeautifulSoup): A Beautiful Soup representation of a particular NWS forecast page\n",
    "\n",
    "  Returns: \n",
    "  last_update_dt (datetime): Datetime representation of time page was last updated (AKST)\n",
    "  \"\"\"\n",
    "  last_update_tag = soup.find('td', string=lambda text: text and 'Last Update:' in text)\n",
    "  last_update_text = re.sub(\"Last Update: |\\s(?=pm|am)|AKST |,\", \"\", last_update_tag.getText())\n",
    "  last_update_dt = dt.datetime.strptime(last_update_text, \"%I:%M%p %b %d %Y\")\n",
    "  return last_update_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Core helper functions\n",
    "def extract_table_data(soup:BeautifulSoup, location:str) -> list:\n",
    "  \"\"\"\n",
    "  Extracts 48hr forecast table data from a Beautiful Soup object as a list of lists\n",
    "\n",
    "  Args: \n",
    "  table_records (list): List of <tr> elements containing NWS forecast data\n",
    "\n",
    "  location (str): The name of the place the forecast is for; used for filling out added \"location\" column \n",
    "\n",
    "  Returns:\n",
    "  table (list): List of lists containing table data \n",
    "  \"\"\"\n",
    "  table_records = soup.find_all(\"table\")[5].find_all(\"tr\")\n",
    "\n",
    "  colspan = table_records[0] # 48hr data is divided into two tables by two colspan elements\n",
    "  table = [tr for  tr in table_records if tr != colspan] # vertically concat tables by removing colspan elements\n",
    "\n",
    "  table = [[ele.getText() for ele in tr.find_all(\"font\")] for tr in table] \n",
    "\n",
    "  # Add location column \n",
    "  location_col = ['location']\n",
    "  location_col.extend([location]*24) # fill out to match length of other columns\n",
    "  table.insert(1, location_col)  # for first half of table\n",
    "  table.insert(19, location_col) # for second half of table\n",
    "\n",
    "  # Add last_update_nws column \n",
    "  last_update_nws = [\"last_update_nws\"]\n",
    "  last_update_nws.extend([get_last_update(soup)] * 24)\n",
    "  table.insert(1, last_update_nws)\n",
    "  table.insert(19, last_update_nws) \n",
    "\n",
    "  return table\n",
    "\n",
    "def transpose_as_dict(table:list) -> dict:\n",
    "  \"\"\"\n",
    "  Takes the list of lists generated by extract_table_data() and transposes it (flip orientation) by casting as a dictionary\n",
    "  \n",
    "  Args:\n",
    "  table (list): list of lists of columnar data generated by extract_table_data()\n",
    "\n",
    "  Returns: \n",
    "  data_map (dict): Dictionary representation of table, transposed and ready to be made into a dataframe\n",
    "  \"\"\"\n",
    "  data_map = {}\n",
    "  for col in table: # Table is still \"landscape-oriented\"\n",
    "    if col[0] not in data_map.keys(): # cols from first half of table\n",
    "      data_map[col[0]] = col[1:]\n",
    "    else: # cols from second half\n",
    "      data_map[col[0]].extend(col[1:])\n",
    "\n",
    "  data_map['Date'] = ff_list(data_map['Date'])\n",
    "\n",
    "  return data_map\n",
    "\n",
    "def transform_df(fcast_dict:dict) -> pd.DataFrame: \n",
    "  \"\"\"\n",
    "  Cast dictionary from transpose_as_dict() to a dataframe and transform\n",
    "\n",
    "  Args: \n",
    "  table (list)\n",
    "  \"\"\"\n",
    "  # Create dataframe\n",
    "  df = pd.DataFrame(fcast_dict)\n",
    "  \n",
    "  # Edit column headers \n",
    "  df.columns = [col.lower() for col in df.columns] \n",
    "  df.rename(columns=lambda x: re.sub('°|\\(|\\)', '', x), inplace=True)\n",
    "  df.rename(columns=lambda x: re.sub('%', 'pct', x), inplace=True)\n",
    "  df.rename(columns=lambda x: re.sub(' ', '_', x.strip()), inplace=True)\n",
    "  \n",
    "  # # Replace missing value indicators with Nan -- Leave Validation for SQL step\n",
    "  # df.replace({'':np.NaN, '--':np.NaN}, inplace=True)\n",
    "  \n",
    "  ## Datetime Transformations\n",
    "  cur_year = dt.datetime.now().year\n",
    "  dt_strings = df['date'] + '/' + str(cur_year) + ' ' + df['hour_akst'] + ':00 AKST'\n",
    "  # Local time (AKST)\n",
    "  df['lst_datetime'] = pd.to_datetime(dt_strings, format='%m/%d/%Y %H:%M AKST')\n",
    "  # UTC time\n",
    "  akst_offset = dt.timedelta(hours=9)\n",
    "  df['utc_datetime'] = df['lst_datetime'] + akst_offset\n",
    "  \n",
    "  ## Reorder columns \n",
    "  col_names = ['location', 'utc_datetime', 'lst_datetime'] + list(df.columns[4:-2]) + [\"last_update_nws\"]\n",
    "  df = df[col_names]\n",
    "\n",
    "\n",
    "  return df "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the main function to scrape the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forecast_df() -> pd.DataFrame:\n",
    "  \"\"\"Get a dataframe of NWS forecast data for the next 6 days from various points in Alaska\"\"\"\n",
    "\n",
    "  nws_urls = locations_df.apply(get_nws_url, axis=1)\n",
    "  url_map = dict(zip(locations_df['station_location'], nws_urls))\n",
    "\n",
    "  combined_table = []\n",
    "  for location, url in url_map.items():\n",
    "    soup_list = [get_soup(url + f\"&AheadHour={hr}\") for hr in (0,48,96)]\n",
    "    table_list = flatten([extract_table_data(soup, location) for soup in soup_list])\n",
    "    combined_table.extend(table_list)\n",
    "  \n",
    "  forecast_dict = transpose_as_dict(combined_table)\n",
    "\n",
    "  return transform_df(forecast_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>utc_datetime</th>\n",
       "      <th>lst_datetime</th>\n",
       "      <th>temperature_f</th>\n",
       "      <th>dewpoint_f</th>\n",
       "      <th>wind_chill_f</th>\n",
       "      <th>surface_wind_mph</th>\n",
       "      <th>wind_dir</th>\n",
       "      <th>gust</th>\n",
       "      <th>sky_cover_pct</th>\n",
       "      <th>precipitation_potential_pct</th>\n",
       "      <th>relative_humidity_pct</th>\n",
       "      <th>rain</th>\n",
       "      <th>thunder</th>\n",
       "      <th>snow</th>\n",
       "      <th>freezing_rain</th>\n",
       "      <th>sleet</th>\n",
       "      <th>fog</th>\n",
       "      <th>last_update_nws</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fairbanks</td>\n",
       "      <td>2023-02-28 19:00:00</td>\n",
       "      <td>2023-02-28 10:00:00</td>\n",
       "      <td>-14</td>\n",
       "      <td>-24</td>\n",
       "      <td>-37</td>\n",
       "      <td>15</td>\n",
       "      <td>E</td>\n",
       "      <td></td>\n",
       "      <td>76</td>\n",
       "      <td>19</td>\n",
       "      <td>57</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>SChc</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>2023-02-28 04:50:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fairbanks</td>\n",
       "      <td>2023-02-28 20:00:00</td>\n",
       "      <td>2023-02-28 11:00:00</td>\n",
       "      <td>-9</td>\n",
       "      <td>-19</td>\n",
       "      <td>-31</td>\n",
       "      <td>15</td>\n",
       "      <td>E</td>\n",
       "      <td></td>\n",
       "      <td>76</td>\n",
       "      <td>19</td>\n",
       "      <td>61</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>SChc</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>2023-02-28 04:50:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fairbanks</td>\n",
       "      <td>2023-02-28 21:00:00</td>\n",
       "      <td>2023-02-28 12:00:00</td>\n",
       "      <td>-6</td>\n",
       "      <td>-14</td>\n",
       "      <td>-26</td>\n",
       "      <td>14</td>\n",
       "      <td>E</td>\n",
       "      <td></td>\n",
       "      <td>97</td>\n",
       "      <td>79</td>\n",
       "      <td>65</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>Ocnl</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>2023-02-28 04:50:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fairbanks</td>\n",
       "      <td>2023-02-28 22:00:00</td>\n",
       "      <td>2023-02-28 13:00:00</td>\n",
       "      <td>-2</td>\n",
       "      <td>-10</td>\n",
       "      <td>-22</td>\n",
       "      <td>14</td>\n",
       "      <td>E</td>\n",
       "      <td></td>\n",
       "      <td>97</td>\n",
       "      <td>79</td>\n",
       "      <td>69</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>Ocnl</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>2023-02-28 04:50:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fairbanks</td>\n",
       "      <td>2023-02-28 23:00:00</td>\n",
       "      <td>2023-02-28 14:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>-7</td>\n",
       "      <td>-18</td>\n",
       "      <td>14</td>\n",
       "      <td>E</td>\n",
       "      <td></td>\n",
       "      <td>97</td>\n",
       "      <td>79</td>\n",
       "      <td>71</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>Ocnl</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>2023-02-28 04:50:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3307</th>\n",
       "      <td>Aleknagik</td>\n",
       "      <td>2023-03-06 14:00:00</td>\n",
       "      <td>2023-03-06 05:00:00</td>\n",
       "      <td>29</td>\n",
       "      <td>26</td>\n",
       "      <td></td>\n",
       "      <td>14</td>\n",
       "      <td>SE</td>\n",
       "      <td></td>\n",
       "      <td>90</td>\n",
       "      <td>56</td>\n",
       "      <td>88</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>Lkly</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>2023-02-28 03:58:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3308</th>\n",
       "      <td>Aleknagik</td>\n",
       "      <td>2023-03-06 15:00:00</td>\n",
       "      <td>2023-03-06 06:00:00</td>\n",
       "      <td>30</td>\n",
       "      <td>26</td>\n",
       "      <td></td>\n",
       "      <td>14</td>\n",
       "      <td>SE</td>\n",
       "      <td></td>\n",
       "      <td>90</td>\n",
       "      <td>56</td>\n",
       "      <td>86</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>Lkly</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>2023-02-28 03:58:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3309</th>\n",
       "      <td>Aleknagik</td>\n",
       "      <td>2023-03-06 16:00:00</td>\n",
       "      <td>2023-03-06 07:00:00</td>\n",
       "      <td>30</td>\n",
       "      <td>25</td>\n",
       "      <td></td>\n",
       "      <td>14</td>\n",
       "      <td>SE</td>\n",
       "      <td></td>\n",
       "      <td>90</td>\n",
       "      <td>56</td>\n",
       "      <td>83</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>Lkly</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>2023-02-28 03:58:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3310</th>\n",
       "      <td>Aleknagik</td>\n",
       "      <td>2023-03-06 17:00:00</td>\n",
       "      <td>2023-03-06 08:00:00</td>\n",
       "      <td>30</td>\n",
       "      <td>24</td>\n",
       "      <td></td>\n",
       "      <td>14</td>\n",
       "      <td>SE</td>\n",
       "      <td></td>\n",
       "      <td>90</td>\n",
       "      <td>56</td>\n",
       "      <td>81</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>Lkly</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>2023-02-28 03:58:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3311</th>\n",
       "      <td>Aleknagik</td>\n",
       "      <td>2023-03-06 18:00:00</td>\n",
       "      <td>2023-03-06 09:00:00</td>\n",
       "      <td>30</td>\n",
       "      <td>24</td>\n",
       "      <td></td>\n",
       "      <td>14</td>\n",
       "      <td>SE</td>\n",
       "      <td></td>\n",
       "      <td>90</td>\n",
       "      <td>56</td>\n",
       "      <td>79</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>Lkly</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>2023-02-28 03:58:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3312 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       location        utc_datetime        lst_datetime temperature_f  \\\n",
       "0     Fairbanks 2023-02-28 19:00:00 2023-02-28 10:00:00           -14   \n",
       "1     Fairbanks 2023-02-28 20:00:00 2023-02-28 11:00:00            -9   \n",
       "2     Fairbanks 2023-02-28 21:00:00 2023-02-28 12:00:00            -6   \n",
       "3     Fairbanks 2023-02-28 22:00:00 2023-02-28 13:00:00            -2   \n",
       "4     Fairbanks 2023-02-28 23:00:00 2023-02-28 14:00:00             0   \n",
       "...         ...                 ...                 ...           ...   \n",
       "3307  Aleknagik 2023-03-06 14:00:00 2023-03-06 05:00:00            29   \n",
       "3308  Aleknagik 2023-03-06 15:00:00 2023-03-06 06:00:00            30   \n",
       "3309  Aleknagik 2023-03-06 16:00:00 2023-03-06 07:00:00            30   \n",
       "3310  Aleknagik 2023-03-06 17:00:00 2023-03-06 08:00:00            30   \n",
       "3311  Aleknagik 2023-03-06 18:00:00 2023-03-06 09:00:00            30   \n",
       "\n",
       "     dewpoint_f wind_chill_f surface_wind_mph wind_dir gust sky_cover_pct  \\\n",
       "0           -24          -37               15        E                 76   \n",
       "1           -19          -31               15        E                 76   \n",
       "2           -14          -26               14        E                 97   \n",
       "3           -10          -22               14        E                 97   \n",
       "4            -7          -18               14        E                 97   \n",
       "...         ...          ...              ...      ...  ...           ...   \n",
       "3307         26                            14       SE                 90   \n",
       "3308         26                            14       SE                 90   \n",
       "3309         25                            14       SE                 90   \n",
       "3310         24                            14       SE                 90   \n",
       "3311         24                            14       SE                 90   \n",
       "\n",
       "     precipitation_potential_pct relative_humidity_pct rain thunder  snow  \\\n",
       "0                             19                    57   --      --  SChc   \n",
       "1                             19                    61   --      --  SChc   \n",
       "2                             79                    65   --      --  Ocnl   \n",
       "3                             79                    69   --      --  Ocnl   \n",
       "4                             79                    71   --      --  Ocnl   \n",
       "...                          ...                   ...  ...     ...   ...   \n",
       "3307                          56                    88   --      --  Lkly   \n",
       "3308                          56                    86   --      --  Lkly   \n",
       "3309                          56                    83   --      --  Lkly   \n",
       "3310                          56                    81   --      --  Lkly   \n",
       "3311                          56                    79   --      --  Lkly   \n",
       "\n",
       "     freezing_rain sleet fog     last_update_nws  \n",
       "0               --    --  -- 2023-02-28 04:50:00  \n",
       "1               --    --  -- 2023-02-28 04:50:00  \n",
       "2               --    --  -- 2023-02-28 04:50:00  \n",
       "3               --    --  -- 2023-02-28 04:50:00  \n",
       "4               --    --  -- 2023-02-28 04:50:00  \n",
       "...            ...   ...  ..                 ...  \n",
       "3307            --    --  -- 2023-02-28 03:58:00  \n",
       "3308            --    --  -- 2023-02-28 03:58:00  \n",
       "3309            --    --  -- 2023-02-28 03:58:00  \n",
       "3310            --    --  -- 2023-02-28 03:58:00  \n",
       "3311            --    --  -- 2023-02-28 03:58:00  \n",
       "\n",
       "[3312 rows x 19 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_forecast_df()\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.) Uploading the Data \n",
    "\n",
    "Here's the structure of our ETL pipeline as an Entity Relationship Diagram (ERD): \n",
    "\n",
    "![TO-DO_ERD]()\n",
    "\n",
    "The end goal of our pipeline is for the NWS forecasts to be easily evaluated against the historic data from the USCRN. We want to track how the forecast accuracy improves as the forecast gets closer to the current time and to make this easily accessible for analysis later on. We also want to make it easy for data scientists and ML engineers forecasting from the USCRN data to evaluate the performance of their models against the NWS forecasts, with how long the NWS forecasts were made in advance (`utc_datetime - last_update_nws`) being a key parameter. \n",
    "\n",
    "By taking a daily snapshot of the NWS forecast data we keep the data highly denormalized, making this sort of analysis much easier and our pipeline simpler to manage. The downside is that we duplicate a lot of data, especially if a forecast for a given time has not changed at all since the day before.\n",
    "\n",
    "Based on the example table we just made, `3312 * 365 = 1208880` rows added to our main data table per year. We could reduce this greatly by using a nested history column (e.g. a JSON array) but memory is less of a concern for us than ease of analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yaml import full_load\n",
    "from google.cloud import bigquery \n",
    "from google.oauth2 import service_account \n",
    "\n",
    "# GCP/BigQuery information\n",
    "with open(\"../config/gcp-config.yaml\", \"r\") as fp:\n",
    "  gcp_config = full_load(fp)\n",
    "\n",
    "  \n",
    "PROJECT_ID = gcp_config['project-id']\n",
    "DATASET_ID = gcp_config['dataset-id']\n",
    "STAGING_TABLE_ID = 'nws_staging'\n",
    "MAIN_TABLE_ID = 'nws' \n",
    "\n",
    "# Set credentials\n",
    "key_path = gcp_config['credentials']\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "  key_path, scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    ")\n",
    "\n",
    "# Create client\n",
    "client = bigquery.Client(credentials=credentials, project=PROJECT_ID)\n",
    "\n",
    "def load_staging_table(df:pd.DataFrame) -> None: \n",
    "  \"\"\"Upload dataframe from get_forecast_df() to BigQuery staging table\"\"\"\n",
    "\n",
    "  # Set Schema\n",
    "  schema = [\n",
    "    bigquery.SchemaField(\"location\", \"STRING\", mode=\"REQUIRED\"), \n",
    "    bigquery.SchemaField(\"utc_datetime\", \"DATETIME\", mode=\"REQUIRED\"), \n",
    "    bigquery.SchemaField(\"lst_datetime\", \"DATETIME\", mode=\"REQUIRED\"), \n",
    "    bigquery.SchemaField(\"temperature_f\", \"INTEGER\", mode=\"REQUIRED\"), \n",
    "    bigquery.SchemaField(\"dewpoint_f\", \"INTEGER\", mode=\"REQUIRED\"), \n",
    "    bigquery.SchemaField(\"wind_chill_f\", \"INTEGER\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"surface_wind_mph\", \"INTEGER\", mode=\"REQUIRED\"), \n",
    "    bigquery.SchemaField(\"wind_dir\", \"STRING\", mode=\"REQUIRED\"), \n",
    "    bigquery.SchemaField(\"gust\", \"INTEGER\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"sky_cover_pct\", \"INTEGER\", mode=\"REQUIRED\"), \n",
    "    bigquery.SchemaField(\"precipitation_potential_pct\", \"FLOAT\", mode=\"REQUIRED\"), \n",
    "    bigquery.SchemaField(\"relative_humidity_pct\", \"FLOAT\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"rain\", \"STRING\", mode=\"REQUIRED\"), \n",
    "    bigquery.SchemaField(\"thunder\", \"STRING\", mode=\"REQUIRED\"), \n",
    "    bigquery.SchemaField(\"snow\", \"STRING\", mode=\"REQUIRED\"), \n",
    "    bigquery.SchemaField(\"freezing_rain\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"sleet\", \"STRING\", mode=\"REQUIRED\"), \n",
    "    bigquery.SchemaField(\"fog\", \"STRING\", mode=\"REQUIRED\"), \n",
    "    bigquery.SchemaField(\"last_update_nws\", \"DATETIME\", mode=\"REQUIRED\")\n",
    "  ] \n",
    "\n",
    "  jc = bigquery.LoadJobConfig(\n",
    "    source_format = bigquery.SourceFormat.CSV,\n",
    "    skip_leading_rows=1,\n",
    "    autodetect=False,\n",
    "    schema=schema,\n",
    "    create_disposition=\"CREATE_IF_NEEDED\",\n",
    "    write_disposition=\"WRITE_TRUNCATE\"   \n",
    "  )\n",
    "\n",
    "  # Set target table in BigQuery\n",
    "  full_table_id = f\"{PROJECT_ID}.{DATASET_ID}.{STAGING_TABLE_ID}\"\n",
    "\n",
    "  # Upload to BigQuery\n",
    "  ## If any columns are missing values, include name of column in error message\n",
    "  try: \n",
    "    job = client.load_table_from_dataframe(df, full_table_id, job_config=jc)\n",
    "    job.result()\n",
    "  except Exception as e:\n",
    "    error_message = str(e)\n",
    "    if 'Required column value for column index' in error_message:\n",
    "      start_index = error_message.index('Required column value for column index') + len('Required column value for column index: ')\n",
    "      end_index = error_message.index(' is missing', start_index)\n",
    "      missing_column_index = int(error_message[start_index:end_index])\n",
    "      missing_column_name = list(df.columns)[missing_column_index]\n",
    "      error_message = error_message[:start_index] + f'{missing_column_name} ({missing_column_index})' + error_message[end_index:]\n",
    "    raise Exception(error_message) \n",
    "  \n",
    "  job = client.load_table_from_dataframe(df, full_table_id, job_config=jc)\n",
    "  job.result()\n",
    "\n",
    "  # Log result \n",
    "  table = client.get_table(full_table_id)\n",
    "  print(f\"Loaded {table.num_rows} rows and {len(table.schema)} columns into {full_table_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3311 rows and 19 columns into alaska-scrape.weather.nws_staging\n"
     ]
    }
   ],
   "source": [
    "load_staging_table(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we make some basic data validations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nan_check() -> None: \n",
    "  \"\"\"Validates data in staging table by checking for NaNs\"\"\"\n",
    "\n",
    "  # The source data uses missing-value indicators so there should be no NaNs\n",
    "  nan_query = f\"\"\"\n",
    "    SELECT COUNT(*) AS num_rows\n",
    "    FROM {DATASET_ID}.{STAGING_TABLE_ID}\n",
    "    WHERE \n",
    "      temperature_f IS NULL OR\n",
    "      dewpoint_f IS NULL OR\n",
    "      wind_chill_f IS NULL OR\n",
    "      surface_wind_mph IS NULL OR\n",
    "      wind_dir IS NULL OR\n",
    "      gust IS NULL OR\n",
    "      sky_cover_pct IS NULL OR\n",
    "      precipitation_potential_pct IS NULL OR\n",
    "      relative_humidity_pct IS NULL OR\n",
    "      rain IS NULL OR\n",
    "      thunder IS NULL OR\n",
    "      snow IS NULL OR\n",
    "      freezing_rain IS NULL OR\n",
    "      sleet IS NULL OR\n",
    "      fog IS NULL OR\n",
    "      last_update_nws IS NULL\n",
    "    \"\"\"\n",
    "  \n",
    "  query_job = client.query(nan_query)\n",
    "  results = query_job.result()\n",
    "\n",
    "  num_rows = list(results)[0]['num_rows'] # number of rows containing nans\n",
    "  if num_rows  > 0:\n",
    "      print(f\"Warning: {num_rows} rows with NaN values found in the dataset\")\n",
    "      # Log bad rows \n",
    "  else:\n",
    "      print(\"No NaN values detected.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dup_check() -> None: \n",
    "  \"\"\"Validates data in staging table by checking for duplicates in location, utc_datetime, and lst_datetime\"\"\"\n",
    "\n",
    "  # It's conceivable that location/utc_datetime have duplicates but not location/lst_datetime, or vice-versa\n",
    "  # We check for each possibility rather than duplicates across all of locations/utc_datetime/lst_datetime\n",
    "\n",
    "  dup_query = f\"\"\"\n",
    "  SELECT *\n",
    "  FROM (\n",
    "    SELECT location, utc_datetime, COUNT(*) as count\n",
    "    FROM {DATASET_ID}.{STAGING_TABLE_ID}\n",
    "    GROUP BY location, utc_datetime\n",
    "    HAVING count > 1\n",
    "  ) AS subquery1\n",
    "  UNION ALL\n",
    "  SELECT *\n",
    "  FROM (\n",
    "    SELECT location, lst_datetime, COUNT(*) as count\n",
    "    FROM {DATASET_ID}.{STAGING_TABLE_ID}\n",
    "    GROUP BY location, lst_datetime\n",
    "    HAVING count > 1\n",
    "  ) AS subquery2\n",
    "  \"\"\"\n",
    "\n",
    "  query_job = client.query(dup_query)\n",
    "\n",
    "  results = query_job.result()\n",
    "\n",
    "  num_rows = list(results)[0]['num_rows'] # number of rows containing duplicates\n",
    "  if num_rows  > 0:\n",
    "      print(f\"Warning: {num_rows} rows with duplicate values in location, utc_datetime, and lst_datetime\")\n",
    "      # Log bad rows \n",
    "      # Drop duplicates\n",
    "  else:\n",
    "      print(\"No duplicate rows detected\")  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having validated the data and removed any bad rows, we can now insert from our staging table into our main table. In so doing we make two minor changes: \n",
    "1. We replace empty values (`\"\"`) with (`0`) in `gust`\n",
    "2. We add a timestamp column indicating the time of the append (useful if we need to undo something)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_table() -> None: \n",
    "  \"\"\"Insert staging table into the main data table\"\"\"\n",
    "\n",
    "  insert_query = f\"\"\"\n",
    "    INSERT INTO {DATASET_ID}.{MAIN_TABLE_ID}\n",
    "    SELECT *, TIMESTAMP()\n",
    "    FROM {DATASET_ID}.{STAGING_TABLE_ID}\n",
    "  \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>utc_datetime</th>\n",
       "      <th>lst_datetime</th>\n",
       "      <th>temperature_f</th>\n",
       "      <th>dewpoint_f</th>\n",
       "      <th>wind_chill_f</th>\n",
       "      <th>surface_wind_mph</th>\n",
       "      <th>wind_dir</th>\n",
       "      <th>gust</th>\n",
       "      <th>sky_cover_pct</th>\n",
       "      <th>precipitation_potential_pct</th>\n",
       "      <th>relative_humidity_pct</th>\n",
       "      <th>rain</th>\n",
       "      <th>thunder</th>\n",
       "      <th>snow</th>\n",
       "      <th>freezing_rain</th>\n",
       "      <th>sleet</th>\n",
       "      <th>fog</th>\n",
       "      <th>last_update_nws</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>St._Paul</td>\n",
       "      <td>2023-03-04 06:00:00</td>\n",
       "      <td>2023-03-03 21:00:00</td>\n",
       "      <td>34</td>\n",
       "      <td>26</td>\n",
       "      <td></td>\n",
       "      <td>36</td>\n",
       "      <td>S</td>\n",
       "      <td></td>\n",
       "      <td>79</td>\n",
       "      <td>51</td>\n",
       "      <td>71</td>\n",
       "      <td>Chc</td>\n",
       "      <td>--</td>\n",
       "      <td>Chc</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>2023-02-28 03:58:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>St._Paul</td>\n",
       "      <td>2023-03-04 07:00:00</td>\n",
       "      <td>2023-03-03 22:00:00</td>\n",
       "      <td>34</td>\n",
       "      <td>26</td>\n",
       "      <td></td>\n",
       "      <td>36</td>\n",
       "      <td>S</td>\n",
       "      <td></td>\n",
       "      <td>79</td>\n",
       "      <td>51</td>\n",
       "      <td>73</td>\n",
       "      <td>Chc</td>\n",
       "      <td>--</td>\n",
       "      <td>Chc</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>2023-02-28 03:58:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>St._Paul</td>\n",
       "      <td>2023-03-04 08:00:00</td>\n",
       "      <td>2023-03-03 23:00:00</td>\n",
       "      <td>34</td>\n",
       "      <td>27</td>\n",
       "      <td></td>\n",
       "      <td>36</td>\n",
       "      <td>S</td>\n",
       "      <td></td>\n",
       "      <td>79</td>\n",
       "      <td>51</td>\n",
       "      <td>75</td>\n",
       "      <td>Chc</td>\n",
       "      <td>--</td>\n",
       "      <td>Chc</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>2023-02-28 03:58:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>St._Paul</td>\n",
       "      <td>2023-03-04 09:00:00</td>\n",
       "      <td>2023-03-04 00:00:00</td>\n",
       "      <td>34</td>\n",
       "      <td>28</td>\n",
       "      <td></td>\n",
       "      <td>36</td>\n",
       "      <td>S</td>\n",
       "      <td></td>\n",
       "      <td>79</td>\n",
       "      <td>51</td>\n",
       "      <td>78</td>\n",
       "      <td>Chc</td>\n",
       "      <td>--</td>\n",
       "      <td>Chc</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>2023-02-28 03:58:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>St._Paul</td>\n",
       "      <td>2023-03-04 10:00:00</td>\n",
       "      <td>2023-03-04 01:00:00</td>\n",
       "      <td>34</td>\n",
       "      <td>28</td>\n",
       "      <td></td>\n",
       "      <td>36</td>\n",
       "      <td>S</td>\n",
       "      <td></td>\n",
       "      <td>79</td>\n",
       "      <td>51</td>\n",
       "      <td>80</td>\n",
       "      <td>Chc</td>\n",
       "      <td>--</td>\n",
       "      <td>Chc</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>2023-02-28 03:58:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3307</th>\n",
       "      <td>Aleknagik</td>\n",
       "      <td>2023-03-06 14:00:00</td>\n",
       "      <td>2023-03-06 05:00:00</td>\n",
       "      <td>29</td>\n",
       "      <td>26</td>\n",
       "      <td></td>\n",
       "      <td>14</td>\n",
       "      <td>SE</td>\n",
       "      <td></td>\n",
       "      <td>90</td>\n",
       "      <td>56</td>\n",
       "      <td>88</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>Lkly</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>2023-02-28 03:58:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3308</th>\n",
       "      <td>Aleknagik</td>\n",
       "      <td>2023-03-06 15:00:00</td>\n",
       "      <td>2023-03-06 06:00:00</td>\n",
       "      <td>30</td>\n",
       "      <td>26</td>\n",
       "      <td></td>\n",
       "      <td>14</td>\n",
       "      <td>SE</td>\n",
       "      <td></td>\n",
       "      <td>90</td>\n",
       "      <td>56</td>\n",
       "      <td>86</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>Lkly</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>2023-02-28 03:58:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3309</th>\n",
       "      <td>Aleknagik</td>\n",
       "      <td>2023-03-06 16:00:00</td>\n",
       "      <td>2023-03-06 07:00:00</td>\n",
       "      <td>30</td>\n",
       "      <td>25</td>\n",
       "      <td></td>\n",
       "      <td>14</td>\n",
       "      <td>SE</td>\n",
       "      <td></td>\n",
       "      <td>90</td>\n",
       "      <td>56</td>\n",
       "      <td>83</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>Lkly</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>2023-02-28 03:58:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3310</th>\n",
       "      <td>Aleknagik</td>\n",
       "      <td>2023-03-06 17:00:00</td>\n",
       "      <td>2023-03-06 08:00:00</td>\n",
       "      <td>30</td>\n",
       "      <td>24</td>\n",
       "      <td></td>\n",
       "      <td>14</td>\n",
       "      <td>SE</td>\n",
       "      <td></td>\n",
       "      <td>90</td>\n",
       "      <td>56</td>\n",
       "      <td>81</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>Lkly</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>2023-02-28 03:58:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3311</th>\n",
       "      <td>Aleknagik</td>\n",
       "      <td>2023-03-06 18:00:00</td>\n",
       "      <td>2023-03-06 09:00:00</td>\n",
       "      <td>30</td>\n",
       "      <td>24</td>\n",
       "      <td></td>\n",
       "      <td>14</td>\n",
       "      <td>SE</td>\n",
       "      <td></td>\n",
       "      <td>90</td>\n",
       "      <td>56</td>\n",
       "      <td>79</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>Lkly</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>2023-02-28 03:58:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>488 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       location        utc_datetime        lst_datetime temperature_f  \\\n",
       "515    St._Paul 2023-03-04 06:00:00 2023-03-03 21:00:00            34   \n",
       "516    St._Paul 2023-03-04 07:00:00 2023-03-03 22:00:00            34   \n",
       "517    St._Paul 2023-03-04 08:00:00 2023-03-03 23:00:00            34   \n",
       "518    St._Paul 2023-03-04 09:00:00 2023-03-04 00:00:00            34   \n",
       "519    St._Paul 2023-03-04 10:00:00 2023-03-04 01:00:00            34   \n",
       "...         ...                 ...                 ...           ...   \n",
       "3307  Aleknagik 2023-03-06 14:00:00 2023-03-06 05:00:00            29   \n",
       "3308  Aleknagik 2023-03-06 15:00:00 2023-03-06 06:00:00            30   \n",
       "3309  Aleknagik 2023-03-06 16:00:00 2023-03-06 07:00:00            30   \n",
       "3310  Aleknagik 2023-03-06 17:00:00 2023-03-06 08:00:00            30   \n",
       "3311  Aleknagik 2023-03-06 18:00:00 2023-03-06 09:00:00            30   \n",
       "\n",
       "     dewpoint_f wind_chill_f surface_wind_mph wind_dir gust sky_cover_pct  \\\n",
       "515          26                            36        S                 79   \n",
       "516          26                            36        S                 79   \n",
       "517          27                            36        S                 79   \n",
       "518          28                            36        S                 79   \n",
       "519          28                            36        S                 79   \n",
       "...         ...          ...              ...      ...  ...           ...   \n",
       "3307         26                            14       SE                 90   \n",
       "3308         26                            14       SE                 90   \n",
       "3309         25                            14       SE                 90   \n",
       "3310         24                            14       SE                 90   \n",
       "3311         24                            14       SE                 90   \n",
       "\n",
       "     precipitation_potential_pct relative_humidity_pct rain thunder  snow  \\\n",
       "515                           51                    71  Chc      --   Chc   \n",
       "516                           51                    73  Chc      --   Chc   \n",
       "517                           51                    75  Chc      --   Chc   \n",
       "518                           51                    78  Chc      --   Chc   \n",
       "519                           51                    80  Chc      --   Chc   \n",
       "...                          ...                   ...  ...     ...   ...   \n",
       "3307                          56                    88   --      --  Lkly   \n",
       "3308                          56                    86   --      --  Lkly   \n",
       "3309                          56                    83   --      --  Lkly   \n",
       "3310                          56                    81   --      --  Lkly   \n",
       "3311                          56                    79   --      --  Lkly   \n",
       "\n",
       "     freezing_rain sleet fog     last_update_nws  \n",
       "515             --    --  -- 2023-02-28 03:58:00  \n",
       "516             --    --  -- 2023-02-28 03:58:00  \n",
       "517             --    --  -- 2023-02-28 03:58:00  \n",
       "518             --    --  -- 2023-02-28 03:58:00  \n",
       "519             --    --  -- 2023-02-28 03:58:00  \n",
       "...            ...   ...  ..                 ...  \n",
       "3307            --    --  -- 2023-02-28 03:58:00  \n",
       "3308            --    --  -- 2023-02-28 03:58:00  \n",
       "3309            --    --  -- 2023-02-28 03:58:00  \n",
       "3310            --    --  -- 2023-02-28 03:58:00  \n",
       "3311            --    --  -- 2023-02-28 03:58:00  \n",
       "\n",
       "[488 rows x 19 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['wind_chill_f'] == \"\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "70623b801652781c2389d9f74154af1ef3dd8a50bfe8b7cd6824c1648ddc5ad0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
