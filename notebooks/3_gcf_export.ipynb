{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Cloud Functions & Cloud Scheduler\n",
    "\n",
    "This notebook shows how I hosted an automate a web-scraping script in Google Cloud Functions (GCF). \n",
    "\n",
    "---\n",
    "\n",
    "Airflow is extremely useful for coordinating complex pipelines in distributed systems and can be run in the cloud with [Google Cloud Composer](https://cloud.google.com/composer). The downside with this approach is that the increased overhead with Airflow results in a more expensive project. By contrast, GCF is dedicated to simple stateless functions and is much more affordable. Thankfully, our use case is simple enough for GCF to handle. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: `nws_update_gcf`\n",
    "\n",
    "We have two DAGs we'd like to port over as Google Cloud Functions. Let's use the NWS script as an example.\n",
    "\n",
    "Each function needs its own directory to export as a package. \n",
    "\n",
    "```bash\n",
    "nws_update_gcf\n",
    "├── main.py     # houses actual script \n",
    "├── README.md   \n",
    "├── requirements.txt # GCF container automatically installs these \n",
    "└── utils     \n",
    "    ├── __init__.py # need for utils to be read as a module\n",
    "    └── utils.py\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To export our DAG as a GCF, all we really need to do is 1.) remove the airflow decorators (```@task``` amd ```@dag```) and the ending DAG constructor statement, and 2.) add a function that serves as the \"entry point\" for GCF to trigger the function. \n",
    "```python \n",
    "## Bash: pip install functions-framework\n",
    "import functions_framework\n",
    "\n",
    "@functions_framework.http\n",
    "def main(request) -> None: \n",
    "  \"\"\"Entry point for google cloud function\"\"\"\n",
    "  df = get_forecast_df()\n",
    "  \n",
    "  load_staging_table(df)\n",
    "  \n",
    "  insert_table()\n",
    "\n",
    "  return \"Mandatory Return Statement\"\n",
    "```\n",
    "\n",
    "Here our function ```main``` is set up to be triggered when a request is sent to the target URL of the function. Notice that ```main``` takes a similar role to the ```dag()``` constructor in airflow, in that it defines the order of how the functions in the script will be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt \n",
    "import logging \n",
    "from io import BytesIO\n",
    "# GCP imports: \n",
    "from google.cloud import bigquery, storage, logging as cloud_logging \n",
    "from google.oauth2 import service_account\n",
    "from google.api_core.exceptions import NotFound\n",
    "# Utils\n",
    "import utils.utils as utils \n",
    "## ^^ For the actual package it will just be \"utils.utils\"\n",
    "# Functions Framework \n",
    "import functions_framework\n",
    "\n",
    "## ---------- GCP INFO ---------- ## \n",
    "PROJECT_ID = \"alaska-scrape\"\n",
    "DATASET_ID = \"weather\"\n",
    "STAGING_TABLE_ID = \"nws_staging\"\n",
    "MAIN_TABLE_ID = \"nws\"\n",
    "\n",
    "SCHEMA =  [\n",
    "    bigquery.SchemaField(\"location\", \"STRING\", mode=\"REQUIRED\"), \n",
    "    bigquery.SchemaField(\"utc_datetime\", \"DATETIME\", mode=\"REQUIRED\"), \n",
    "    bigquery.SchemaField(\"lst_datetime\", \"DATETIME\", mode=\"REQUIRED\"), \n",
    "    bigquery.SchemaField(\"temperature_f\", \"INTEGER\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"dewpoint_f\", \"INTEGER\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"wind_chill_f\", \"INTEGER\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"surface_wind_mph\", \"INTEGER\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"wind_dir\", \"STRING\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"gust\", \"INTEGER\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"sky_cover_pct\", \"INTEGER\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"precipitation_potential_pct\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"relative_humidity_pct\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"rain\", \"STRING\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"thunder\", \"STRING\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"snow\", \"STRING\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"freezing_rain\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"sleet\", \"STRING\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"fog\", \"STRING\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"last_update_nws\", \"DATETIME\", mode=\"NULLABLE\")\n",
    "  ] \n",
    "\n",
    "## ---------- LOGGING ---------- ## \n",
    "# Cloud logging client\n",
    "# logger_client = cloud_logging.Client(credentials=credentials, project=credentials.project_id)\n",
    "logger_client = cloud_logging.Client()\n",
    "\n",
    "# Cloud logging handler\n",
    "handler = logger_client.get_default_handler()\n",
    "\n",
    "# Create logger with cloud handler\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "# Set logging levels \n",
    "logger.setLevel(logging.INFO)\n",
    "handler.setLevel(logging.INFO)\n",
    "\n",
    "# Format logger \n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "# Confirm logger is working  \n",
    "logger.info(f\"Running daily scrape of NWS Weather Forecasts in Alaska\")\n",
    "\n",
    "## ---------- CLOUD STORAGE ---------- ## \n",
    "# storage_client = storage.Client(credentials=credentials, project=credentials.project_id)\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(f\"{PROJECT_ID}-bucket\")\n",
    "\n",
    "# Locations \n",
    "blob = bucket.blob(\"locations.csv\")\n",
    "content = blob.download_as_bytes()\n",
    "locations_df = pd.read_csv(BytesIO(content))\n",
    "\n",
    "## ---------- BIGQUERY ---------- ## \n",
    "# bq_client = bigquery.Client(credentials=credentials, project=credentials.project_id)\n",
    "bq_client = bigquery.Client()\n",
    "\n",
    "\n",
    "def get_forecast_df() -> pd.DataFrame:\n",
    "  \"\"\"Get dataframe of forecast data for next 6 days from various points in Alaska\"\"\"\n",
    "\n",
    "  nws_urls = locations_df.apply(utils.get_nws_url, axis=1)\n",
    "  url_map = dict(zip(locations_df['station_location'], nws_urls))\n",
    "\n",
    "  combined_table = []\n",
    "  for location, url in url_map.items():\n",
    "    soup_list = [utils.get_soup(url + f\"&AheadHour={hr}\") for hr in (0,48,96)]\n",
    "    table_list = utils.flatten([utils.extract_table_data(soup, location) for soup in soup_list])\n",
    "    combined_table.extend(table_list)\n",
    "\n",
    "  forecast_dict = utils.transpose_as_dict(combined_table)\n",
    "  forecast_df = utils.transform_df(forecast_dict)\n",
    "  \n",
    "  return forecast_df\n",
    "\n",
    "\n",
    "def load_staging_table(df:pd.DataFrame) -> None:\n",
    "  \"\"\"Upload dataframe from transform_df() to BigQuery staging table\"\"\"\n",
    "\n",
    "  jc = bigquery.LoadJobConfig(\n",
    "    source_format = bigquery.SourceFormat.CSV,\n",
    "    skip_leading_rows=1,\n",
    "    autodetect=False,\n",
    "    schema=SCHEMA,\n",
    "    create_disposition=\"CREATE_IF_NEEDED\",\n",
    "    write_disposition=\"WRITE_TRUNCATE\"   \n",
    "  )\n",
    " \n",
    "  # Set target table in BigQuery\n",
    "  full_table_id = f\"{PROJECT_ID}.{DATASET_ID}.{STAGING_TABLE_ID}\"\n",
    "\n",
    "  # Upload to BigQuery\n",
    "  ## If any required columns are missing values, include name of column in error message\n",
    "  try: \n",
    "    job = bq_client.load_table_from_dataframe(df, full_table_id, job_config=jc)\n",
    "    job.result()\n",
    "  except Exception as e:\n",
    "    error_message = str(e)\n",
    "    if 'Required column value for column index' in error_message:\n",
    "      start_index = error_message.index('Required column value for column index') + len('Required column value for column index: ')\n",
    "      end_index = error_message.index(' is missing', start_index)\n",
    "      missing_column_index = int(error_message[start_index:end_index])\n",
    "      missing_column_name = list(df.columns)[missing_column_index]\n",
    "      error_message = error_message[:start_index] + f'{missing_column_name} ({missing_column_index})' + error_message[end_index:]\n",
    "    raise Exception(error_message) \n",
    "\n",
    "  # Log result \n",
    "  table_ref = bq_client.get_table(full_table_id)\n",
    "  logger.info(f\"Loaded {table_ref.num_rows} rows and {table_ref.schema} columns\")\n",
    "\n",
    "  \n",
    "def insert_table() -> None: \n",
    "  \"\"\"Insert staging table into the main data table -- creates the table if it doesn't exist yet\"\"\"\n",
    "  \n",
    "  insert_query=f\"\"\"\n",
    "    INSERT INTO {DATASET_ID}.{MAIN_TABLE_ID} \n",
    "    SELECT *, CURRENT_TIMESTAMP() as date_added_utc\n",
    "    FROM {DATASET_ID}.{STAGING_TABLE_ID}\n",
    "    \"\"\"\n",
    "  \n",
    "  full_table_id = f\"{PROJECT_ID}.{DATASET_ID}.{MAIN_TABLE_ID}\"\n",
    "\n",
    "  try: \n",
    "    query_job = bq_client.query(insert_query) \n",
    "    query_job.result()\n",
    "  except NotFound:\n",
    "    logger.info(f\"Table {DATASET_ID}.{MAIN_TABLE_ID} does not exist. Creating.\")\n",
    "\n",
    "    # Adding date_added to SCHEMA \n",
    "    schema = SCHEMA + [bigquery.SchemaField(\"date_added_utc\", \"TIMESTAMP\", mode=\"REQUIRED\")]\n",
    "\n",
    "    table = bigquery.Table(full_table_id, schema=schema)\n",
    "    table = bq_client.create_table(table)\n",
    "\n",
    "    query_job = bq_client.query(insert_query)\n",
    "    query_job.result()\n",
    "    \n",
    "  table = bq_client.get_table(full_table_id)\n",
    "  logger.info(f\"Loaded {table.num_rows} rows and {len(table.schema)} columns into {full_table_id}\\n\")\n",
    "\n",
    "@functions_framework.http\n",
    "def main(request) -> None: \n",
    "  \"\"\"Entry point for google cloud function\"\"\"\n",
    "  df = get_forecast_df()\n",
    "  \n",
    "  load_staging_table(df)\n",
    "  \n",
    "  insert_table()\n",
    "\n",
    "  return \"Mandatory Return Statement\" # Can put anything but must be present. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To export our directory as a cloud function, we use the  ```gcloud functions deploy``` command: \n",
    "\n",
    "```bash \n",
    "\n",
    "gcloud functions deploy YOUR_FUNCTION_NAME \\\n",
    "[--gen2] \\ # gen1 vs gen2 has some minor differences \n",
    "--region=<google-server-location> \\ # e.g. us-central1\n",
    "--runtime=<python-version> \\ # python37, python38, etc.\n",
    "--source=</path/to/your/directory> \\ \n",
    "--entry-point=<your-target-function> \\ # here it's our function \"main()\"\n",
    "--memory=512 # 512MB, default is 256MB \n",
    "--trigger-http # how the function gets triggered -- can have multiple \n",
    "--allow-unauthenticated # allows anyone with URL to call function -- enable for setup convenience, NOT secure\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documentation on [this](https://cloud.google.com/functions/docs/deploy#from-local-machine) page is pretty good but it does not clarify exactly how the script should be structured. Be sure to have a proper entry point defined in your script for the function to trigger.\n",
    "\n",
    "You can test your function by clicking \"Test in Cloud Shell\" on the testing tab on the function's page.  \n",
    "\n",
    "![gcf-test](../img/gcf_testingpage.png)\n",
    "\n",
    "By default it will pre-fill a JSON with `Hello World!` -- this JSON is used to pass information to your function when you call it. We don't need that so we can leave the JSON empty. \n",
    "\n",
    "Now we are able to trigger our function on demand from cloud shell or our command line:\n",
    "\n",
    "```bash\n",
    "\n",
    "curl -m 706 -X POST https://your-function-target-url \\\n",
    "-H \"Authorization: bearer $(gcloud auth print-identity-token)\" \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-d '{}'\n",
    "\n",
    "```\n",
    "This setup does not go over authentication, as that requires more involved set up to coordinate service accounts and access permissions. We can forgo it for a non-critical project like this. I do suggest you name your function with a string of random characters in order to keep your endpoint URL even more obscure."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cloud Scheduler \n",
    "\n",
    "Once you have the function exported to cloud functions, we can schedule it to run at regular intervals using [Cloud Scheduler](https://cloud.google.com/scheduler). This is a simple service that can schedule and send requests to the target URL generated when we deployed our function. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. From the above page, go to the Cloud Scheduler Console for your project.\n",
    "2. Click \"Create Job\" at the top of the page.\n",
    "\n",
    "![gcf-screenshot](../img/gcf_screenshot.resized.png)\n",
    "  \n",
    "\n",
    "3. Set the Timezone and Schedule interval using [CRON syntax](https://crontab.guru/) (e.g. for noon, `0 12 * * *`)\n",
    "4. Under \"Configure execution\" paste your function's target URL and select `Target type: HTTP` and `HTTP method: POST`. \n",
    "5. You can leave the `Retry config` settings in the next section as default. \n",
    "\n",
    "Refer back to the Cloud Scheduler and [Cloud Logging](https://cloud.google.com/logging) consoles to confirm that your function is working as scheduled. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a5edab282632443219e051e4ade2d1d5bbc671c781051bf1437897cbdfea0f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
