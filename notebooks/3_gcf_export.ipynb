{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Cloud Functions\n",
    "\n",
    "This notebook shows how to export, host, and automate a web-scraping script in Google Cloud Functions (GCF). \n",
    "\n",
    "---\n",
    "\n",
    "Airflow is extremely useful for coordinating complex pipelines in distributed systems and can be run in the cloud with [Google Cloud Composer](https://cloud.google.com/composer). The downside with this approach is that the increased overhead with Airflow results in a more expensive project. By contrast, GCF is dedicated to simple stateless functions and is much more affordable. Thankfully, our use case is simple enough for GCF to handle. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: `nws_update_gcf`\n",
    "\n",
    "We have two DAGs we'd like to port over as Google Cloud Functions. Let's use the NWS script as an example.\n",
    "\n",
    "Each function needs its own directory to export as a package. \n",
    "\n",
    "```bash\n",
    "nws_update_gcf\n",
    "├── main.py     # houses actual script \n",
    "├── README.md   \n",
    "├── requirements.txt # GCF container automatically installs these \n",
    "└── utils     \n",
    "    ├── __init__.py\n",
    "    └── utils.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'venv' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/ -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime as dt \n",
    "import logging \n",
    "from io import BytesIO\n",
    "# GCP imports: \n",
    "from google.cloud import bigquery, storage, logging as cloud_logging \n",
    "from google.oauth2 import service_account\n",
    "from google.api_core.exceptions import NotFound\n",
    "# Utils\n",
    "import utils.utils\n",
    "## ^^ For the actual package it will just be \"utils.utils\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon creating a function in Google Cloud Functions, it is automatically associated with a single Google Cloud project. When creating various GCP client (e.g. `storage.Client()`) we would not have to specify the project or our credentials. We define these here just to test-run our code locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating GCP Connection \n",
    "from yaml import full_load\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "with open(\"../airflow/dags/config/gcp-config.yaml\", \"r\") as fp: \n",
    "    gcp_config = full_load(fp)\n",
    "\n",
    "PROJECT_ID = gcp_config[\"project-id\"]\n",
    "DATASET_ID = gcp_config[\"dataset-id\"]\n",
    "STAGING_TABLE_ID =  \"nws_staging\"\n",
    "MAIN_TABLE_ID = \"nws\"\n",
    "\n",
    "SCHEMA =  [\n",
    "    bigquery.SchemaField(\"location\", \"STRING\", mode=\"REQUIRED\"), \n",
    "    bigquery.SchemaField(\"utc_datetime\", \"DATETIME\", mode=\"REQUIRED\"), \n",
    "    bigquery.SchemaField(\"lst_datetime\", \"DATETIME\", mode=\"REQUIRED\"), \n",
    "    bigquery.SchemaField(\"temperature_f\", \"INTEGER\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"dewpoint_f\", \"INTEGER\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"wind_chill_f\", \"INTEGER\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"surface_wind_mph\", \"INTEGER\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"wind_dir\", \"STRING\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"gust\", \"INTEGER\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"sky_cover_pct\", \"INTEGER\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"precipitation_potential_pct\", \"FLOAT\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"relative_humidity_pct\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"rain\", \"STRING\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"thunder\", \"STRING\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"snow\", \"STRING\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"freezing_rain\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"sleet\", \"STRING\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"fog\", \"STRING\", mode=\"NULLABLE\"), \n",
    "    bigquery.SchemaField(\"last_update_nws\", \"DATETIME\", mode=\"NULLABLE\")\n",
    "  ] \n",
    "\n",
    "\n",
    "key_path = gcp_config[\"credentials\"]\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "   key_path, scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    "\n",
    ")\n",
    "\n",
    "## ---------- LOGGING ---------- ## \n",
    "# Cloud logging client\n",
    "logger_client = cloud_logging.Client(credentials=credentials, project=credentials.project_id)\n",
    "\n",
    "# Cloud logging handler\n",
    "handler = logger_client.get_default_handler()\n",
    "\n",
    "# Create logger with cloud handler\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "# Set logging levels \n",
    "logger.setLevel(logging.INFO)\n",
    "handler.setLevel(logging.INFO)\n",
    "\n",
    "# Format logger \n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "# Confirm logger is working  \n",
    "logger.info(f\"Running daily scrape of NWS Weather Forecasts in Alaska\")\n",
    "\n",
    "## ---------- CLOUD STORAGE ---------- ## \n",
    "storage_client = storage.Client(credentials=credentials, project=credentials.project_id)\n",
    "bucket = storage_client.bucket(f\"{PROJECT_ID}-bucket\")\n",
    "\n",
    "# Locations \n",
    "blob = bucket.blob(\"locations.csv\")\n",
    "content = blob.download_as_bytes()\n",
    "locations_df = pd.read_csv(BytesIO(content))\n",
    "\n",
    "## ---------- BIGQUERY ---------- ## \n",
    "bq_client = bigquery.Client(credentials=credentials, project=credentials.project_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forecast_dict() -> pd.DataFrame:\n",
    "  \"\"\"Get dataframe of forecast data for next 6 days from various points in Alaska\"\"\"\n",
    "\n",
    "  nws_urls = locations_df.apply(utils.get_nws_url, axis=1)\n",
    "  url_map = dict(zip(locations_df['station_location'], nws_urls))\n",
    "\n",
    "  combined_table = []\n",
    "  for location, url in url_map.items():\n",
    "    soup_list = [utils.get_soup(url + f\"&AheadHour={hr}\") for hr in (0,48,96)]\n",
    "    table_list = utils.flatten([utils.extract_table_data(soup, location) for soup in soup_list])\n",
    "    combined_table.extend(table_list)\n",
    "\n",
    "  forecast_dict = utils.transpose_as_dict(combined_table)\n",
    "  \n",
    "  return forecast_dict\n",
    "\n",
    "fore_cast_dict = get_forecast_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transform_df(forecast_dict:dict) -> pd.DataFrame: \n",
    "  \"\"\"Cast dictionary from transpose_as_dict() to a dataframe and transform\"\"\"\n",
    "  ## Create dataframe\n",
    "  df = pd.DataFrame(forecast_dict)\n",
    "  \n",
    "  ## Edit column headers \n",
    "  df.columns = [col.lower() for col in df.columns] \n",
    "  df.rename(columns=lambda x: re.sub('°|\\(|\\)', '', x), inplace=True)\n",
    "  df.rename(columns=lambda x: re.sub('%', 'pct', x), inplace=True)\n",
    "  df.rename(columns=lambda x: re.sub(' ', '_', x.strip()), inplace=True)\n",
    "  \n",
    "  ## Replace missing values\n",
    "  # Replace missing values in gust with zero -- gust is never *actually* 0 so no masking\n",
    "  # Replace missing values in windchill with an explicity np.NaN\n",
    "  df.replace({'gust':{'':0}, 'wind_chill_f':{'':np.nan}}, inplace=True)\n",
    "\n",
    "  ## Datetime Transformations\n",
    "  cur_year = dt.datetime.now().year\n",
    "  dt_strings = df['date'] + '/' + str(cur_year) + ' ' + df['hour_akst'] + ':00 AKST'\n",
    "  # Local time (AKST)\n",
    "  df['lst_datetime'] = pd.to_datetime(dt_strings, format='%m/%d/%Y %H:%M AKST')\n",
    "  # UTC time\n",
    "  akst_offset = dt.timedelta(hours=9)\n",
    "  df['utc_datetime'] = df['lst_datetime'] + akst_offset\n",
    "\n",
    "  ## Drop duplicates in composite key columns \n",
    "  duplicates = df.duplicated(subset=[\"location\", \"lst_datetime\"], keep=False)\n",
    "  duplicate_rows = df[duplicates]\n",
    "  if not duplicate_rows.empty:\n",
    "    logger.warning(f\"Warning: {len(duplicate_rows)} rows have duplicate values in location and lst_datetime\")\n",
    "    logger.info(\"Dropping\")\n",
    "    df.drop_duplicates(subset=['location', 'lst_datetime'], inplace=True, ignore_index=True)\n",
    "\n",
    "  ## Reorder columns \n",
    "  col_names = ['location', 'utc_datetime', 'lst_datetime'] + list(df.columns[4:-2]) + [\"last_update_nws\"]\n",
    "  df = df[col_names]\n",
    "\n",
    "  return df \n",
    "\n",
    "transformed_df = transform_df(fore_cast_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_staging_table(df:pd.DataFrame) -> None:\n",
    "  \"\"\"Upload dataframe from transform_df() to BigQuery staging table\"\"\"\n",
    "\n",
    "  jc = bigquery.LoadJobConfig(\n",
    "    source_format = bigquery.SourceFormat.CSV,\n",
    "    skip_leading_rows=1,\n",
    "    autodetect=False,\n",
    "    schema=SCHEMA,\n",
    "    create_disposition=\"CREATE_IF_NEEDED\",\n",
    "    write_disposition=\"WRITE_TRUNCATE\"   \n",
    "  )\n",
    " \n",
    "  # Set target table in BigQuery\n",
    "  full_table_id = f\"{PROJECT_ID}.{DATASET_ID}.{STAGING_TABLE_ID}\"\n",
    "\n",
    "  # Upload to BigQuery\n",
    "  ## If any required columns are missing values, include name of column in error message\n",
    "  try: \n",
    "    job = bq_client.load_table_from_dataframe(df, full_table_id, job_config=jc)\n",
    "    job.result()\n",
    "  except Exception as e:\n",
    "    error_message = str(e)\n",
    "    if 'Required column value for column index' in error_message:\n",
    "      start_index = error_message.index('Required column value for column index') + len('Required column value for column index: ')\n",
    "      end_index = error_message.index(' is missing', start_index)\n",
    "      missing_column_index = int(error_message[start_index:end_index])\n",
    "      missing_column_name = list(df.columns)[missing_column_index]\n",
    "      error_message = error_message[:start_index] + f'{missing_column_name} ({missing_column_index})' + error_message[end_index:]\n",
    "    raise Exception(error_message) \n",
    "\n",
    "  # Log result \n",
    "  table_ref = bq_client.get_table(full_table_id)\n",
    "  logger.info(f\"Loaded {table_ref.num_rows} rows and {table_ref.schema} columns\")\n",
    "\n",
    "load_staging_table(transformed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_table() -> None: \n",
    "  \"\"\"Insert staging table into the main data table -- creates the table if it doesn't exist yet\"\"\"\n",
    "  \n",
    "  insert_query=f\"\"\"\n",
    "    INSERT INTO {DATASET_ID}.{MAIN_TABLE_ID} \n",
    "    SELECT *, CURRENT_TIMESTAMP() as date_added\n",
    "    FROM {DATASET_ID}.{STAGING_TABLE_ID}\n",
    "    \"\"\"\n",
    "  \n",
    "  full_table_id = f\"{PROJECT_ID}.{DATASET_ID}.{MAIN_TABLE_ID}\"\n",
    "\n",
    "  try: \n",
    "    query_job = bq_client.query(insert_query) \n",
    "    query_job.result()\n",
    "  except NotFound:\n",
    "    logger.info(f\"Table {DATASET_ID}.{MAIN_TABLE_ID} does not exist. Creating.\")\n",
    "\n",
    "    # Adding date_added to SCHEMA \n",
    "    schema = SCHEMA + [bigquery.SchemaField(\"date_added\", \"TIMESTAMP\", mode=\"REQUIRED\")]\n",
    "\n",
    "    table = bigquery.Table(full_table_id, schema=schema)\n",
    "    table = bq_client.create_table(table)\n",
    "\n",
    "    query_job = bq_client.query(insert_query)\n",
    "    query_job.result()\n",
    "    \n",
    "  table = bq_client.get_table(full_table_id)\n",
    "  logger.info(f\"Loaded {table.num_rows} rows and {len(table.schema)} columns into {full_table_id}\\n\")\n",
    "\n",
    "insert_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we copy this script into `main.py` "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a5edab282632443219e051e4ade2d1d5bbc671c781051bf1437897cbdfea0f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
